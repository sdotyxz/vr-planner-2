# Godot - 3D

**Pages:** 59

---

## 2D and 3D physics interpolation — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/physics/interpolation/2d_and_3d_physics_interpolation.html

**Contents:**
- 2D and 3D physics interpolation
- Global versus local interpolation
- Resetting physics interpolation
- 2D Particles
- Other
- User-contributed notes

Generally 2D and 3D physics interpolation work in very similar ways. However, there are a few differences, which will be described here.

In 3D, physics interpolation is performed independently on the global transform of each 3D instance.

In 2D by contrast, physics interpolation is performed on the local transform of each 2D instance.

This has some implications:

In 3D, it is easy to turn interpolation on and off at the level of each Node, via the physics_interpolation_mode property in the Inspector, which can be set to On, Off, or Inherited.

However this means that in 3D, pivots that occur in the SceneTree (due to parent child relationships) can only be interpolated approximately over the physics tick. In most cases this will not matter, but in some situations the interpolation can look slightly wrong.

In 2D, interpolated local transforms are passed down to children during rendering. This means that if a parent has physics_interpolation_mode set to On, but the child is set to Off, the child will still be interpolated if the parent is moving. Only the child's local transform is uninterpolated. Controlling the on / off behavior of 2D nodes therefore requires a little more thought and planning.

On the positive side, pivot behavior in the scene tree is perfectly preserved during interpolation in 2D, which gives super smooth behavior.

Whenever objects are moved to a completely new position, and interpolation is not desired (so as to prevent a "streaking" artefact), it is the responsibility of the user to call reset_physics_interpolation().

The good news is that in 2D, this is automatically done for you when nodes first enter the tree. This reduces boiler plate, and reduces the effort required to get an existing project working.

If you move objects after adding to the scene tree, you will still need to call reset_physics_interpolation() as with 3D.

Currently only CPUParticles2D are supported for physics interpolation in 2D. It is recommended to use a physics tick rate of at least 20-30 ticks per second to keep particles looking fluid.

Particles2D (GPU particles) are not yet interpolated, so for now it is recommended to convert to CPUParticles2D (but keep a backup of your Particles2D in case we get these working).

get_global_transform_interpolated() is currently only available for 3D.

MultiMeshes are supported in both 2D and 3D.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## 3D antialiasing — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/3d_antialiasing.html

**Contents:**
- 3D antialiasing
- Introduction
- Multisample antialiasing (MSAA)
- Temporal antialiasing (TAA)
- AMD FidelityFX Super Resolution 2.2 (FSR2)
- Fast approximate antialiasing (FXAA)
- Sub-pixel Morphological Antialiasing (SMAA 1x)
- Supersample antialiasing (SSAA)
- Screen-space roughness limiter
- Texture roughness limiter on import

Godot also supports antialiasing in 2D rendering. This is covered on the 2D antialiasing page.

Due to their limited resolution, scenes rendered in 3D can exhibit aliasing artifacts. These artifacts commonly manifest as a "staircase" effect on surface edges (edge aliasing) and as flickering and/or sparkles on reflective surfaces (specular aliasing).

In the example below, you can notice how edges have a blocky appearance. The vegetation is also flickering in and out, and thin lines on top of the box have almost disappeared:

Image is scaled by 2× with nearest-neighbor filtering to make aliasing more noticeable.

To combat this, various antialiasing techniques can be used in Godot. These are detailed below.

You can compare antialiasing algorithms in action using the 3D Antialiasing demo project.

This is available in all renderers.

This technique is the "historical" way of dealing with aliasing. MSAA is very effective on geometry edges (especially at higher levels). MSAA does not introduce any blurriness whatsoever.

MSAA is available in 3 levels: 2×, 4×, 8×. Higher levels are more effective at antialiasing edges, but are significantly more demanding. In games with modern visuals, sticking to 2× or 4× MSAA is highly recommended as 8× MSAA is usually too demanding.

The downside of MSAA is that it only operates on edges. This is because MSAA increases the number of coverage samples, but not the number of color samples. However, since the number of color samples did not increase, fragment shaders are still run for each pixel only once. Therefore, MSAA does not reduce transparency aliasing for materials using the Alpha Scissor transparency mode (1-bit transparency). MSAA is also ineffective on specular aliasing.

To mitigate aliasing on alpha scissor materials, alpha antialiasing (also called alpha to coverage) can be enabled on specific materials in the StandardMaterial3D or ORMMaterial3D properties. Alpha to coverage has a moderate performance cost, but it's effective at reducing aliasing on transparent materials without introducing any blurriness.

To make specular aliasing less noticeable, use the Screen-space roughness limiter, which is enabled by default.

MSAA can be enabled in the Project Settings by changing the value of the Rendering > Anti Aliasing > Quality > MSAA 3D setting. It's important to change the value of the MSAA 3D setting and not MSAA 2D, as these are entirely separate settings.

Comparison between no antialiasing (left) and various MSAA levels (right). Note that alpha antialiasing is not used here:

This is only available in the Forward+ renderer, not the Mobile or Compatibility renderers.

Temporal antialiasing works by converging the result of previously rendered frames into a single, high-quality frame. This is a continuous process that works by jittering the position of all vertices in the scene every frame. This jittering is done to capture sub-pixel detail and should be unnoticeable except in extreme situations.

This technique is commonly used in modern games, as it provides the most effective form of antialiasing against specular aliasing and other shader-induced artifacts. TAA also provides full support for transparency antialiasing.

TAA introduces a small amount of blur when enabled in still scenes, but this blurring effect becomes more pronounced when the camera is moving. Another downside of TAA is that it can exhibit ghosting artifacts behind moving objects. Rendering at a higher framerate will allow TAA to converge faster, therefore making those ghosting artifacts less visible.

Temporal antialiasing can be enabled in the Project Settings by changing the value of the Rendering > Anti Aliasing > Quality > TAA setting.

Comparison between no antialiasing (left) and TAA (right):

This is only available in the Forward+ renderer, not the Mobile or Compatibility renderers.

Since Godot 4.2, there is built-in support for AMD FidelityFX Super Resolution 2.2. This is an upscaling method compatible with all recent GPUs from any vendor. FSR2 is normally designed to improve performance by lowering the internal 3D rendering resolution, then upscaling to the output resolution.

However, unlike FSR1, FSR2 also provides temporal antialiasing. This means FSR2 can be used at native resolution for high-quality antialiasing, with the input resolution being equal to the output resolution. In this situation, enabling FSR2 will actually decrease performance, but it will significantly improve rendering quality.

Using FSR2 at native resolution is more demanding than using TAA at native resolution, so its use is only recommended if you have significant GPU headroom. On the bright side, FSR2 provides better antialiasing coverage with less blurriness compared to TAA, especially in motion.

Comparison between no antialiasing (left) and FSR2 at native resolution (right):

By default, the FSR Sharpness project setting is set to 0.2 (higher values result in less sharpening). For the purposes of comparison, FSR sharpening has been disabled by setting it to 2.0 on the above screenshot.

This is only available in the Forward+ and Mobile renderers, not the Compatibility renderer.

Fast approximate antialiasing is a post-processing antialiasing solution. It is faster to run than any other antialiasing technique and also supports antialiasing transparency. However, since it lacks temporal information, it will not do much against specular aliasing.

This technique is still sometimes used in mobile games. However, on desktop platforms, FXAA generally fell out of fashion in favor of temporal antialiasing, which is much more effective against specular aliasing. Nonetheless, exposing FXAA as an in-game option may still be worthwhile for players with low-end GPUs.

FXAA introduces a moderate amount of blur when enabled (more than TAA when still, but less than TAA when the camera is moving).

FXAA can be enabled in the Project Settings by changing the value of the Rendering > Anti Aliasing > Quality > Screen Space AA setting to FXAA.

Comparison between no antialiasing (left) and FXAA (right):

This is only available in the Forward+ and Mobile renderers, not the Compatibility renderer.

Sub-pixel Morphological Antialiasing is a post-processing antialiasing solution. It runs slightly slower than FXAA, but produces less blurriness. This is very helpful when the screen resolution is 1080p or below. Just like FXAA, SMAA 1x lacks temporal information and will therefore not do much against specular aliasing.

Use SMAA 1x if you can't afford MSAA, but find FXAA too blurry.

Combine it with TAA, or even FSR2, to maximize antialiasing at a higher GPU cost and some added blurriness. This is most beneficial in fast-moving scenes or just after a camera cut, especially at lower FPS.

SMAA 1x can be enabled in the Project Settings by changing the value of the Rendering > Anti Aliasing > Quality > Screen Space AA setting to SMAA.

Comparison between no antialiasing (left) and SMAA 1x (right):

This is available in all renderers.

Supersampling provides the highest quality of antialiasing possible, but it's also the most expensive. It works by shading every pixel in the scene multiple times. This allows SSAA to antialias edges, transparency and specular aliasing at the same time, without introducing potential ghosting artifacts.

The downside of SSAA is its extremely high cost. This cost generally makes SSAA difficult to use for game purposes, but you may still find supersampling useful for offline rendering.

Supersample antialiasing is performed by increasing the Rendering > Scaling 3D > Scale advanced project setting above 1.0 while ensuring Rendering > Scaling 3D > Mode is set to Bilinear (the default). Since the scale factor is defined per-axis, a scale factor of 1.5 will result in 2.25× SSAA while a scale factor of 2.0 will result in 4× SSAA. Since Godot uses the hardware's own bilinear filtering to perform the downsampling, the result will look crisper at integer scale factors (namely, 2.0).

Comparison between no antialiasing (left) and various SSAA levels (right):

Supersampling also has high video RAM requirements, since it needs to render in the target resolution then downscale to the window size. For example, displaying a project in 3840×2160 (4K resolution) with 4× SSAA will require rendering the scene in 7680×4320 (8K resolution), which is 4 times more pixels.

If you are using a high window size such as 4K, you may find that increasing the resolution scale past a certain value will cause a heavy slowdown (or even a crash) due to running out of VRAM.

This is only available in the Forward+ and Mobile renderers, not the Compatibility renderer.

This is not an edge antialiasing method, but it is a way of reducing specular aliasing in 3D.

The screen-space roughness limiter works best on detailed geometry. While it has an effect on roughness map rendering itself, its impact is limited there.

The screen-space roughness limiter is enabled by default; it doesn't require any manual setup. It has a small performance impact, so consider disabling it if your project isn't affected by specular aliasing much. You can disable it with the Rendering > Quality > Screen Space Filters > Screen Space Roughness Limiter project setting.

Like the screen-space roughness limiter, this is not an edge antialiasing method, but it is a way of reducing specular aliasing in 3D.

Roughness limiting on import works by specifying a normal map to use as a guide for limiting roughness. This is done by selecting the roughness map in the FileSystem dock, then going to the Import dock and setting Roughness > Mode to the color channel the roughness map is stored in (typically Green), then setting the path to the material's normal map. Remember to click Reimport at the bottom of the Import dock after setting the path to the normal map.

Since this processing occurs purely on import, it has no performance cost whatsoever. However, its visual impact is limited. Limiting roughness on import only helps reduce specular aliasing within textures, not the aliasing that occurs on geometry edges on detailed meshes.

There is no "one size fits all" antialiasing technique. Since antialiasing is often demanding on the GPU or can introduce unwanted blurriness, you'll want to add a setting to allow players to disable antialiasing.

For projects with a photorealistic art direction, TAA is generally the most suitable option. While TAA can introduce ghosting artifacts, there is no other technique that combats specular aliasing as well as TAA does. The screen-space roughness limiter helps a little, but is far less effective against specular aliasing overall. If you have spare GPU power, you can use FSR2 at native resolution for a better-looking form of temporal antialiasing compared to standard TAA.

For projects with a low amount of reflective surfaces (such as a cartoon artstyle), MSAA can work well. MSAA is also a good option if avoiding blurriness and temporal artifacts is important, such as in competitive games.

When targeting low-end platforms such as mobile or integrated graphics, FXAA is usually the only viable option. 2× MSAA may be usable in some circumstances, but higher MSAA levels are unlikely to run smoothly on mobile GPUs.

Godot allows using multiple antialiasing techniques at the same time. This is usually unnecessary, but it can provide better visuals on high-end GPUs or for non-real-time rendering. For example, to make moving edges look better when TAA is enabled, you can also enable MSAA at the same time.

Specular antialiasing

Transparency antialiasing

MSAA does not work well with materials with Alpha Scissor (1-bit transparency). This can be mitigated by enabling alpha antialiasing on the material.

TAA/FSR2 transparency antialiasing is most effective when using Alpha Scissor.

SSAA has some blur from bilinear downscaling. This can be mitigated by using an integer scaling factor of 2.0.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## 3D gizmo plugins — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/plugins/editor/3d_gizmos.html

**Contents:**
- 3D gizmo plugins
- Introduction
- The EditorNode3DGizmoPlugin
- Simple approach
- Alternative approach
- User-contributed notes

The content of this page was not yet updated for Godot 4.5 and may be outdated. If you know how to improve this page or you can confirm that it's up to date, feel free to open a pull request.

3D gizmo plugins are used by the editor and custom plugins to define the gizmos attached to any kind of Node3D node.

This tutorial shows the two main approaches to defining your own custom gizmos. The first option works well for simple gizmos and creates less clutter in your plugin structure, and the second one will let you store some per-gizmo data.

This tutorial assumes you already know how to make generic plugins. If in doubt, refer to the Making plugins page.

Regardless of the approach we choose, we will need to create a new EditorNode3DGizmoPlugin. This will allow us to set a name for the new gizmo type and define other behaviors such as whether the gizmo can be hidden or not.

This would be a basic setup:

For simple gizmos, inheriting EditorNode3DGizmoPlugin is enough. If you want to store some per-gizmo data or you are porting a Godot 3.0 gizmo to 3.1+, you should go with the second approach.

The first step is to, in our custom gizmo plugin, override the _has_gizmo() method so that it returns true when the node parameter is of our target type.

Then we can override methods like _redraw() or all the handle related ones.

Note that we created a material in the _init method, and retrieved it in the _redraw method using get_material(). This method retrieves one of the material's variants depending on the state of the gizmo (selected and/or editable).

So the final plugin would look somewhat like this:

Note that we just added some handles in the _redraw method, but we still need to implement the rest of handle-related callbacks in EditorNode3DGizmoPlugin to get properly working handles.

In some cases we want to provide our own implementation of EditorNode3DGizmo, maybe because we want to have some state stored in each gizmo or because we are porting an old gizmo plugin and we don't want to go through the rewriting process.

In these cases all we need to do is, in our new gizmo plugin, override _create_gizmo(), so it returns our custom gizmo implementation for the Node3D nodes we want to target.

This way all the gizmo logic and drawing methods can be implemented in a new class extending EditorNode3DGizmo, like so:

Note that we just added some handles in the _redraw method, but we still need to implement the rest of handle-related callbacks in EditorNode3DGizmo to get properly working handles.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
# my_custom_gizmo_plugin.gd
extends EditorNode3DGizmoPlugin


func _get_gizmo_name():
    return "CustomNode"
```

Example 2 (javascript):
```javascript
# MyCustomEditorPlugin.gd
@tool
extends EditorPlugin


const MyCustomGizmoPlugin = preload("res://addons/my-addon/my_custom_gizmo_plugin.gd")

var gizmo_plugin = MyCustomGizmoPlugin.new()


func _enter_tree():
    add_node_3d_gizmo_plugin(gizmo_plugin)


func _exit_tree():
    remove_node_3d_gizmo_plugin(gizmo_plugin)
```

Example 3 (unknown):
```unknown
# ...


func _has_gizmo(node):
    return node is MyCustomNode3D


# ...
```

Example 4 (gdscript):
```gdscript
# ...


func _init():
    create_material("main", Color(1, 0, 0))
    create_handle_material("handles")


func _redraw(gizmo):
    gizmo.clear()

    var node3d = gizmo.get_node_3d()

    var lines = PackedVector3Array()

    lines.push_back(Vector3(0, 1, 0))
    lines.push_back(Vector3(0, node3d.my_custom_value, 0))

    var handles = PackedVector3Array()

    handles.push_back(Vector3(0, 1, 0))
    handles.push_back(Vector3(0, node3d.my_custom_value, 0))

    gizmo.add_lines(lines, get_material("main", gizmo), false)
    gizmo.add_handles(handles, get_material("handles", gizmo), [])


# ...
```

---

## 3D — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/index.html

**Contents:**
- 3D
- Rendering
- Optimization
- Tools

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## 3D lights and shadows — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/lights_and_shadows.html

**Contents:**
- 3D lights and shadows
- Introduction
- Light nodes
- Light number limits
- Shadow mapping
  - Tweaking shadow bias
- Directional light
  - Directional shadow mapping
- Omni light
  - Omni shadow mapping

Light sources emit light that mixes with the materials and produces a visible result. Light can come from several types of sources in a scene:

From the material itself, in the form of the emission color (though it does not affect nearby objects unless baked or screen-space indirect lighting is enabled).

Light nodes: DirectionalLight3D, OmniLight3D and SpotLight3D.

Ambient light in the Environment or Reflection probes.

Global illumination (LightmapGI, VoxelGI or SDFGI).

The emission color is a material property. You can read more about it in the Standard Material 3D and ORM Material 3D tutorial.

You can compare various types of lights in action using the 3D Lights and Shadows demo project.

There are three types of light nodes: DirectionalLight3D, OmniLight3D and SpotLight3D. Let's take a look at the common parameters for lights:

Each property has a specific function:

Color: Base color for emitted light.

Energy: Energy multiplier. This is useful for saturating lights or working with High dynamic range lighting.

Indirect Energy: Secondary multiplier used with indirect light (light bounces). This works with Using Lightmap global illumination, VoxelGI or SDFGI.

Volumetric Fog Energy: Secondary multiplier used with volumetric fog. This only has an effect when volumetric fog is enabled.

Negative: Light becomes subtractive instead of additive. It's sometimes useful to manually compensate some dark corners.

Specular: Affects the intensity of the specular blob in objects affected by this light. At zero, this light becomes a pure diffuse light.

Bake Mode: Sets the bake mode for the light. See Using Lightmap global illumination.

Cull Mask: Objects that are in the selected layers below will be affected by this light. Note that objects disabled via this cull mask will still cast shadows. If you don't want disabled objects to cast shadows, adjust the Cast Shadow property on the GeometryInstance3D to the desired value.

See Physical light and camera units if you wish to use real world units to configure your lights' intensity and color temperature.

When using the Forward+ renderer, Godot uses a clustering approach for real-time lighting. As many lights as desired can be added (as long as performance allows). However, there's still a default limit of 512 clustered elements that can be present in the current camera view. A clustered element is an omni light, a spot light, a decal or a reflection probe. This limit can be increased by adjusting Max Clustered Elements in Project Settings > Rendering > Limits > Cluster Builder.

When using the Mobile renderer, there is a limitation of 8 OmniLights + 8 SpotLights per mesh resource. There is also a limit of 256 OmniLights + 256 SpotLights that can be rendered in the current camera view. These limits currently cannot be changed.

When using the Compatibility renderer, up to 8 OmniLights + 8 SpotLights can be rendered per mesh resource. This limit can be increased in the advanced Project Settings by adjusting Max Renderable Elements and/or Max Lights per Object in Rendering > Limits > OpenGL, at the cost of performance and longer shader compilation times. The limit can also be decreased to reduce shader compilation times and improve performance slightly.

With all rendering methods, up to 8 DirectionalLights can be visible at a time. However, each additional DirectionalLight with shadows enabled will reduce the effective shadow resolution of each DirectionalLight. This is because directional shadow atlas is shared between all lights.

If the rendering limit is exceeded, lights will start popping in and out during camera movement, which can be distracting. Enabling Distance Fade on light nodes can help reduce this issue while also improving performance. Splitting your meshes into smaller portions can also help, especially for level geometry (which also improves culling efficiency).

If you need to render more lights than possible in a given renderer, consider using baked lightmaps with lights' bake mode set to Static. This allows lights to be fully baked, which also makes them much faster to render. You can also use emissive materials with any global illumination technique as a replacement for light nodes that emit light over a large area.

Lights can optionally cast shadows. This gives them greater realism (light does not reach occluded areas), but it can incur a bigger performance cost. There is a list of generic shadow parameters, each also has a specific function:

Enabled: Check to enable shadow mapping in this light.

Opacity: Areas occluded are darkened by this opacity factor. Shadows are fully opaque by default, but this can be changed to make shadows translucent for a given light.

Bias: When this parameter is too low, self-shadowing occurs. When too high, shadows separate from the casters. Tweak to what works best for you.

Normal Bias: When this parameter is too low, self-shadowing occurs. When too high, shadows appear misaligned from the casters. Tweak to what works best for you.

Transmittance Bias: When this parameter is too low, self-shadowing occurs on materials that have transmittance enabled. When too high, shadows will not affect materials that have transmittance enabled consistently. Tweak to what works best for you.

Reverse Cull Face: Some scenes work better when shadow mapping is rendered with face-culling inverted.

Blur: Multiplies the shadow blur radius for this light. This works with both traditional shadow mapping and contact-hardening shadows (lights with Angular Distance or Size greater than 0.0). Higher values result in softer shadows, which will also appear to be more temporally stable for moving objects. The downside of increasing shadow blur is that it will make the grainy pattern used for filtering more noticeable. See also Shadow filter mode.

Caster Mask: Shadows are only cast by objects in these layers. Note that this mask does not affect which objects shadows are cast onto.

Below is an image of what tweaking bias looks like. Default values work for most cases, but in general, it depends on the size and complexity of geometry.

If the Shadow Bias or Shadow Normal Bias is set too low for a given light, the shadow will be "smeared" onto the objects. This will cause the light's intended appearance to darken, and is called shadow acne:

On the other hand, if the Shadow Bias or Shadow Normal Bias is set too high for a given light, the shadow may appear to be disconnected from the object. This is called peter-panning:

In general, increasing Shadow Normal Bias is preferred over increasing Shadow Bias. Increasing Shadow Normal Bias does not cause as much peter-panning as increasing Shadow Bias, but it can still resolve most shadow acne issues efficiently. The downside of increasing Shadow Normal Bias is that it can make shadows appear thinner for certain objects.

Any sort of bias issues can be fixed by increasing the shadow map resolution, at the cost of decreased performance.

Tweaking shadow mapping settings is an art – there are no "one size fits all" settings. To achieve the best visuals, you may need to use different shadow bias values on a per-light basis.

Note on Appearance Changes: When enabling shadows on a light, be aware that the light's appearance might change compared to when it's rendered without shadows in the compatibility renderer. Due to limitations with older mobile devices, shadows are implemented using a multi-pass rendering approach so lights with shadows are rendered in sRGB space instead of linear space. This change in rendering space can sometimes drastically alter the light's appearance. To achieve a similar appearance to an unshadowed light, you may need to adjust the light's energy setting.

This is the most common type of light and represents a light source very far away (such as the sun). It is also the cheapest light to compute and should be used whenever possible (although it's not the cheapest shadow-map to compute, but more on that later).

Directional light models an infinite number of parallel light rays covering the whole scene. The directional light node is represented by a big arrow which indicates the direction of the light rays. However, the position of the node does not affect the lighting at all and can be anywhere.

Every face whose front-side is hit by the light rays is lit, while the others stay dark. Unlike most other light types, directional lights don't have specific parameters.

The directional light also offers an Angular Distance property, which determines the light's angular size in degrees. Increasing this above 0.0 will make shadows softer at greater distances from the caster, while also affecting the sun's appearance in procedural sky materials. This is called a contact-hardening shadow (also known as PCSS).

For reference, the angular distance of the Sun viewed from the Earth is approximately 0.5. This kind of shadow is expensive, so check the recommendations in PCSS recommendations if setting this value above 0.0 on lights with shadows enabled.

To compute shadow maps, the scene is rendered (only depth) from an orthogonal point of view that covers the whole scene (or up to the max distance). There is, however, a problem with this approach because objects closer to the camera receive low-resolution shadows that may appear blocky.

To fix this, a technique named Parallel Split Shadow Maps (PSSM) is used. This splits the view frustum in 2 or 4 areas. Each area gets its own shadow map. This allows small areas close to the viewer to have the same shadow resolution as a huge, far-away area. When shadows are enabled for DirectionalLight3D, the default shadow mode is PSSM with 4 splits. In scenarios where an object is large enough to appear in all four splits, it results in increased draw calls. Specifically, such an object will be rendered five times in total: once for each of the four shadow splits and once for the final scene rendering. This can impact performance, understanding this behavior is important for optimizing your scene and managing performance expectations.

With this, shadows become more detailed:

To control PSSM, a number of parameters are exposed:

Each split distance is controlled relative to the camera far (or shadow Max Distance if greater than 0.0). 0.0 is the eye position and 1.0 is where the shadow ends at a distance. Splits are in-between. Default values generally work well, but tweaking the first split a bit is common to give more detail to close objects (like a character in a third-person game).

Always make sure to set a shadow Max Distance according to what the scene needs. A lower maximum distance will result in better-looking shadows and better performance, as fewer objects will need to be included in shadow rendering. You can also adjust Fade Start to control how aggressive the shadow fade-out should be at a distance. For scenes where the Max Distance fully covers the scene at any given camera position, you can increase Fade Start to 1.0 to prevent the shadow from fading at a distance. This should not be done in scenes where Max Distance doesn't fully cover the scene, as the shadow will appear to be suddenly cut off at a distance.

Sometimes, the transition between a split and the next can look bad. To fix this, the Blend Splits option can be turned on, which sacrifices detail and performance in exchange for smoother transitions:

The Shadow > Normal Bias parameter can be used to fix special cases of self-shadowing when objects are perpendicular to the light. The only downside is that it makes the shadow a bit thinner. Consider increasing Shadow > Normal Bias before increasing Shadow > Bias in most situations.

Lastly, Pancake Size is a property that can be adjusted to fix missing shadows when using large objects with unsubdivided meshes. Only change this value if you notice missing shadows that are not related to shadow biasing issues.

Omni light is a point source that emits light spherically in all directions up to a given radius.

In real life, light attenuation is an inverse function, which means omni lights don't have a radius. This is a problem because it means computing several omni lights would become demanding.

To solve this, a Range parameter is introduced together with an attenuation function.

These two parameters allow tweaking how this works visually in order to find aesthetically pleasing results.

A Size parameter is also available in OmniLight3D. Increasing this value will make the light fade out slower and shadows appear blurrier when far away from the caster. This can be used to simulate area lights to an extent. This is called a contact-hardening shadow (also known as PCSS). This kind of shadow is expensive, so check the recommendations in PCSS recommendations if setting this value above 0.0 on lights with shadows enabled.

Omni light shadow mapping is relatively straightforward. The main issue that needs to be considered is the algorithm used to render it.

Omni Shadows can be rendered as either Dual Paraboloid or Cube mapped. Dual Paraboloid renders quickly, but can cause deformations, while Cube is more correct, but slower. The default is Cube, but consider changing it to Dual Paraboloid for lights where it doesn't make much of a visual difference.

If the objects being rendered are mostly irregular and subdivided, Dual Paraboloid is usually enough. In any case, as these shadows are cached in a shadow atlas (more on that at the end), it may not make a difference in performance for most scenes.

Omni lights with shadows enabled can make use of projectors. The projector texture will multiply the light's color by the color at a given point on the texture. As a result, lights will usually appear to be darker once a projector texture is assigned; you can increase Energy to compensate for this.

Omni light projector textures require a special 360° panorama mapping, similar to PanoramaSkyMaterial textures.

With the projector texture below, the following result is obtained:

If you've acquired omni projectors in the form of cubemap images, you can use this web-based conversion tool to convert them to a single panorama image.

Spot lights are similar to omni lights, except they emit light only into a cone (or "cutoff"). They are useful to simulate flashlights, car lights, reflectors, spots, etc. This type of light is also attenuated towards the opposite direction it points to.

Spot lights share the same Range, Attenuation and Size as OmniLight3D, and add two extra parameters:

Angle: The aperture angle of the light.

Angle Attenuation: The cone attenuation, which helps soften the cone borders.

Spots feature the same parameters as omni lights for shadow mapping. Rendering spot shadow maps is significantly faster compared to omni lights, as only one shadow texture needs to be rendered (instead of rendering 6 faces, or 2 in dual paraboloid mode).

Spot lights with shadows enabled can make use of projectors. The projector texture will multiply the light's color by the color at a given point on the texture. As a result, lights will usually appear to be darker once a projector texture is assigned; you can increase Energy to compensate for this.

Unlike omni light projectors, a spot light projector texture doesn't need to follow a special format to look correct. It will be mapped in a way similar to a decal.

With the projector texture below, the following result is obtained:

Spot lights with wide angles will have lower-quality shadows than spot lights with narrow angles, as the shadow map is spread over a larger surface. At angles wider than 89 degrees, spot light shadows will stop working entirely. If you need shadows for wider lights, use an omni light instead.

Unlike Directional lights, which have their own shadow texture, omni and spot lights are assigned to slots of a shadow atlas. This atlas can be configured in the advanced Project Settings (Rendering > Lights And Shadows > Positional Shadow).

The resolution applies to the whole shadow atlas. This atlas is divided into four quadrants:

Each quadrant can be subdivided to allocate any number of shadow maps; the following is the default subdivision:

The shadow atlas allocates space as follows:

The biggest shadow map size (when no subdivision is used) represents a light the size of the screen (or bigger).

Subdivisions (smaller maps) represent shadows for lights that are further away from view and proportionally smaller.

Every frame, the following procedure is performed for all lights:

Check if the light is on a slot of the right size. If not, re-render it and move it to a larger/smaller slot.

Check if any object affecting the shadow map has changed. If it did, re-render the light.

If neither of the above has happened, nothing is done, and the shadow is left untouched.

If the slots in a quadrant are full, lights are pushed back to smaller slots, depending on size and distance. If all slots in all quadrants are full, some lights will not be able to render shadows even if shadows are enabled on them.

The default shadow allocation strategy allows rendering up to 88 lights with shadows enabled in the camera frustum (4 + 4 + 16 + 64):

The first and most detailed quadrant can store 4 shadows.

The second quadrant can store 4 other shadows.

The third quadrant can store 16 shadows, with less detail.

The fourth and least detailed quadrant can store 64 shadows, with even less detail.

Using a higher number of shadows per quadrant allows supporting a greater amount of total lights with shadows enabled, while also improving performance (as shadows will be rendered at a lower resolution for each light). However, increasing the number of shadows per quadrant comes at the cost of lower shadow quality.

In some cases, you may want to use a different allocation strategy. For example, in a top-down game where all lights are around the same size, you may want to set all quadrants to have the same subdivision so that all lights have shadows of similar quality level.

Shadow rendering is a critical topic in 3D rendering performance. It's important to make the right choices here to avoid creating bottlenecks.

Directional shadow quality settings can be changed at runtime by calling the appropriate RenderingServer methods.

Positional (omni/spot) shadow quality settings can be changed at runtime on the root Viewport.

High shadow resolutions result in sharper shadows, but at a significant performance cost. It should also be noted that sharper shadows are not always more realistic. In most cases, this should be kept at its default value of 4096 or decreased to 2048 for low-end GPUs.

If positional shadows become too blurry after decreasing the shadow map size, you can counteract this by adjusting the shadow atlas quadrants to contain fewer shadows. This will allow each shadow to be rendered at a higher resolution.

Several shadow map quality settings can be chosen here. The default Soft Low is a good balance between performance and quality for scenes with detailed textures, as the texture detail will help make the dithering pattern less noticeable.

However, in projects with less detailed textures, the shadow dithering pattern may be more visible. To hide this pattern, you can either enable Temporal antialiasing (TAA), AMD FidelityFX Super Resolution 2.2 (FSR2), Fast approximate antialiasing (FXAA), or increase the shadow filter quality to Soft Medium or higher.

The Soft Very Low setting will automatically decrease shadow blur to make artifacts from the low sample count less visible. Conversely, the Soft High and Soft Ultra settings will automatically increase shadow blur to better make use of the increased sample count.

By default, Godot uses 16-bit depth textures for shadow map rendering. This is recommended in most cases as it performs better without a noticeable difference in quality.

If 16 Bits is disabled, 32-bit depth textures will be used instead. This can result in less artifacting in large scenes and large lights with shadows enabled. However, the difference is often barely visible, yet this can have a significant performance cost.

OmniLight3D and SpotLight3D offer several properties to hide distant lights. This can improve performance significantly in large scenes with dozens of lights or more.

Enabled: Controls whether distance fade (a form of LOD) is enabled. The light will fade out over Begin + Length, after which it will be culled and not sent to the shader at all. Use this to reduce the number of active lights in a scene and thus improve performance.

Begin: The distance from the camera at which the light begins to fade away (in 3D units).

Shadow: The distance from the camera at which the shadow begins to fade away (in 3D units). This can be used to fade out shadows sooner compared to the light, further improving performance. Only available if shadows are enabled for the light.

Length: The distance over which the light and shadow fades (in 3D units). The light becomes slowly more transparent over this distance and is completely invisible at the end. Higher values result in a smoother fade-out transition, which is more suited when the camera moves fast.

Percentage-closer soft shadows (PCSS) provide a more realistic shadow mapping appearance, with the penumbra size varying depending on the distance between the caster and the surface receiving the shadow. This comes at a high performance cost, especially for directional lights.

To avoid performance issues, it's recommended to:

Only use a handful of lights with PCSS shadows enabled at a given time. The effect is generally most visible on large, bright lights. Secondary light sources that are more faint usually don't benefit much from using PCSS shadows.

Provide a setting for users to disable PCSS shadows. On directional lights, this can be done by setting the DirectionalLight3D's light_angular_distance property to 0.0 in a script. On positional lights, this can be done by setting the OmniLight3D or SpotLight3D's light_size property to 0.0 in a script.

The way projectors are rendered also has an impact on performance. The Rendering > Textures > Light Projectors > Filter advanced project setting lets you control how projector textures should be filtered. Nearest/Linear do not use mipmaps, which makes them faster to render. However, projectors will look grainy at distance. Nearest/Linear Mipmaps will look smoother at a distance, but projectors will look blurry when viewed from oblique angles. This can be resolved by using Nearest/Linear Mipmaps Anisotropic, which is the highest-quality mode, but also the most expensive.

If your project has a pixel art style, consider setting the filter to one of the Nearest values so that projectors use nearest-neighbor filtering. Otherwise, stick to Linear.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## 3D Particle attractors — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/particles/attractors.html

**Contents:**
- 3D Particle attractors
- Common properties
- Box attractors
- Sphere attractors
- Vector field attractors
- User-contributed notes

Particle attractors are nodes that apply a force to all particles within their reach. They pull particles closer or push them away based on the direction of that force. There are three types of attractors: GPUParticlesAttractorBox3D, GPUParticlesAttractorSphere3D, and GPUParticlesAttractorVectorField3D. You can instantiate them at runtime and change their properties from gameplay code; you can even animate and combine them for complex attraction effects.

Particle attractors are not yet implemented for 2D particle systems.

The first thing you have to do if you want to use attractors is enable the Attractor Interaction property on the ParticleProcessMaterial. Do this for every particle system that needs to react to attractors. Like most properties in Godot, you can also change this at runtime.

Common attractor properties

There are some properties that you can find on all attractors. They're located in the GPUParticlesAttractor3D section in the inspector.

Strength controls how strong the attractor force is. A positive value pulls particles closer to the attractor's center, while a negative value pushes them away.

Attenuation controls the strength falloff within the attractor's influence region. Every particle attractor has a boundary. Its strength is weakest at the border of this boundary and strongest at its center. Particles outside of the boundary are not affected by the attractor at all. The attenuation curve controls how the strength weakens over that distance. A straight line means that the strength is proportional to the distance: if a particle is halfway between the boundary and the center, the attractor strength will be half of what it is at the center. Different curve shapes change how fast particles accelerate towards the attractor.

Strength increase variations: constantly over the distance to the attractor (left), fast at the boundary border and slowly at the center (middle), slowly at the boundary and fast at the center (right).

The Directionality property changes the direction towards which particles are pulled. At a value of 0.0, there is no directionality, which means that particles are pulled towards the attractor's center. At 1.0, the attractor is fully directional, which means particles will be pulled along the attractor's local -Z-axis. You can change the global direction by rotating the attractor. If Strength is negative, particles are instead pulled along the +Z-axis.

No directionality (left) vs. full directionality (right). Notice how the particles move along the attractor's local Z-axis.

The Cull Mask property controls which particle systems are affected by an attractor based on each system's visibility layers. A particle system is only affected by an attractor if at least one of the system's visibility layers is enabled in the attractor's cull mask.

There is a known issue with GPU particle attractors that prevent the cull mask from working properly in Godot 4.0. We will update the documentation as soon as it is fixed.

Box attractor in the node list

Box attractors have a box-shaped influence region. You control their size with the Extents property. Box extents always measure half of the sides of its bounds, so a value of (X=1.0,Y=1.0,Z=1.0) creates a box with an influence region that is 2 meters wide on each side.

To create a box attractor, add a new child node to your scene and select GPUParticlesAttractorBox3D from the list of available nodes. You can animate the box position or attach it to a moving node for more dynamic effects.

A box attractor with a negative strength value parts a particle field as it moves through it.

Sphere attractor in the node list

Sphere attractors have a spherical influence region. You control their size with the Radius property. While box attractors don't have to be perfect cubes, sphere attractors will always be spheres: You can't set width independently from height. If you want to use a sphere attractor for elongated shapes, you have to change its Scale in the attractor's Node3D section.

To create a sphere attractor, add a new child node to your scene and select GPUParticlesAttractorSphere3D from the list of available nodes. You can animate the sphere position or attach it to a moving node for more dynamic effects.

A sphere attractor with a negative strength value parts a particle field as it moves through it.

Vector field attractor in the node list

A vector field is a 3D area that contains vectors positioned on a grid. The grid density controls how many vectors there are and how far they're spread apart. Each vector in a vector field points in a specific direction. This can be completely random or aligned in a way that forms distinct patterns and paths.

When particles interact with a vector field, their movement direction changes to match the nearest vector in the field. As a particle moves closer to the next vector in the field, it changes direction to match that vector's direction. The particle's speed depends on the vector's length.

Like box attractors, vector field attractors have a box-shaped influence region. You control their size with the Extents property, where a value of (X=1.0,Y=1.0,Z=1.0) creates a box with an influence region that is 2 meters wide on each side. The Texture property takes a 3D texture where every pixel represents a vector with the pixel's color interpreted as the vector's direction and size.

When a texture is used as a vector field, there are two types of conversion you need to be aware of:

The texture coordinates map to the attractor bounds. The image below shows which part of the texture corresponds to which part of the vector field volume. For example, the bottom half of the texture affects the top half of the vector field attractor because +Y points down in the texture UV space, but up in Godot's world space.

The pixel color values map to direction vectors in space. The image below provides an overview. Since particles can move in two directions along each axis, the lower half of the color range represents negative direction values while the upper half represents positive direction values. So a yellow pixel (R=1,G=1,B=0) maps to the vector (X=1,Y=1,Z=-1) while a neutral gray (R=0.5,G=0.5,B=0.5) results in no movement at all.

To create a vector field attractor, add a new child node to your scene and select GPUParticlesAttractorVectorField3D from the list of available nodes. You can animate the attractor's position or attach it to a moving node for more dynamic effects.

If you don't have external tools to create vector field textures, you can use a NoiseTexture3D with a Color Ramp attached as a vector field texture. The Color Ramp can be modified to adjust how much each coordinate is affected by the vector field.

Two particle systems are affected by the same vector field attractor. Click here to download the 3D texture.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## 3D Particle collisions — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/particles/collision.html

**Contents:**
- 3D Particle collisions
- Common properties
- Box collision
- Sphere collision
- Height field collision
- SDF collision
- Troubleshooting
- User-contributed notes

Since GPU particles are processed entirely on the GPU, they don't have access to the game's physical world. If you need particles to collide with the environment, you have to set up particle collision nodes. There are four of them: GPUParticlesCollisionBox3D, GPUParticlesCollisionSphere3D, GPUParticlesCollisionSDF3D, and GPUParticlesCollisionHeightField3D.

Common collision properties

There are some properties that you can find on all collision nodes. They're located in the GPUParticlesCollision3D section in the inspector.

The Cull Mask property controls which particle systems are affected by a collision node based on each system's visibility layers. A particle system collides with a collision node only if at least one of the system's visibility layers is enabled in the collider's cull mask.

There is a known issue with GPU particle collision that prevent the cull mask from working properly in Godot 4.0. We will update the documentation as soon as it is fixed.

Box collision in the node list

Box collision nodes are shaped like a solid, rectangular box. You control their size with the Extents property. Box extents always measure half of the sides of its bounds, so a value of (X=1.0,Y=1.0,Z=1.0) creates a box that is 2 meters wide on each side. Box collision nodes are useful for simulating floor and wall geometry that particles should collide against.

To create a box collision node, add a new child node to your scene and select GPUParticlesCollisionBox3D from the list of available nodes. You can animate the box position or attach it to a moving node for more dynamic effects.

Two particle systems collide with a box collision node

Sphere collision in the node list

Sphere collision nodes are shaped like a solid sphere. The Radius property controls the size of the sphere. While box collision nodes don't have to be perfect cubes, sphere collision nodes will always be spheres. If you want to set width independently from height, you have to change the Scale property in the Node3D section.

To create a sphere collision node, add a new child node to your scene and select GPUParticlesCollisionSphere3D from the list of available nodes. You can animate the sphere's position or attach it to a moving node for more dynamic effects.

Two particle systems collide with a sphere collision node

Height field collision in the node list

Height field particle collision is very useful for large outdoor areas that need to collide with particles. At runtime, the node creates a height field from all the meshes within its bounds that match its cull mask. Particles collide against the mesh that this height field represents. Since the height field generation is done dynamically, it can follow the player camera around and react to changes in the level. Different settings for the height field density offer a wide range of performance adjustments.

To create a height field collision node, add a new child node to your scene and select GPUParticlesCollisionHeightField3D from the list of available nodes.

A height field collision node is shaped like a box. The Extents property controls its size. Extents always measure half of the sides of its bounds, so a value of (X=1.0,Y=1.0,Z=1.0) creates a box that is 2 meters wide on each side. Anything outside of the node's extents is ignored for height field creation.

The Resolution property controls how detailed the height field is. A lower resolution performs faster at the cost of accuracy. If the height field resolution is too low, it may look like particles penetrate level geometry or get stuck in the air during collision events. They might also ignore some smaller meshes completely.

At low resolutions, height field collision misses some finer details (left)

The Update Mode property controls when the height field is recreated from the meshes within its bounds. Set it to When Moved to make it refresh only when it moves. This performs well and is suited for static scenes that don't change very often. If you need particles to collide with dynamic objects that change position frequently, you can select Always to refresh every frame. This comes with a cost to performance and should only be used when necessary.

It's important to remember that when Update Mode is set to When Moved, it is the height field node whose movement triggers an update. The height field is not updated when one of the meshes inside it moves.

The Follow Camera Enabled property makes the height field follow the current camera when enabled. It will update whenever the camera moves. This property can be used to make sure that there is always particle collision around the player while not wasting performance on regions that are out of sight or too far away.

SDF collision in the node list

SDF collision nodes create a signed distance field that particles can collide with. SDF collision is similar to height field collision in that it turns multiple meshes within its bounds into a single collision volume for particles. A major difference is that signed distance fields can represent holes, tunnels and overhangs, which is impossible to do with height fields alone. The performance overhead is larger compared to height fields, so they're best suited for small-to-medium-sized environments.

To create an SDF collision node, add a new child node to your scene and select GPUParticlesCollisionSDF3D from the list of available nodes. SDF collision nodes have to be baked in order to have any effect on particles in the level. To do that, click the Bake SDF button in the viewport toolbar while the SDF collision node is selected and choose a directory to store the baked data. Since SDF collision needs to be baked in the editor, it's static and cannot change at runtime.

SDF particle collision allows for very detailed 3-dimensional collision shapes

An SDF collision node is shaped like a box. The Extents property controls its size. Extents always measure half of the sides of its bounds, so a value of (X=1.0,Y=1.0,Z=1.0) creates a box that is 2 meters wide on each side. Anything outside of the node's extents is ignored for collision.

The Resolution property controls how detailed the distance field is. A lower resolution performs faster at the cost of accuracy. If the resolution is too low, it may look like particles penetrate level geometry or get stuck in the air during collision events. They might also ignore some smaller meshes completely.

The same area covered by a signed distance field at different resolutions: 16 (left) and 256 (right)

The Thickness property gives the distance field, which is usually hollow on the inside, a thickness to prevent particles from penetrating at high speeds. If you find that some particles don't collide with the level geometry and instead shoot right through it, try setting this property to a higher value.

The Bake Mask property controls which meshes will be considered when the SDF is baked. Only meshes that render on the active layers in the bake mask contribute to particle collision.

For particle collision to work, the particle's visibility AABB must overlap with the collider's AABB. If collisions appear to be not working despite colliders being set up, generate an updated visibility AABB by selecting the GPUParticles3D node and choosing GPUParticles3D > Generate Visibility AABB… at the top of the 3D editor viewport.

If the particles move fast and colliders are thin. There are two solutions for this:

Make the colliders thicker. For instance, if particles cannot get below a solid floor, you could make the collider representing the floor thicker than its actual visual representation. The heightfield collider automatically handles this by design, as heightfields cannot represent "room over room" collision.

Increased Fixed FPS in the GPUParticles3D node, which will perform collision checks more often. This comes at a performance cost, so avoid setting this too high.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## 3D Particle system properties — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/particles/properties.html

**Contents:**
- 3D Particle system properties
- Emitter properties
- Time properties
- Collision properties
- Drawing properties
- Trail properties
- User-contributed notes

The checkbox next to the Emitting property activates and deactivates the particle system. Particles will only be processed and rendered if the box is checked. You can set this property at runtime if you want to activate or deactivate particle systems dynamically.

The Amount property controls the maximum number of particles visible at any given time. Increase the value to spawn more particles at the cost of performance.

The Amount Ratio property is the ratio of particles compared to the amount that will be emitted. If it's less than 1.0, the amount of particles emitted through the lifetime will be the Amount * Amount Ratio. Changing this value while emitted doesn't affect already created particles and doesn't cause the particle system to restart. It's useful for making effects where the number of emitted particles varies over time.

You can set another particle node as a Sub Emitter, which will be spawned as a child of each particle. See the Sub-emitters section in this manual for a detailed explanation of how to add a sub-emitter to a particle system.

The Lifetime property controls how long each particle exists before it disappears again. It is measured in seconds. A lot of particle properties can be set to change over the particle's lifetime and blend smoothly from one value to another.

Lifetime and Amount are related. They determine the particle system's emission rate. Whenever you want to know how many particles are spawned per second, this is the formula you would use:

Example: Emitting 32 particles with a lifetime of 4 seconds each would mean the system emits 8 particles per second.

The Interp to End property causes all the particles in the node to interpolate towards the end of their lifetime.

If the checkbox next to the One Shot property is checked, the particle system will emit amount particles and then disable itself. It "runs" only once. This property is unchecked by default, so the system will keep emitting particles until it is disabled or destroyed manually. One-shot particles are a good fit for effects that react to a single event, like item pickups or splinters that burst away when a bullet hits a wall.

The Preprocess property is a way to fast-forward to a point in the middle of the particle system's lifetime and start rendering from there. It is measured in seconds. A value of 1 means that when the particle system starts, it will look as if it has been running for one second already.

This can be useful if you want the particle system to look like it has been active for a while even though it was just loaded into the scene. Consider the example below. Both particle systems simulate dust flying around in the area. With a preprocess value of 0, there wouldn't be any dust for the first couple of seconds because the system has not yet emitted enough particles for the effect to become noticeable. This can be seen in the video on the left. Compare that to the video on the right where the particle system is preprocessed for 4 seconds. The dust is fully visible from the very beginning because we skipped the first four seconds of "setup" time.

No preprocess (left) vs. 4 seconds of preprocess (right)

You can slow down or speed up the particle system with the Speed Scale property. This applies to processing the data as well as rendering the particles. Set it to 0 to pause the particle system completely or set it to something like 2 to make it move twice as fast.

Different speed scale values: 0.1 (left), 0.5 (middle), 1.0 (right)

The Explosiveness property controls whether particles are emitted sequentially or simultaneously. A value of 0 means that particles emit one after the other. A value of 1 means that all amount particles emit at the same time, giving the effect a more "explosive" appearance.

The Randomness property adds some randomness to the particle emission timing. When set to 0, there is no randomness at all and the interval between the emission of one particle and the next is always the same: the particles are emitted at regular intervals. A Randomness value of 1 makes the interval completely random. You can use this property to break up some of the uniformity in your effects. When Explosiveness is set to 1, this property has no effect.

Interpolation off (left) vs. on (right)

The Fixed FPS property limits how often the particle system is processed. This includes property updates as well as collision and attractors. This can improve performance a lot, especially in scenes that make heavy use of particle collision. Note that this does not change the speed at which particles move or rotate. You would use the Speed Scale property for that.

When you set Fixed FPS to very low values, you will notice that the particle animation starts to look choppy. This can sometimes be desired if it fits the art direction, but most of the time, you'll want particle systems to animate smoothly. That's what the Interpolate property does. It blends particle properties between updates so that even a particle system running at 10 FPS appears as smooth as running at 60.

When using particle collision, tunneling can occur if the particles move fast and colliders are thin. This can be remedied by increasing Fixed FPS (at a performance cost).

Setting up particle collision requires following further steps described in 3D Particle collisions.

The Base Size property defines each particle's default collision size, which is used to check whether a particle is currently colliding with the environment. You would usually want this to be about the same size as the particle. It can make sense to increase this value for particles that are very small and move very fast to prevent them from clipping through the collision geometry.

The Visibility AABB property defines a box around the particle system's origin. As long as any part of this box is in the camera's field of view, the particle system is visible. As soon as it leaves the camera's field of view, the particle system stops being rendered at all. You can use this property to boost performance by keeping the box as small as possible.

One thing to keep in mind when you set a size for the Visibility AABB is that particles that are outside of its bounds disappear instantly when it leaves the camera's field of view. Particle collision will also not occur outside the Visibility AABB. While not technically a bug, this can have a negative effect on the visual experience.

When the Local Coords property is checked, all particle calculations use the local coordinate system to determine things like up and down, gravity, and movement direction. Up and down, for example, would follow the particle system's or its parent node's rotation. When the property is unchecked, the global world space is used for these calculations: Down will always be -Y in world space, regardless of the particle system's rotation.

Local space coordinates (left) vs. world space coordinates (right)

The Draw Order property controls the order in which individual particles are drawn. Index means that they are drawn in the order of emission: particles that are spawned later are drawn on top of earlier ones. Lifetime means that they are drawn in the order of their remaining lifetime. Reverse Lifetime reverses the Lifetime draw order. View Depth means particles are drawn according to their distance from the camera: The ones closer to the camera on top of those farther away.

The Transform Align property controls the particle's default rotation. Disabled means they don't align in any particular way. Instead, their rotation is determined by the values set in the process material. Z-Billboard means that the particles will always face the camera. This is similar to the Billboard property in the Standard Material. Y to Velocity means that each particle's Y-axis aligns with its movement direction. This can be useful for things like bullets or arrows, where you want particles to always point "forward". Z-Billboard + Y to Velocity combines the previous two modes. Each particle's Z-axis will point towards the camera while its Y-axis will align with their velocity.

Particle trail properties

The Enabled property controls whether particles are rendered as trails. The box needs to be checked if you want to make use of particle trails.

The Length Secs property controls for how long a trail should be emitted. The longer this duration is, the longer the trail will be.

See the Particle trails section in this manual for a detailed explanation of how particle trails work and how to set them up.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## 3D Particle trails — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/particles/trails.html

**Contents:**
- 3D Particle trails
- Ribbon trails
- Tube trails
- User-contributed notes

Setting up particle trails

Godot provides several types of trails you can add to a particle system. Before you can work with trails, you need to set up a couple of parameters first. Create a new particle system and assign a process material as described before. In the Trails group of the particle system, check the box next to Enabled and increase the emission duration by setting Lifetime to something like 0.8. On the process material, set Direction to (X=0,Y=1.0,Z=0) and Initial Velocity to 10.0 for both Min and Max.

The only thing that's still missing is a mesh for the draw pass. The type of mesh that you set here controls what kind of particle trail you will end up with.

Important ribbon mesh parameters

The simplest type of particle trail is the ribbon trail. Navigate to the Draw Passes section and select New RibbonTrailMesh from the options for Pass 1. A RibbonTrailMesh is a simple quad that is divided into sections and then stretched and repeated along those sections.

Assign a new Standard Material to the Material property and enable Use Particle Trails in the Transform property group. The particles should now be emitting in trails.

You have two options for the ribbon mesh Shape parameter. Cross creates two perpendicular quads, making the particle trail a little more three-dimensional. This really only makes sense if you don't draw the trails in Particle Billboard mode and helps when looking at the particles from different angles. The Flat option limits the mesh to a single quad and works best with billboard particles.

The Size parameter controls the trail's width. Use it to make trails wider or more narrow.

Sections, Section Length and Section Segments all work together to control how smooth the particle trail looks. When a particle trail does not travel in a straight line, the more sections it has the smoother it looks as it bends and swirls. Section Length controls the length of each section. Multiply this value by the number of sections to know the trail's total length.

3 sections, 1m section length (left) vs. 12 sections, 0.25m section length (right). Notice how the total length of the trails stays the same.

The Section Segments parameter further subdivides each section into segments. It has no effect on the smoothness of the trail's sections, though. Instead, it controls the smoothness of the particle trail's overall shape. The Curve property defines this shape. Click the box next to Curve and assign or create a new curve. The trail will be shaped just like the curve with the curve's value at 0.0 at the trail's head and the curve's value at 1.0 at the trail's tail.

Particle trails shaped by different curves. The trails move from left to right.

Depending on the complexity of the curve, the particle trail's shape will not look very smooth when the number of sections is low. This is where the Section Segments property comes in. Increasing the amount of section segments adds more vertices to the trail's sides so that it can follow the curve more closely.

Particle trail shape smoothness: 1 segment per section (top), 12 segments per section (bottom)

Tube trails share a lot of their properties with ribbon trails. The big difference between them is that tube trails emit cylindrical meshes instead of quads.

Tube trails emit cylindrical particles

To create a tube trail, navigate to the Draw Passes section and select New TubeTrailMesh from the options for Pass 1. A TubeTrailMesh is a cylinder that is divided into sections and then stretched and repeated along those sections. Assign a new Standard Material to the Material property and enable Use Particle Trails in the Transform property group. The particles should now be emitting in long, cylindrical trails.

Important tube mesh parameters

The Radius and Radial Steps properties are to tube trails what Size is to ribbon trails. Radius defines the radius of the tube and increases or decreases its overall size. Radial Steps controls the number of sides around the tube's circumference. A higher value increases the resolution of the tube's cap.

Sections and Section Length work the same for tube trails and ribbon trails. They control how smooth the tube trail looks when it is bending and twisting instead of moving in a straight line. Increasing the number of sections will make it look smoother. Change the Section Length property to change the length of each section and with it the total length of the trail. Section Rings is the tube equivalent of the Section Segments property for ribbons. It subdivides the sections and adds more geometry to the tube to better fit the custom shape defined in the Curve property.

You can shape tube trails with curves, just as you can with ribbon trails. Click the box next to the Curve property and assign or create a new curve. The trail will be shaped like the curve with the curve's value at 0.0 at the trail's head and the curve's value at 1.0 at the trail's tail.

Particle tube trails with a custom curve shape: 4 radial steps, 3 sections, 1 section ring (left), 12 radial steps, 9 sections, 3 section rings (right)

An important property you might want to set is Transform Align in the particle system's Drawing group. If you leave it as is, the tubes will not preserve volume; they flatten out as they move because their Y-axis keeps pointing up even as they change direction. This can cause a lot of rendering artifacts. Set the property to Y to Velocity instead and each particle trail keeps its Y-axis aligned along the direction of its movement.

Particle tube trails without alignment (left) and with Y-axis aligned to velocity (right)

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## 3D rendering limitations — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/3d_rendering_limitations.html

**Contents:**
- 3D rendering limitations
- Introduction
- Texture size limits
- Color banding
- Depth buffer precision
- Transparency sorting
- User-contributed notes

Due to their focus on performance, real-time rendering engines have many limitations. Godot's renderer is no exception. To work effectively with those limitations, you need to understand them.

On desktops and laptops, textures larger than 8192×8192 may not be supported on older devices. You can check your target GPU's limitations on GPUinfo.org.

Mobile GPUs are typically limited to 4096×4096 textures. Also, some mobile GPUs don't support repeating non-power-of-two-sized textures. Therefore, if you want your texture to display correctly on all platforms, you should avoid using textures larger than 4096×4096 and use a power of two size if the texture needs to repeat.

To limit the size of a specific texture that may be too large to render, you can set the Process > Size Limit import option to a value greater than 0. This will reduce the texture's dimensions on import (preserving aspect ratio) without affecting the source file.

When using the Forward+ or Mobile rendering methods, Godot's 3D engine renders internally in HDR. However, the rendering output will be tonemapped to a low dynamic range so it can be displayed on the screen. This can result in visible banding, especially when using untextured materials. For performance reasons, color precision is also lower when using the Mobile rendering method compared to Forward+.

When using the Compatibility rendering method, HDR is not used and the color precision is the lowest of all rendering methods. This also applies to 2D rendering, where banding may be visible when using smooth gradient textures.

There are two main ways to alleviate banding:

If using the Forward+ or Forward Mobile rendering methods, enable Use Debanding in Project Settings > Rendering > Anti Aliasing. This applies a fullscreen debanding shader as a post-processing effect and is very cheap.

Alternatively, bake some noise into your textures. This is mainly effective in 2D, e.g. for vignetting effects. In 3D, you can also use a custom debanding shader to be applied on your materials. This technique works even if your project is rendered with low color precision, which means it will work when using the Mobile and Compatibility rendering methods.

Color banding comparison (contrast increased for more visibility)

See Banding in Games: A Noisy Rant (PDF) for more details about banding and ways to combat it.

To sort objects in 3D space, rendering engines rely on a depth buffer (also called Z-buffer). This buffer has a finite precision: 24-bit on desktop platforms, sometimes 16-bit on mobile platforms (for performance reasons). If two different objects end up on the same buffer value, then Z-fighting will occur. This will materialize as textures flickering back and forth as the camera moves or rotates.

To make the depth buffer more precise over the rendered area, you should increase the Camera node's Near property. However, be careful: if you set it too high, players will be able to see through nearby geometry. You should also decrease the Camera node's Far property to the lowest permissible value for your use case, though keep in mind it won't impact precision as much as the Near property.

If you only need high precision when the player can see far away, you could change it dynamically based on the game conditions. For instance, if the player enters an airplane, the Near property can be temporarily increased to avoid Z-fighting in the distance. It can then be decreased once the player leaves the airplane.

Depending on the scene and viewing conditions, you may also be able to move the Z-fighting objects further apart without the difference being visible to the player.

Z-fighting comparison (before and after tweaking the scene by offsetting the Label3D away from the floor)

In Godot, transparent materials are drawn after opaque materials. Transparent objects are sorted back to front before being drawn based on the Node3D's position, not the vertex position in world space. Due to this, overlapping objects may often be sorted out of order. To fix improperly sorted objects, tweak the material's Render Priority property or the node's Sorting Offset. Render Priority will force specific materials to appear in front of or behind other transparent materials, while Sorting Offset will move the object forward or backward for the purpose of sorting. Even then, these may not always be sufficient.

Some rendering engines feature order-independent transparency techniques to alleviate this, but this is costly on the GPU. Godot currently doesn't provide this feature. There are still several ways to avoid this problem:

Only make materials transparent if you actually need it. If a material only has a small transparent part, consider splitting it into a separate material. This will allow the opaque part to cast shadows and will also improve performance.

If your texture mostly has fully opaque and fully transparent areas, you can use alpha testing instead of alpha blending. This transparency mode is faster to render and doesn't suffer from transparency issues. Enable Transparency > Transparency to Alpha Scissor in StandardMaterial3D, and adjust Transparency > Alpha Scissor Threshold accordingly if needed. Note that MSAA will not antialias the texture's edges unless alpha antialiasing is enabled in the material's properties. However, FXAA, TAA and supersampling will be able to antialias the texture's edges regardless of whether alpha antialiasing is enabled on the material.

If you need to render semi-transparent areas of the texture, alpha scissor isn't suitable. Instead, setting the StandardMaterial3D's Transparency > Transparency property to Depth Pre-Pass can sometimes work (at a performance cost). You can also try the Alpha Hash mode.

If you want a material to fade with distance, use the StandardMaterial3D distance fade mode Pixel Dither or Object Dither instead of Pixel Alpha. This will make the material opaque, which also speeds up rendering.

Transparency sorting comparison (alpha-blended materials on the left, alpha scissor materials on the right)

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## 3D text — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/3d_text.html

**Contents:**
- 3D text
- Introduction
- Label3D
  - Advantages
  - Limitations
- TextMesh
  - Advantages
  - Limitations
- Projected Label node (or any other Control)
  - Advantages

In a project, there may be times when text needs to be created as part of a 3D scene and not just in the HUD. Godot provides 2 methods to do this: the Label3D node and the TextMesh resource for a MeshInstance3D node.

Additionally, Godot makes it possible to position Control nodes according to a 3D point's position on the camera. This can be used as an alternative to "true" 3D text in situations where Label3D and TextMesh aren't flexible enough.

You can see 3D text in action using the 3D Labels and Texts demo project.

This page does not cover how to display a GUI scene within a 3D environment. For information on how to achieve that, see the GUI in 3D demo project.

Label3D behaves like a Label node, but in 3D space. Unlike the Label node, this Label3D node does not inherit properties of a GUI theme. However, its look remains customizable and uses the same font subresource as Control nodes (including support for MSDF font rendering).

Label3D is faster to generate than TextMesh. While both use a caching mechanism to only render new glyphs once, Label3D will still be faster to (re)generate, especially for long text. This can avoid stuttering during gameplay on low-end CPUs or mobile.

Label3D can use bitmap fonts and dynamic fonts (with and without MSDF or mipmaps). This makes it more flexible on that aspect compared to TextMesh, especially for rendering fonts with self-intersecting outlines or colored fonts (emoji).

See Using Fonts for guidelines on configuring font imports.

By default, Label3D has limited interaction with a 3D environment. It can be occluded by geometry and lit by light sources if the Shaded flag is enabled. However, it will not cast shadows even if Cast Shadow is set to On in the Label3D's GeometryInstance3D properties. This is because the node internally generates a quad mesh (one glyph per quad) with transparent textures and has the same limitations as Sprite3D. Transparency sorting issues can also become apparent when several Label3Ds overlap, especially if they have outlines.

This can be mitigated by setting the Label3D's transparency mode to Alpha Cut, at the cost of less smooth text rendering. The Opaque Pre-Pass transparency mode can preserve text smoothness while allowing the Label3D to cast shadows, but some transparency sorting issues will remain.

See Transparency sorting section in the 3D rendering limitations page for more information.

Text rendering quality can also suffer when the Label3D is viewed at a distance. To improve text rendering quality, enable mipmaps on the font or switch the font to use MSDF rendering.

The TextMesh resource has similarities to Label3D. They both display text in a 3D scene, and will use the same font subresource. However, instead of generating transparent quads, TextMesh generates 3D geometry that represents the glyphs' contours and has the properties of a mesh. As a result, a TextMesh is shaded by default and automatically casts shadows onto the environment. A TextMesh can also have a material applied to it (including custom shaders).

Here is an example of a texture and how it's applied to the mesh. You can use the texture below as a reference for the generated mesh's UV map:

TextMesh has a few advantages over Label3D:

TextMesh can use a texture to modify text color on a per-side basis.

TextMesh geometry can have actual depth to it, giving glyphs a 3D look.

TextMesh can use custom shaders, unlike Label3D.

There are some limitations to TextMesh:

No built-in outline support, unlike Label3D. This can be simulated using custom shaders though.

Only dynamic fonts are supported (.ttf, .otf, .woff, .woff2). Bitmap fonts in the .fnt or .font formats are not supported.

Fonts with self-intersecting outlines will not render correctly. If you notice rendering issues on fonts downloaded from websites such as Google Fonts, try downloading the font from the font author's official website instead.

Antialiasing the text rendering requires a full-scene antialiasing method to be enabled such as MSAA, FXAA and temporal antialiasing (TAA). If no antialiasing method is enabled, text will appear grainy, especially at a distance. See 3D antialiasing for more information.

There is a last solution that is more complex to set up, but provides the most flexibility: projecting a 2D node onto 3D space. This can be achieved using the return value of unproject_position method on a Camera3D node in a script's _process() function. This return value should then be used to set the position property of a Control node.

See the 3D waypoints demo for an example of this.

Any Control node can be used, including Label, RichTextLabel or even nodes such as Button. This allows for powerful formatting and GUI interaction.

The script-based approach allows for complete freedom in positioning. For example, this makes it considerably easier to pin Controls to the screen's edges when they go off-screen (for in-game 3D markers).

Control theming is obeyed. This allows for easier customization that globally applies to the project.

Projected Controls cannot be occluded by 3D geometry in any way. You can use a RayCast to fully hide the control if its target position is occluded by a collider, but this doesn't allow for partially hiding the control behind a wall.

Changing text size depending on distance by adjusting the Control's scale property is possible, but it needs to be done manually. Label3D and TextMesh automatically take care of this, at the cost of less flexibility (can't set a minimum/maximum text size in pixels).

Handling resolution and aspect ratio changes must be taken into account in the script, which can be challenging.

In most scenarios, Label3D is recommended as it's easier to set up and provides higher rendering quality (especially if 3D antialiasing is disabled).

For advanced use cases, TextMesh is more flexible as it allows styling the text with custom shaders. Custom shaders allow for modifying the final geometry, such as curving the text along a surface. Since the text is actual 3D geometry, the text can optionally have depth to it and can also contribute to global illumination.

If you need features such as BBCode or Control theming support, then using a projected RichTextLabel node is the only way to go.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Advanced Import Settings — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/assets_pipeline/importing_3d_scenes/advanced_import_settings.html

**Contents:**
- Advanced Import Settings
- Using the Advanced Import Settings dialog
  - Configuring node import options
  - Configuring mesh and material import options
- Extracting materials to separate files
- Animation options
  - Optimizer
  - Save to file
  - Slices
- User-contributed notes

While the regular import panel provides many essential options for imported 3D models, the advanced import settings provides per object options, model previews, and animation previews. To open it select the Advanced... button at the bottom of the import dock.

This is available for 3D models imported as scenes, as well as animation libraries.

This page does not go over options also available in the import dock, or anything outside of the advanced import settings. For information on those please read the Import configuration page.

The first tab you'll see is the Scene tab. The options available in the panel on the right are identical to the Import dock, but you have access to a 3D preview. The 3D preview can be rotated by holding down the left mouse button then dragging the mouse. Zoom can be adjusted using the mouse wheel.

Advanced Import Settings dialog (Scene tab). Credit: Modern Arm Chair 01 - Poly Haven

You can select individual nodes that compose the scene while in the Scene tab using the tree view at the left:

Selecting a node in the Advanced Import Settings dialog (Materials tab)

This exposes several per-node import options:

Skip Import: If checked, the node will not be present in the final imported scene. Enabling this disables all other options.

Generate > Physics: If checked, generates a PhysicsBody3D parent node with collision shapes that are siblings to the MeshInstance3D node.

Generate > NavMesh: If checked, generates a NavigationRegion3D child node for navigation. Mesh + NavMesh will keep the original mesh visible, while NavMesh Only will only import the navigation mesh (without a visual representation). NavMesh Only is meant to be used when you've manually authored a simplified mesh for navigation.

Generate > Occluder: If checked, generates an OccluderInstance3D sibling node for occlusion culling using the mesh's geometry as a basis for the occluder's shape. Mesh + Occluder will keep the original mesh visible, while Occluder Only will only import the occluder (without a visual representation). Occluder Only is meant to be used when you've manually authored a simplified mesh for occlusion culling.

These options are only visible if some of the above options are enabled:

Physics > Body Type: Only visible if Generate > Physics is enabled. Controls the PhysicsBody3D that should be created. Static creates a StaticBody3D, Dynamic creates a RigidBody3D, Area creates an Area3D.

Physics > Shape Type: Only visible if Generate > Physics is enabled. Trimesh allows for precise per-triangle collision, but it can only be used with a Static body type. Other types are less precise and may require manual configuration, but can be used with any body type. For static level geometry, use Trimesh. For dynamic geometry, use primitive shapes if possible for better performance, or use one of the convex decomposition modes if the shape is large and complex.

Decomposition > Advanced: Only visible if Physics > Shape Type is Decompose Convex. If checked, allows adjusting advanced decomposition options. If disabled, only a preset Precision can be adjusted (which is usually sufficient).

Decomposition > Precision: Only visible if Physics > Shape Type is Decompose Convex. Controls the precision to use for convex decomposition. Higher values result in more detailed collision, at the cost of slower generation and increased CPU usage during physics simulation. To improve performance, it's recommended to keep this value as low as possible for your use cases.

Occluder > Simplification Distance: Only visible if Generate > Occluder is set to Mesh + Occluder or Occluder Only. Higher values result in an occluder mesh with fewer vertices (resulting in decreased CPU utilization), at the cost of more occlusion culling issues (such as false positives or false negatives). If you run into objects disappearing when they shouldn't when the camera is near a certain mesh, try decreasing this value.

In the Advanced Import Settings dialog, there are 2 ways to select individual meshes or materials:

Switch to the Meshes or Materials tab in the top-left corner of the dialog.

Stay in the Scene tab, but unfold the options on the tree view on the left. After choosing a mesh or material, this presents the same information as the Meshes and Materials tabs, but in a tree view instead of a list.

If you select a mesh, different options will appear in the panel on the right:

Advanced Import Settings dialog (Meshes tab)

The options are as follows:

Save to File: Saves the Mesh resource to an external file (this isn't a scene file). You generally don't need to use this for placing the mesh in a 3D scene – instead, you should instance the 3D scene directly. However, having direct access to the Mesh resource is useful for specific nodes, such as MeshInstance3D, MultiMeshInstance3D, GPUParticles3D or CPUParticles3D. - You will also need to specify an output file path using the option that appears after enabling Save to File. It's recommended to use the .res output file extension for smaller file sizes and faster loading speeds, as .tres is inefficient for writing large amounts of data.

Generate > Shadow Meshes: Per-mesh override for the Meshes > Create Shadow Meshes scene-wide import option described in Using the Import dock. Default will use the scene-wide import option, while Enable or Disable can forcibly enable or disable this behavior on a specific mesh.

Generate > Lightmap UV: Per-mesh override for the Meshes > Light Baking scene-wide import option described in Using the Import dock. Default will use the scene-wide import option, while Enable or Disable can forcibly enable or disable this behavior on a specific mesh. - Setting this to Enable on a scene with the Static light baking mode is equivalent to configuring this mesh to use Static Lightmaps. Setting this to Disable on a scene with the Static Lightmaps light baking mode is equivalent to configuring this mesh to use Static instead.

Generate > LODs: Per-mesh override for the Meshes > Generate LODs scene-wide import option described in Using the Import dock. Default will use the scene-wide import option, while Enable or Disable can forcibly enable or disable this behavior on a specific mesh.

LODs > Normal Merge Angle: The minimum angle difference between two vertices required to preserve a geometry edge in mesh LOD generation. If running into visual issues with LOD generation, decreasing this value may help (at the cost of less efficient LOD generation).

If you select a material, only one option will appear in the panel on the right:

Advanced Import Settings dialog (Materials tab)

When Use External is checked and an output path is specified, this lets you use an external material instead of the material that is included in the original 3D scene file; see the section below.

While Godot can import materials authored in 3D modeling software, the default configuration may not be suitable for your needs. For example:

You want to configure material features not supported by your 3D application.

You want to use a different texture filtering mode, as this option is configured in the material since Godot 4.0 (and not in the image).

You want to replace one of the materials with an entirely different material, such as a custom shader.

To be able to modify the 3D scene's materials in the Godot editor, you need to use external material resources.

In the top-left corner of the Advanced Import Settings dialog, choose Actions… > Extract Materials:

Extracting all built-in materials to external resources in the Advanced Import Settings dialog

After choosing this option, select a folder to extract material .tres files to, then confirm the extraction:

Confirming material extraction in the Advanced Import Settings subdialog

After extracting materials, the 3D scene will automatically be configured to use external material references. As a result, you don't need to manually enable Use External on every material to make the external .tres material effective.

When Use External is enabled, remember that the Advanced Import Settings dialog will keep displaying the mesh's original materials (the ones designed in the 3D modeling software). This means your customizations to the materials won't be visible within this dialog. To preview your modified materials, you need to place the imported 3D scene in another scene using the editor.

Godot will not overwrite changes made to extracted materials when the source 3D scene is reimported. However, if the material name is changed in the source 3D file, the link between the original material and the extracted material will be lost. As a result, you'll need to use the Advanced Import Settings dialog to associate the renamed material to the existing extracted material.

The above can be done in the dialog's Materials tab by selecting the material, enabling Save to File, then specifying the save path using the Path option that appears after enabling Save to File.

Several extra options are available for the generated AnimationPlayer nodes, as well as their individual animations when they're selected in the Scene tab.

When animations are imported, an optimizer is run, which reduces the size of the animation considerably. In general, this should always be turned on unless you suspect that an animation might be broken due to it being enabled.

By default, animations are saved as built-in. It is possible to save them to a file instead. This allows adding custom tracks to the animations and keeping them after a reimport.

It is possible to specify multiple animations from a single timeline as slices. For this to work, the model must have only one animation that is named default. To create slices, change the slice amount to something greater than zero. You can then name a slice, specify which frames it starts and stops on, and choose whether the animation loops or not.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Animating thousands of fish with MultiMeshInstance3D — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/performance/vertex_animation/animating_thousands_of_fish.html

**Contents:**
- Animating thousands of fish with MultiMeshInstance3D
- Animating one Fish
- Making a school of fish
- Animating a school of fish
- User-contributed notes

The content of this page was not yet updated for Godot 4.5 and may be outdated. If you know how to improve this page or you can confirm that it's up to date, feel free to open a pull request.

This tutorial explores a technique used in the game ABZU for rendering and animating thousands of fish using vertex animation and static mesh instancing.

In Godot, this can be accomplished with a custom Shader and a MultiMeshInstance3D. Using the following technique you can render thousands of animated objects, even on low-end hardware.

We will start by animating one fish. Then, we will see how to extend that animation to thousands of fish.

We will start with a single fish. Load your fish model into a MeshInstance3D and add a new ShaderMaterial.

Here is the fish we will be using for the example images, you can use any fish model you like.

The fish model in this tutorial is made by QuaterniusDev and is shared with a creative commons license. CC0 1.0 Universal (CC0 1.0) Public Domain Dedication https://creativecommons.org/publicdomain/zero/1.0/

Typically, you would use bones and a Skeleton3D to animate objects. However, bones are animated on the CPU and so you end having to calculate thousands of operations every frame and it becomes impossible to have thousands of objects. Using vertex animation in a vertex shader, you avoid using bones and can instead calculate the full animation in a few lines of code and completely on the GPU.

The animation will be made of four key motions:

A side to side motion

A pivot motion around the center of the fish

A panning wave motion

A panning twist motion

All the code for the animation will be in the vertex shader with uniforms controlling the amount of motion. We use uniforms to control the strength of the motion so that you can tweak the animation in editor and see the results in real time, without the shader having to recompile.

All the motions will be made using cosine waves applied to VERTEX in model space. We want the vertices to be in model space so that the motion is always relative to the orientation of the fish. For example, side-to-side will always move the fish back and forth in its left to right direction, instead of on the x axis in the world orientation.

In order to control the speed of the animation, we will start by defining our own time variable using TIME.

The first motion we will implement is the side to side motion. It can be made by offsetting VERTEX.x by cos of TIME. Each time the mesh is rendered, all the vertices will move to the side by the amount of cos(time).

The resulting animation should look something like this:

Next, we add the pivot. Because the fish is centered at (0, 0), all we have to do is multiply VERTEX by a rotation matrix for it to rotate around the center of the fish.

We construct a rotation matrix like so:

And then we apply it in the x and z axes by multiplying it by VERTEX.xz.

With only the pivot applied you should see something like this:

The next two motions need to pan down the spine of the fish. For that, we need a new variable, body. body is a float that is 0 at the tail of the fish and 1 at its head.

The next motion is a cosine wave that moves down the length of the fish. To make it move along the spine of the fish, we offset the input to cos by the position along the spine, which is the variable we defined above, body.

This looks very similar to the side to side motion we defined above, but in this one, by using body to offset cos each vertex along the spine has a different position in the wave making it look like a wave is moving along the fish.

The last motion is the twist, which is a panning roll along the spine. Similarly to the pivot, we first construct a rotation matrix.

We apply the rotation in the xy axes so that the fish appears to roll around its spine. For this to work, the fish's spine needs to be centered on the z axis.

Here is the fish with twist applied:

If we apply all these motions one after another, we get a fluid jelly-like motion.

Normal fish swim mostly with the back half of their body. Accordingly, we need to limit the panning motions to the back half of the fish. To do this, we create a new variable, mask.

mask is a float that goes from 0 at the front of the fish to 1 at the end using smoothstep to control the point at which the transition from 0 to 1 happens.

Below is an image of the fish with mask used as COLOR:

For the wave, we multiply the motion by mask which will limit it to the back half.

In order to apply the mask to the twist, we use mix. mix allows us to mix the vertex position between a fully rotated vertex and one that is not rotated. We need to use mix instead of multiplying mask by the rotated VERTEX because we are not adding the motion to the VERTEX we are replacing the VERTEX with the rotated version. If we multiplied that by mask, we would shrink the fish.

Putting the four motions together gives us the final animation.

Go ahead and play with the uniforms in order to alter the swim cycle of the fish. You will find that you can create a wide variety of swim styles using these four motions.

Godot makes it easy to render thousands of the same object using a MultiMeshInstance3D node.

A MultiMeshInstance3D node is created and used the same way you would make a MeshInstance3D node. For this tutorial, we will name the MultiMeshInstance3D node School, because it will contain a school of fish.

Once you have a MultiMeshInstance3D add a MultiMesh, and to that MultiMesh add your Mesh with the shader from above.

MultiMeshes draw your Mesh with three additional per-instance properties: Transform (rotation, translation, scale), Color, and Custom. Custom is used to pass in 4 multi-use variables using a Color.

instance_count specifies how many instances of the mesh you want to draw. For now, leave instance_count at 0 because you cannot change any of the other parameters while instance_count is larger than 0. We will set instance count in GDScript later.

transform_format specifies whether the transforms used are 3D or 2D. For this tutorial, select 3D.

For both color_format and custom_data_format you can choose between None, Byte, and Float. None means you won't be passing in that data (either a per-instance COLOR variable, or INSTANCE_CUSTOM) to the shader. Byte means each number making up the color you pass in will be stored with 8 bits while Float means each number will be stored in a floating-point number (32 bits). Float is slower but more precise, Byte will take less memory and be faster, but you may see some visual artifacts.

Now, set instance_count to the number of fish you want to have.

Next we need to set the per-instance transforms.

There are two ways to set per-instance transforms for MultiMeshes. The first is entirely in editor and is described in the MultiMeshInstance3D tutorial.

The second is to loop over all the instances and set their transforms in code. Below, we use GDScript to loop over all the instances and set their transform to a random position.

Running this script will place the fish in random positions in a box around the position of the MultiMeshInstance3D.

If performance is an issue for you, try running the scene with fewer fish.

Notice how all the fish are all in the same position in their swim cycle? It makes them look very robotic. The next step is to give each fish a different position in the swim cycle so the entire school looks more organic.

One of the benefits of animating the fish using cos functions is that they are animated with one parameter, time. In order to give each fish a unique position in the swim cycle, we only need to offset time.

We do that by adding the per-instance custom value INSTANCE_CUSTOM to time.

Next, we need to pass a value into INSTANCE_CUSTOM. We do that by adding one line into the for loop from above. In the for loop we assign each instance a set of four random floats to use.

Now the fish all have unique positions in the swim cycle. You can give them a little more individuality by using INSTANCE_CUSTOM to make them swim faster or slower by multiplying by TIME.

You can even experiment with changing the per-instance color the same way you changed the per-instance custom value.

One problem that you will run into at this point is that the fish are animated, but they are not moving. You can move them by updating the per-instance transform for each fish every frame. Although doing so will be faster than moving thousands of MeshInstance3Ds per frame, it'll still likely be slow.

In the next tutorial we will cover how to use GPUParticles3D to take advantage of the GPU and move each fish around individually while still receiving the benefits of instancing.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
//time_scale is a uniform float
float time = TIME * time_scale;
```

Example 2 (unknown):
```unknown
//side_to_side is a uniform float
VERTEX.x += cos(time) * side_to_side;
```

Example 3 (unknown):
```unknown
//angle is scaled by 0.1 so that the fish only pivots and doesn't rotate all the way around
//pivot is a uniform float
float pivot_angle = cos(time) * 0.1 * pivot;
mat2 rotation_matrix = mat2(vec2(cos(pivot_angle), -sin(pivot_angle)), vec2(sin(pivot_angle), cos(pivot_angle)));
```

Example 4 (unknown):
```unknown
VERTEX.xz = rotation_matrix * VERTEX.xz;
```

---

## Available 3D formats — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/assets_pipeline/importing_3d_scenes/available_formats.html

**Contents:**
- Available 3D formats
- Exporting glTF 2.0 files from Blender (recommended)
- Importing .blend files directly within Godot
- Exporting DAE files from Blender
- Importing OBJ files in Godot
- Importing FBX files in Godot
- User-contributed notes

When dealing with 3D assets, Godot has a flexible and configurable importer.

Godot works with scenes. This means that the entire scene being worked on in your favorite 3D modeling software will be transferred as close as possible.

Godot supports the following 3D scene file formats:

glTF 2.0 (recommended). Godot has support for both text (.gltf) and binary (.glb) formats.

.blend (Blender). This works by calling Blender to export to glTF in a transparent manner (requires Blender to be installed).

DAE (COLLADA), an older format that is supported.

OBJ (Wavefront) format + their MTL material files. This is also supported, but pretty limited given the format's limitations (no support for pivots, skeletons, animations, UV2, PBR materials, ...).

FBX, supported via the ufbx library. The previous import workflow used FBX2glTF integration. This requires installing an external program that links against the proprietary FBX SDK, so we recommend using the default ufbx method or other formats listed above (if suitable for your workflow).

Copy the scene file together with the textures and mesh data (if separate) to the project repository, then Godot will do a full import when focusing the editor window.

There are 3 ways to export glTF files from Blender:

As a glTF binary file (.glb).

As a glTF text-based file with separate binary data and textures (.gltf file + .bin file + textures).

glTF binary files (.glb) are the smaller option. They include the mesh and textures set up in Blender. When brought into Godot the textures are part of the object's material file.

There are two reasons to use glTF with the textures separate. One is to have the scene description in a text based format and the binary data in a separate binary file. This can be useful for version control if you want to review changes in a text-based format. The second is you need the texture files separate from the material file. If you don't need either of those, glTF binary files are fine.

The glTF import process first loads the glTF file's data into an in-memory GLTFState class. This data is then used to generate a Godot scene. When importing files at runtime, this scene can be directly added to the tree. The export process is the reverse of this, a Godot scene is converted to a GLTFState class, then the glTF file is generated from that.

When importing glTF files in the editor, there are two more steps. After generating the Godot scene, the ResourceImporterScene class is used to apply additional import settings, including settings you set through the Import dock and the Advanced Import Settings dialog. This is then saved as a Godot scene file, which is what gets used when you run/export your game.

If your model contains blend shapes (also known as "shape keys" and "morph targets"), your glTF export setting Data > Armature > Export Deformation Bones Only needs to be configured to Enabled.

Exporting non-deforming bones anyway will lead to incorrect shading.

Blender versions older than 3.2 do not export emissive textures with the glTF file. If your model uses one and you're using an older version of Blender, it must be brought in separately.

By default, Blender has backface culling disabled on materials and will export materials to match how they render in Blender. This means that materials in Godot will have their cull mode set to Disabled. This can decrease performance since backfaces will be rendered, even when they are being culled by other faces. To resolve this, enable Backface Culling in Blender's Materials tab, then export the scene to glTF again.

This functionality requires Blender 3.0 or later. For best results, we recommend using Blender 3.5 or later, as it includes many fixes to the glTF exporter.

It is strongly recommended to use an official Blender release downloaded from blender.org, as opposed to a Linux distribution package or Flatpak. This avoids any issues related to packaging, such as different library versions that can cause incompatibilities or sandboxing restrictions.

The editor can directly import .blend files by calling Blender's glTF export functionality in a transparent manner.

This allows you to iterate on your 3D scenes faster, as you can save the scene in Blender, alt-tab back to Godot then see your changes immediately. When working with version control, this is also more efficient as you no longer need to commit a copy of the exported glTF file to version control.

To use .blend import, you must install Blender before opening the Godot editor (if opening a project that already contains .blend files). If you keep Blender installed at its default location, Godot should be able to detect its path automatically. If this isn't the case, configure the path to the Blender executable in the Editor Settings (Filesystem > Import > Blender > Blender Path).

If you keep .blend files within your project folder but don't want them to be imported by Godot, disable Filesystem > Import > Blender > Enabled in the advanced Project Settings.

The .blend import process converts to glTF first, so it still uses Godot's glTF import code. Therefore, the .blend import process is the same as the glTF import process, but with an extra step at the beginning.

When working in a team, keep in mind using .blend files in your project will require all team members to have Blender installed. While Blender is a free download, this may add friction when working on the project. .blend import is also not available on the Android and web editors, as these platforms can't call external programs.

If this is problematic, consider using glTF scenes exported from Blender instead.

Blender has built-in COLLADA support, but it does not work properly for the needs of game engines and shouldn't be used as-is. However, scenes exported with the built-in Collada support may still work for simple scenes without animation.

For complex scenes or scenes that contain animations it is highly recommend to use glTF instead.

OBJ is one of the simplest 3D formats out there, so Godot should be able to import most OBJ files successfully. However, OBJ is also a very limited format: it doesn't support skinning, animation, UV2 or PBR materials.

There are 2 ways to use OBJ meshes in Godot:

Load them directly in a MeshInstance3D node, or any other property that expects as mesh (such as GPUParticles3D). This is the default mode.

Change their import mode to OBJ as Scene in the Import dock then restart the editor. This allows you to use the same import options as glTF or Collada scenes, such as unwrapping UV2 on import (for Using Lightmap global illumination).

Blender 3.4 and later can export RGB vertex colors in OBJ files (this is a nonstandard extension of the OBJ format). Godot is able to import those vertex colors, but they will not be displayed on the material unless you enable Vertex Color > Use As Albedo on the material.

Vertex colors from OBJ meshes keep their original color space once imported (sRGB/linear), but their brightness is clamped to 1.0 (they can't be overbright).

By default any FBX file added to a Godot project in Godot 4.3 or later will use the ufbx import method. Any file that was was added to a project in a previous version, such as 4.2, will continue to be imported via the FBX2glTF method unless you go into that files import settings, and change the importer to ufbx.

If you keep .fbx files within your project folder but don't want them to be imported by Godot, disable Filesystem > Import > FBX > Enabled in the advanced Project Settings.

If you want to setup the FBX2glTF workflow, which is generally not recommend unless you have a specific reason to use it, you need to download the FBX2glTF executable, then specify the path to that executable in the editor settings under Filesystem > Import > FBX > FBX2glTFPath

The FBX2glTF import process converts to glTF first, so it still uses Godot's glTF import code. Therefore, the FBX import process is the same as the glTF import process, but with an extra step at the beginning.

The full installation process for using FBX2glTF in Godot is described on the FBX import page of the Godot website.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Complex emission shapes — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/particles/complex_shapes.html

**Contents:**
- Complex emission shapes
- Emission shape textures
- User-contributed notes

When it is not enough to emit particles from one of the simple shapes available in the process material, Godot provides a way to emit particles from arbitrary, complex shapes. The shapes are generated from meshes in the scene and stored as textures in the particle process material. This is a very versatile workflow that has allowed users to use particle systems for things that go beyond traditional use cases, like foliage, leaves on a tree, or complex holographic effects.

When you create emission points from meshes, you can only select a single node as emission source. If you want particles to emit from multiple shapes, you either have to create several particle systems or combine the meshes into one in an external DCC software.

Create particle emission points...

...from a mesh instance as the source

More points = higher particle density

To make use of this feature, start by creating a particle system in the current scene. Add a mesh instance that serves as the source of the particle emission points. With the particle system selected, navigate to the viewport menu and select the GPUParticles3D entry. From there, select Create Emission Points From Node.

A dialog window will pop up and ask you to select a node as the emission source. Choose one of the mesh instances in the scene and confirm your selection. The next dialog window deals with the amount of points and how to generate them.

Emission Points controls the total number of points that you are about to generate. Particles will spawn from these points, so what to enter here depends on the size of the source mesh (how much area you have to cover) and the desired density of the particles.

Emission Source offers 3 different options for how the points are generated. Select Surface Points if all you want to do is distribute the emission points across the surface of the mesh. Select Surface Points + Normal (Directed) if you also want to generate information about the surface normals and make particles move in the direction that the normals point at. The last option, Volume, creates emission points everywhere inside the mesh, not just across its surface.

The emission points are stored in the particle system's local coordinate system, so you can move the particle node around and the emission points will follow. This might be useful when you want to use the same particle system in several different places. On the other hand, you might have to regenerate the emission points when you move either the particle system or the source mesh.

The available emission shape textures

All the data for complex particle emission shapes is stored in a set of textures. How many, depends on the type of emission shape you use. If you set the Shape property in the Emission Shape group on the particle process material to Points, you have access to 2 texture properties, the Point Texture and the Color Texture. Set it to Directed Points and there is a third property called Normal Texture.

Point Texture contains all possible emission points that were generated in the previous step. A point is randomly selected for every particle when it spawns. Normal Texture, if it exists, provides a direction vector at that same location. If the Color Texture property is also set, it provides color for the particle, sampled at the same location as the other two textures and modulating any other color that was set up on the process material.

There is also the Point Count property that you can use to change the number of emission points at any time after creating the emission shape. This includes dynamically at runtime while the playing the game.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Connecting navigation meshes — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/navigation/navigation_connecting_navmesh.html

**Contents:**
- Connecting navigation meshes
- User-contributed notes

Different NavigationMeshes are automatically merged by the NavigationServer when at least two vertex positions of one edge exactly overlap.

To connect over arbitrary distances see Using NavigationLinks.

The same is true for multiple NavigationPolygon resources. As long as their outline points overlap exactly the NavigationServer will merge them. NavigationPolygon outlines must be from different NavigationPolygon resources to connect.

Overlapping or intersecting outlines on the same NavigationPolygon will fail the navigation mesh creation. Overlapping or intersecting outlines from different NavigationPolygons will often fail to create the navigation region edge connections on the NavigationServer and should be avoided.

Exactly means exactly for the vertex position merge. Small float errors that happen quite regularly with imported meshes will prevent a successful vertex merge.

Alternatively navigation meshes are not merged but still considered as connected by the NavigationServer when their edges are nearly parallel and within distance to each other. The connection distance is defined by the edge_connection_margin for each navigation map. In many cases navigation mesh edges cannot properly connect when they partly overlap. Better avoid any navigation mesh overlap at all time for a consistent merge behavior.

If navigation debug is enabled and the NavigationServer active the established navigation mesh connections will be visualized. See Navigation debug tools for more info about navigation debug options.

The default 2D edge_connection_margin can be changed in the ProjectSettings under navigation/2d/default_edge_connection_margin.

The default 3D edge_connection_margin can be changed in the ProjectSettings under navigation/3d/default_edge_connection_margin.

The edge connection margin value of any navigation map can also be changed at runtime with the NavigationServer API.

Changing the edge connection margin will trigger a full update of all navigation mesh connections on the NavigationServer.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (gdscript):
```gdscript
extends Node2D

func _ready() -> void:
    # 2D margins are designed to work with 2D "pixel" values.
    var default_map_rid: RID = get_world_2d().get_navigation_map()
    NavigationServer2D.map_set_edge_connection_margin(default_map_rid, 50.0)
```

Example 2 (unknown):
```unknown
using Godot;

public partial class MyNode2D : Node2D
{
    public override void _Ready()
    {
        // 2D margins are designed to work with 2D "pixel" values.
        Rid defaultMapRid = GetWorld2D().NavigationMap;
        NavigationServer2D.MapSetEdgeConnectionMargin(defaultMapRid, 50.0f);
    }
}
```

Example 3 (gdscript):
```gdscript
extends Node3D

func _ready() -> void:
    # 3D margins are designed to work with 3D world unit values.
    var default_map_rid: RID = get_world_3d().get_navigation_map()
    NavigationServer3D.map_set_edge_connection_margin(default_map_rid, 0.5)
```

Example 4 (unknown):
```unknown
using Godot;

public partial class MyNode3D : Node3D
{
    public override void _Ready()
    {
        // 3D margins are designed to work with 3D world unit values.
        Rid defaultMapRid = GetWorld3D().NavigationMap;
        NavigationServer3D.MapSetEdgeConnectionMargin(defaultMapRid, 0.5f);
    }
}
```

---

## Creating a 3D particle system — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/particles/creating_a_3d_particle_system.html

**Contents:**
- Creating a 3D particle system
- The process material
- Draw passes
- Particle conversion
- User-contributed notes

Required particle node properties

To get started with particles, the first thing we need to do is add a GPUParticles3D node to the scene. Before we can actually see any particles, we have to set up two parameters on the node: the Process Material and at least one Draw Pass.

To add a process material to your particles node, go to Process Material in the inspector panel. Click on the box next to Process Material and from the dropdown menu select New ParticleProcessMaterial.

Creating a process material

ParticleProcessMaterial is a special kind of material. We don't use it to draw any objects. We use it to update particle data and behavior on the GPU instead of the CPU, which comes with a massive performance boost. A click on the newly added material displays a long list of properties that you can set to control each particle's behavior.

At least one draw pass is required

In order to render any particles, at least one draw pass needs to be defined. To do that, go to Draw Passes in the inspector panel. Click on the box next to Pass 1 and select New QuadMesh from the dropdown menu. After that, click on the mesh and set its Size to 0.1 for both x and y. Reducing the mesh's size makes it a little easier to tell the individual particle meshes apart at this stage.

You can use up to 4 draw passes per particle system. Each pass can render a different mesh with its own unique material. All draw passes use the data that is computed by the process material, which is an efficient method for composing complex effects: Compute particle behavior once and feed it to multiple render passes.

Using multiple draw passes: yellow rectangles (pass1) and blue spheres (pass 2)

If you followed the steps above, your particle system should now be emitting particles in a waterfall-like fashion, making them move downwards and disappear after a few seconds. This is the foundation for all particle effects. Take a look at the documentation for particle and particle material properties to learn how to make particle effects more interesting.

Turning GPU into CPU particles

You can convert GPU particles to CPU particles at any time using the entry in the viewport menu. When you do so, keep in mind that not every feature of GPU particles is available for CPU particles, so the resulting particle system will look and behave differently from the original.

You can also convert CPU particles to GPU particles if you no longer need to use CPU particles. This is also done from the viewport menu.

Some of the most notable features that are lost during the conversion include:

You also lose the following properties:

Emission Shape Offset

Inherit Velocity Ratio

Converting GPU particles to CPU particles can become necessary when you want to release a game on older devices that don't support modern graphics APIs.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## CSGMesh3D — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/classes/class_csgmesh3d.html

**Contents:**
- CSGMesh3D
- Description
- Tutorials
- Properties
- Property Descriptions
- User-contributed notes

Inherits: CSGPrimitive3D < CSGShape3D < GeometryInstance3D < VisualInstance3D < Node3D < Node < Object

A CSG Mesh shape that uses a mesh resource.

This CSG node allows you to use any mesh resource as a CSG shape, provided it is manifold. A manifold shape is closed, does not self-intersect, does not contain internal faces and has no edges that connect to more than two faces. See also CSGPolygon3D for drawing 2D extruded polygons to be used as CSG nodes.

Note: CSG nodes are intended to be used for level prototyping. Creating CSG nodes has a significant CPU cost compared to creating a MeshInstance3D with a PrimitiveMesh. Moving a CSG node within another CSG node also has a significant CPU cost, so it should be avoided during gameplay.

Prototyping levels with CSG

void set_material(value: Material)

Material get_material()

The Material used in drawing the CSG shape.

void set_mesh(value: Mesh)

The Mesh resource to use as a CSG shape.

Note: Some Mesh types such as PlaneMesh, PointMesh, QuadMesh, and RibbonTrailMesh are excluded from the type hint for this property, as these primitives are non-manifold and thus not compatible with the CSG algorithm.

Note: When using an ArrayMesh, all vertex attributes except Mesh.ARRAY_VERTEX, Mesh.ARRAY_NORMAL and Mesh.ARRAY_TEX_UV are left unused. Only Mesh.ARRAY_VERTEX and Mesh.ARRAY_TEX_UV will be passed to the GPU.

Mesh.ARRAY_NORMAL is only used to determine which faces require the use of flat shading. By default, CSGMesh will ignore the mesh's vertex normals, recalculate them for each vertex and use a smooth shader. If a flat shader is required for a face, ensure that all vertex normals of the face are approximately equal.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Environment and post-processing — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/environment_and_post_processing.html

**Contents:**
- Environment and post-processing
- Environment
  - Camera3D node (high priority)
  - WorldEnvironment node (medium priority, recommended)
  - Preview environment and sun (low priority)
- Camera attributes
- Environment options
  - Background
  - Sky materials
  - Ambient light

Godot 4 provides a redesigned Environment resource, as well as a new post-processing system with many available effects right out of the box.

As of Godot 4, Environment performance/quality settings are defined in the project settings instead of in the Environment resource. This makes global adjustments easier, as you no longer have to tweak Environment resources individually to suit various hardware configurations.

Note that most Environment performance/quality settings are only visible after enabling the Advanced toggle in the Project Settings.

The Environment resource stores all the information required for controlling the 2D and 3D rendering environment. This includes the sky, ambient lighting, tone mapping, effects, and adjustments. By itself, it does nothing, but you can enable it by using it in one of the following locations, in order of priority:

An Environment can be set to a Camera3D node. It will have priority over any other setting.

This is mostly useful when you want to override an existing environment, but in general it's a better idea to use the option below.

The WorldEnvironment node can be added to any scene, but only one can exist per active scene tree. Adding more than one will result in a warning.

Any Environment added has higher priority than the default Environment (explained below). This means it can be overridden on a per-scene basis, which makes it quite useful.

Since Godot 4, the preview environment and sun system replace the default_env.tres file that was used in Godot 3 projects.

If no WorldEnvironment node or DirectionalLight3D node is present in the current scene, the editor will display a preview environment and sun instead. This can be disabled using the buttons at the top of the 3D editor:

Clicking on the 3 vertical dots on the right will display a dialog which allows you to customize the appearance of the preview environment:

The preview sun and sky is only visible in the editor, not in the running project. Using the buttons at the bottom of the dialog, you can add the preview sun and sky into the scene as nodes.

If you hold Shift while clicking Add Sun to Scene or Add Environment to Scene in the preview environment editor, this will add both a preview sun and environment to the current scene (as if you clicked both buttons separately). Use this to speed up project setup and prototyping.

In Godot 4, exposure and depth of field information was split from the Environment resource into a separate CameraAttributes resource. This allows adjusting those properties independently of other Environment settings more easily.

The CameraAttributes resource stores exposure and depth of field information. It also allows enabling automatic exposure adjustments depending on scene brightness.

There are two kinds of CameraAttribute resources available:

CameraAttributesPractical: Features are exposed using arbitrary units, which are easier to reason about for most game use cases.

CameraAttributesPhysical: Features are exposed using real world units, similar to a digital camera. For example, field of view is set using a focal length in millimeters instead of a value in degrees. Recommended when physical accuracy is important, such as for photorealistic rendering.

Both CameraAttribute resource types allow you to use the same features, but they are configured differently. If you don't know which one to choose, use CameraAttributesPractical.

Using a CameraAttributesPhysical on a Camera3D node will lock out FOV and aspect adjustments in that Camera3D, as field of view is adjusted in the CameraAttributesPhysical resource instead. If used in a WorldEnvironment, the CameraAttributesPhysical will not override any Camera3D in the scene.

A CameraAttributes resource can be added to a Camera3D or a WorldEnvironment node. When the current camera has a CameraAttributes set, it will override the one set in WorldEnvironment (if any).

In most situations, setting the CameraAttributes resource on the Camera3D node instead of the WorldEnvironment is recommended. Unlike WorldEnvironment, assigning the CameraAttributes resource to the Camera3D node prevents depth of field from displaying in the 3D editor viewport, unless the camera is being previewed.

The following is a detailed description of all environment options and how they are intended to be used.

The Background section contains settings on how to fill the background (parts of the screen where objects were not drawn). The background not only serves the purpose of displaying an image or color. By default, it also affects how objects are affected by ambient and reflected light. This is called image-based lighting (IBL).

As a result, the background sky may greatly impact your scene's overall appearance, even if the sky is never directly visible on screen. This should be taken into account when tweaking lighting in your scene.

There are several background modes available:

Clear Color uses the default clear color defined in the project settings. The background will be a constant color.

Custom Color is like Clear Color, but with a custom color value.

Sky lets you define a background sky material (see below). By default, objects in the scene will reflect this sky material and absorb ambient light from it.

Canvas displays the 2D scene as a background to the 3D scene. This can be used to make environment effects visible on 2D rendering, such as glow in 2D.

Keep does not draw any sky, keeping what was present on previous frames instead. This improves performance in purely indoor scenes, but creates a "hall of mirrors" visual glitch if the sky is visible at any time.

When using the Sky background mode (or the ambient/reflected light mode is set to Sky), a Sky subresource becomes available to edit in the Environment resource. Editing this subresource allows you to create a SkyMaterial resource within the Sky.

There are 3 built-in sky materials to choose from:

PanoramaSkyMaterial: Use a 360 degree panorama sky image (2:1 aspect ratio recommended). To benefit from high dynamic range, the panorama image must be in an HDR-compatible format such as .hdr or .exr rather than a standard dynamic range format like .png or .jpg.

ProceduralSkyMaterial: Use a procedurally generated sky with adjustable ground, sun, sky and horizon colors. This is the type of sky used in the editor preview. The sun's position is automatically derived from the first 4 DirectionalLight3D nodes present in the scene. There can be up to 4 suns at a given time.

PhysicalSkyMaterial: Use a physically-based procedural sky with adjustable scattering parameters. The sun's position is automatically derived from the first DirectionalLight3D node present in the scene. PhysicalSkyMaterial is slightly more expensive to render compared to ProceduralSkyMaterial. There can be up to 1 sun at a given time.

Panorama sky images are sometimes called HDRIs (High Dynamic Range Images). You can find freely licensed HDRIs on Poly Haven.

HDR PanoramaSkyMaterial textures with very bright spots (such as real life photos with the sun visible) may result in visible sparkles on ambient and specular reflections. This is caused by the texture's peak exposure being too high.

To resolve this, select the panorama texture in the FileSystem dock, go to the Import dock, enable HDR Clamp Exposure then click Reimport.

If you need a custom sky material (e.g. for procedural clouds), you can create a custom sky shader.

Ambient light (as defined here) is a type of light that affects every piece of geometry with the same intensity. It is global and independent of lights that might be added to the scene. Ambient light is one of the two components of image-based lighting. Unlike reflected light, ambient light does not vary depending on the camera's position and viewing angle.

There are several types of ambient light to choose from:

Background: Source ambient light from the background, such as the sky, custom color or clear color (default). Ambient light intensity will vary depending on the sky image's contents, which can result in more visually appealing ambient lighting. A sky must be set as background for this mode to be visible.

Disabled: Do not use any ambient light. Useful for purely indoor scenes.

Color: Use a constant color for ambient light, ignoring the background sky. Ambient light intensity will be the same on all sides, which may result in the scene's lighting looking more flat. Useful for indoor scenes where pitch black shadows may be too dark, or to maximize performance on low-end devices.

Sky: Source ambient light from a specified sky, even if the background is set to a mode other than Sky. If the background mode is already Sky, this mode behaves identically to Background.

When the ambient light mode is set to Sky or Background (and background is set to Sky), it's possible to blend between the ambient color and sky using the Sky Contribution property. This value is set to 1.0 by default, which means that only the ambient sky is used. The ambient color is ignored unless Sky Contribution is decreased below 1.0.

Here is a comparison of how different ambient light affects a scene:

Finally, there is an Energy setting which is a multiplier. It's useful when working with HDR.

In general, you should only rely on ambient light alone for simple scenes or large exteriors. You may also do so to boost performance. Ambient light is fast to render, but it doesn't provide the best lighting quality. It's better to generate ambient light from ReflectionProbe, VoxelGI or SDFGI, as these will simulate how indirect light propagates more accurately. Below is a comparison, in terms of quality, between using a flat ambient color and a VoxelGI:

Using one of the methods described above will replace constant ambient lighting with ambient lighting from the probes.

Reflected light (also called specular light) is the other of the two components of image-based lighting.

Reflected light can be set to one of 3 modes:

Background: Reflect from the background, such as the sky, custom color or clear color (default).

Disabled: Do not reflect any light from the environment. Useful for purely indoor scenes, or to maximize performance on low-end devices.

Sky: Reflect from the background sky, even if the background is set to a mode other than Sky. If the background mode is already Sky, this mode behaves identically to Background.

This section refers to non-volumetric fog only. It is possible to use both non-volumetric fog and Volumetric fog and fog volumes at the same time.

Fog, as in real life, makes distant objects fade away into a uniform color. There are two kinds of fog in Godot:

Depth Fog: This one is applied based on the distance from the camera.

Height Fog: This one is applied to any objects below (or above) a certain height, regardless of the distance from the camera.

Both of these fog types can have their curve tweaked, making their transition more or less sharp.

Two properties can be tweaked to make the fog effect more interesting:

The first is Sun Scatter, which makes use of the DirectionalLight3D's color and energy in the current scene. When looking towards the directional light (usually a sun), the fog will be tinted according to the light's color to simulate the sunlight passing through the fog.

The second is Aerial Perspective, which tints the fog color according to the sky color to better blend the sky with the background. Higher values will result in more tinting, with 1.0 fully replacing the regular fog color with aerial perspective. This can be used in large open world levels to provide a better sense of depth, or to avoid color discontinuities between the sky and fog colors.

If both Sun Scatter and Aerial Perspective are greater than 0.0, sun scattering is applied on top of aerial perspective.

Fog can cause banding to appear on the viewport, especially at higher density levels. See Color banding for guidance on reducing banding.

Volumetric fog provides a realistic fog effect to the scene, with fog color being affected by the lights that traverse the fog.

See Volumetric fog and fog volumes for documentation on setting up volumetric fog.

Tonemap selects the tonemapping algorithm that will be applied to the scene, from a list of standard algorithms used in the film and game industries. Tonemapping modes other than Linear are used to make light and dark areas more homogeneous, while also avoiding clipping of bright highlights. Each algorithm has a different performance characteristic that should be considered when choosing your tonemapper.

The tone mapping options are:

Mode: The tonemapping mode to use.

Linear: Does not modify color data, resulting in a linear tonemapping curve which unnaturally clips bright values, causing bright lighting to look blown out. The simplest and fastest tonemapper.

Reinhard: A simple tonemapping curve that rolls off bright values to prevent clipping. This results in an image that can appear dull and low contrast. Slower than Linear. When White is left at the default value of 1.0, Reinhard produces an identical image to Linear.

Filmic: Uses a film-like tonemapping curve to prevent clipping of bright values and provide better contrast than Reinhard. Slightly slower than Reinhard.

ACES: Uses a high-contrast film-like tonemapping curve and desaturates bright values for a more realistic appearance. Slightly slower than Filmic.

AgX: Uses a film-like tonemapping curve and desaturates bright values for a more realistic appearance. Better than other tonemappers at maintaining the hue of colors as they become brighter. The slowest tonemapping option. White is fixed at a value of 16.29, which makes AgX unsuitable for use with the Mobile rendering method.

Exposure: Adjusts the brightness of values before they are provided to the tonemapper. Higher Exposure values result in a brighter image. Values provided to the tonemapper will also be multiplied by 2.0 and 1.8 for Filmic and ACES respectively to produce a similar apparent brightness as Linear.

White: The white reference value for tonemapping, which indicates where bright white is located in the scale of values provided to the tonemapper. For photorealistic lighting, recommended values are between 6.0 and 8.0. Higher values result in less blown out highlights, but may make the scene appear lower contrast. White is not available when using Linear or AgX.

The Environment resource supports many popular mid- and post-processing effects.

Screen-space effects such as SSR, SSAO, SSIL and glow do not operate on geometry that is located outside the camera view or is occluded by other opaque geometry. Consider this when tweaking their settings to avoid distracting changes during gameplay.

This feature is only available when using the Forward+ renderer, not Mobile or Compatibility.

While Godot supports several sources of reflection data such as Reflection probes, they may not provide enough detail for all situations. Scenarios where screen-space reflections make the most sense are when objects are in contact with each other (object over floor, over a table, floating on water, etc).

On top of providing more detail, screen-space reflections also work in real-time (while other types of reflections are usually precomputed). This can be used to make characters, cars, etc. reflect on surrounding surfaces when moving around.

Screen-space reflections can be used at the same time as other reflection sources to benefit from detailed reflections when possible, while having a fallback when screen-space reflections cannot be used (for example, to reflect off-screen objects).

A few user-controlled parameters are available to better tweak the technique:

Max Steps: Determines the length of the reflection. The bigger this number, the more costly it is to compute.

Fade In: Allows adjusting the fade-in curve, which is useful to make the contact area softer.

Fade Out: Allows adjusting the fade-out curve, so the step limit fades out softly.

Depth Tolerance: Can be used to allow screen-space rays to pass behind objects. The rays will treat each object as if it has this depth in determining if it can pass behind the object. Higher values will make screen-space reflections exhibit fewer "breakups", at the cost of some objects creating physically incorrect reflections.

Keep in mind that screen-space-reflections only work for reflecting opaque geometry. Transparent materials won't be reflected, as they don't write to the depth buffer. This also applies to shaders that use hint_screen_texture or hint_depth_texture uniforms.

This feature is only available when using the Forward+ renderer, not Mobile or Compatibility.

As mentioned in the Ambient section, areas where light from light nodes does not reach (either because it's outside the radius or shadowed) are lit with ambient light. Godot can simulate this using VoxelGI, ReflectionProbe, the Sky, or a constant ambient color. The problem, however, is that all the methods proposed previously act more on a larger scale (large regions) than at the smaller geometry level.

Constant ambient color and Sky are the same everywhere, while GI and Reflection probes have more local detail, but not enough to simulate situations where light is not able to fill inside hollow or concave features.

This can be simulated with Screen Space Ambient Occlusion. As you can see in the image below, its purpose is to make sure concave areas are darker, simulating a narrower path for the light to enter:

It is a common mistake to enable this effect, turn on a light, and not be able to appreciate it. This is because SSAO only acts on ambient light. It does not affect direct light.

This is why, in the image above, the effect is less noticeable under the direct light (on the left). If you want to force SSAO to work with direct light too, use the Light Affect parameter. Even though this is not physically correct, some artists like how it looks.

SSAO looks best when combined with a real source of indirect light, like VoxelGI:

Tweaking SSAO is possible with several parameters:

Radius: The distance at which objects can occlude each other when calculating screen-space ambient occlusion. Higher values will result in occlusion over a greater distance at the cost of performance and quality.

Intensity: The primary screen-space ambient occlusion intensity. Acts as a multiplier for the screen-space ambient occlusion effect. A higher value results in darker occlusion. Since SSAO is a screen-space effect, it's recommended to remain conservative with this value. SSAO that is too strong can be distracting during gameplay.

Power: The distribution of occlusion. A higher value results in darker occlusion, similar to Intensity, but with a sharper falloff.

Detail: Sets the strength of the additional level of detail for the screen-space ambient occlusion effect. A high value makes the detail pass more prominent, but it may contribute to aliasing in your final image.

Horizon: The threshold for considering whether a given point on a surface is occluded or not represented as an angle from the horizon mapped into the 0.0-1.0 range. A value of 1.0 results in no occlusion.

Sharpness: The amount that the screen-space ambient occlusion effect is allowed to blur over the edges of objects. Setting too high will result in aliasing around the edges of objects. Setting too low will make object edges appear blurry.

Light Affect: The screen-space ambient occlusion intensity in direct light. In real life, ambient occlusion only applies to indirect light, which means its effects can't be seen in direct light. Values higher than 0 will make the SSAO effect visible in direct light. Values above 0.0 are not physically accurate, but some artists prefer this effect.

AO Channel Affect The screen-space ambient occlusion intensity on materials that have an AO texture defined. Values higher than 0.0 will make the SSAO effect visible in areas darkened by AO textures.

This feature is only available when using the Forward+ renderer, not Mobile or Compatibility.

SSIL provides indirect lighting for small details or dynamic geometry that other global illumination techniques cannot cover. This applies to bounced diffuse lighting, but also emissive materials. When SSIL is enabled on its own, the effect may not be that noticeable, which is intended.

Instead, SSIL is meant to be used as a complement to other global illumination techniques such as VoxelGI, SDFGI and LightmapGI. SSIL also provides a subtle ambient occlusion effect, similar to SSAO, but with less detail.

This feature only provides indirect lighting. It is not a full global illumination solution. This makes it different from screen-space global illumination (SSGI) offered by other 3D engines. SSIL can be combined with SSR and/or SSAO for greater visual quality (at the cost of performance).

Tweaking SSIL is possible with several parameters:

Radius: The distance that bounced lighting can travel when using the screen space indirect lighting effect. A larger value will result in light bouncing further in a scene, but may result in under-sampling artifacts which look like long spikes surrounding light sources.

Intensity: The brightness multiplier for the screen-space indirect lighting effect. A higher value will result in brighter light.

Sharpness: The amount that the screen-space indirect lighting effect is allowed to blur over the edges of objects. Setting too high will result in aliasing around the edges of objects. Setting too low will make object edges appear blurry.

Normal Rejection: Amount of normal rejection used when calculating screen-space indirect lighting. Normal rejection uses the normal of a given sample point to reject samples that are facing away from the current pixel. Normal rejection is necessary to avoid light leaking when only one side of an object is illuminated. However, normal rejection can be disabled if light leaking is desirable, such as when the scene mostly contains emissive objects that emit light from faces that cannot be seen from the camera.

This feature is only available when using the Forward+ renderer, not Mobile or Compatibility.

Signed distance field global illumination (SDFGI) is a form of real-time global illumination. It is not a screen-space effect, which means it can provide global illumination for off-screen elements (unlike SSIL).

See Signed distance field global illumination (SDFGI) for instructions on setting up this global illumination technique.

When using the Compatibility rendering method, glow uses a different implementation with some properties being unavailable and hidden from the inspector: Levels, Normalized, Strength, Blend Mode, Mix, Map, and Map Strength.

This implementation is optimized to run on low-end devices and is less flexible as a result.

In photography and film, when light amount exceeds the maximum luminance (brightness) supported by the media, it generally bleeds outwards to darker regions of the image. This is simulated in Godot with the Glow effect.

By default, even if the effect is enabled, it will be weak or invisible. One of two conditions need to happen for it to actually show:

The light in a pixel surpasses the HDR Threshold (where 0 is all light surpasses it, and 1.0 is light over the tonemapper White value). Normally, this value is expected to be at 1.0, but it can be lowered to allow more light to bleed. There is also an extra parameter, HDR Scale, that allows scaling (making brighter or darker) the light surpassing the threshold.

The Bloom property has a value greater than 0.0. As it increases, it sends the whole screen to the glow processor at higher amounts.

Both will cause the light to start bleeding out of the brighter areas.

Once glow is visible, it can be controlled with a few extra parameters:

Intensity is an overall scale for the effect, it can be made stronger or weaker (0.0 removes it).

Strength is how strong the gaussian filter kernel is processed. Greater values make the filter saturate and expand outwards. In general, changing this is not needed, as the size can be adjusted more efficiently with the Levels.

The Blend Mode of the effect can also be changed:

Additive is the strongest one, as it only adds the glow effect over the image with no blending involved. In general, it's too strong to be used, but can look good with low-intensity Bloom (produces a dream-like effect).

Screen ensures glow never brightens more than itself and it works great as an all around.

Softlight is the default and weakest one, producing only a subtle color disturbance around the objects. This mode works best on dark scenes.

Replace can be used to blur the whole screen or debug the effect. It only shows the glow effect without the image below.

Mix mixes the glow effect with the main image. This can be used for greater artistic control. The mix factor is controlled by the Mix property which appears above the blend mode (only when the blend mode is set to Mix). High mix factor values will appear to darken the image unless Bloom is increased.

To change the glow effect size and shape, Godot provides Levels. Smaller levels are strong glows that appear around objects, while large levels are hazy glows covering the whole screen:

The real strength of this system, though, is to combine levels to create more interesting glow patterns:

Finally, the glow effect can be controlled using a glow map, which is a texture that determines how bright glow should be on each part of the screen. This texture can optionally be colored to tint the glow effect to the glow map's color. The texture is stretched to fit the viewport, so using an aspect ratio that matches your viewport's most common aspect ratio (such as 16:9) is recommended to avoid visible distortion.

There are 2 main use cases for a glow map texture:

Create a "lens dirt" effect using a dirt pattern texture.

Make glow less strong on specific parts of the screen by using a gradient texture.

By default, glow uses a bicubic scaling filter on desktop platforms and a bilinear scaling filter on mobile platforms. The bicubic scaling filter results in higher quality with a less blocky appearance, but it has a performance cost on the GPU which can be significant on integrated graphics. The scale mode can be controlled using the Rendering > Environment > Glow > Upscale Mode project setting. This setting is only effective when using the Forward+ or Mobile renderers, as Compatibility uses a different glow implementation.

There are 2 ways to use glow in 2D:

Since Godot 4.2, you can enable HDR for 2D rendering when using the Forward+ and Mobile rendering methods. This has a performance cost, but it allows for a greater dynamic range. This also allows you to control which objects glow using their individual Modulate or Self Modulate properties (use the RAW mode in the color picker). Enabling HDR can also reduce banding in the 2D rendering output.

To enable HDR in 2D, open the Project Settings, enable Rendering > Viewport > HDR 2D then restart the editor.

If you want to maximize performance, you can leave HDR disabled for 2D rendering. However, you will have less control on which objects glow.

Enable glow, set the environment background mode to Canvas then decrease Glow HDR Threshold so that pixels that are not overbright will still glow. To prevent UI elements from glowing, make them children of a CanvasLayer node. You can control which layers are affected by glow using the Background > Canvas Max Layer property of the Environment resource.

Example of using glow in a 2D scene. HDR 2D is enabled, while coins and the bullet have their Modulate property increased to overbright values using the RAW mode in the color picker.

The 2D renderer renders in linear color space if the Rendering > Viewport > HDR 2D project setting is enabled, so the source_color hint must also be used for uniform samplers that are used as color input in canvas_item shaders. If this is not done, the texture will appear washed out.

If 2D HDR is disabled, source_color will keep working correctly in canvas_item shaders, so it's recommend to use it when relevant either way.

Using linear color space also means that alpha blending will change. Sprites with low opacity values generally become more visible, and font rendering will look bolder due to the low-opacity pixels from the font antialiasing becoming more visible. This also affects the editor's own rendering.

Glow can be used to blur the whole viewport, which is useful for background blur when a menu is open. Only 3D rendering will be affected unless the environment's background mode is set to Canvas. To prevent UI elements from being blurred when using the Canvas background mode, make them children of a CanvasLayer node. You can control which layers are affected by this blurring effect using the Background > Canvas Max Layer property of the Environment resource.

To use glow as a blurring solution:

Enable Normalized and adjust levels according to preference. Increasing higher level indices will result in a more blurred image. It's recommended to leave a single glow level at 1.0 and leave all other glow levels at 0.0, but this is not required. Note that the final appearance will vary depending on viewport resolution.

Set Intensity to 1.0 and Bloom to 1.0.

Set the blend mode to Replace and HDR Luminance Cap to 1.0.

Example of using glow to blur the 2D rendering in the menu's background

At the end of processing, Godot offers the possibility to do some standard image adjustments.

Basic BCS adjustments

The first adjustment is being able to change the typical Brightness, Contrast, and Saturation properties:

Color correction using a 1D gradient

The second adjustment is by supplying a color correction gradient. This can be done by assigning a GradientTexture1D resource to the Color Correction property, or by loading a texture containing a horizontal gradient. The leftmost part of the gradient represents black in the source image, whereas the rightmost part of the gradient represents white in the source image.

A linear black-to-white gradient like the following one will produce no effect:

But creating custom ones will allow to map each channel to a different color:

Color correction using a 3D LUT

A 3D look-up-texture (LUT) can also be used for color correction. This is a special texture used to modify each color channel separately from one another (red, green, blue). This image can be of any resolution, but since color correction is low-frequency data, sticking to low resolutions is recommended for performance reasons. A LUT texture's resolution is typically 17×17×17, 33×33×33, 51×51×51 or 65×65×65 (the odd size allows for better interpolation).

For this to work, the look-up texture's import mode must be set to Texture3D in the Import dock (instead of being imported as a regular Texture2D):

Make sure to configure the number of horizontal and vertical slices to import as well. If you don't do this, the LUT texture will not affect the viewport correctly when used. You can preview how the 3D texture was imported by double-clicking it, in the FileSystem dock, then going to the inspector to flip through the texture's layers.

You can use this neutral 33×33×33 LUT template as a base (right-click and choose Save as…):

With the above LUT template, after changing its import mode to Texture3D, set its number of Horizontal slices to 33 in the Import dock then click Reimport. If you load this LUT into the Color Correction property, you won't see any visible difference for now since this texture is designed to be a neutral starting point.

This LUT template can be modified in an image editor to provide a different mood to the image. A common workflow is to place the LUT image next to a screenshot of the project's 3D viewport, then use an image editor to modify both the LUT image and the screenshot at the same time. The LUT can then be saved and applied to the game engine to perform the same color correction in real-time.

For example, modifying the LUT template in an image editor to give it a "sepia" look results in the image on the right:

Adjustments and color correction are applied after tonemapping. This means the tonemapping properties defined above still have an effect when adjustments are enabled.

This effect simulates focal distance on cameras. It blurs objects behind a given range. It has an initial Distance with a Transition region (in world units):

The Amount parameter controls the amount of blur. For larger blurs, tweaking the depth of field quality in the advanced project settings may be needed to avoid artifacts.

This effect simulates focal distance on cameras. It blurs objects close to the camera (acts in the opposite direction as far blur). It has an initial Distance with a Transition region (in world units):

The Amount parameter controls the amount of blur. For larger blurs, tweaking the Quality may be needed in order to avoid artifacts.

It is common to use both blurs together to focus the viewer's attention on a given object, or create a so-called "tilt shift" effect.

When using CameraAttributesPhysical instead of CameraAttributesPractical, depth of field is automatically computed from the camera attributes' focus distance, focal length, and aperture.

This multiplies the overall scene brightness visible from the camera. Higher values result in a visually brighter scene.

This feature is only available when using the Forward+ renderer, not Mobile or Compatibility.

Even though, in most cases, lighting and texturing are heavily artist controlled, Godot supports a basic high dynamic range implementation with the auto exposure mechanism. This is generally used to add realism when combining interior areas with low light and bright outdoor areas. Auto exposure simulates the camera (or eye) in an effort to adapt between light and dark locations and their different amounts of light.

Auto exposure needs to evaluate the scene's brightness every frame, which has a moderate performance cost. Therefore, it's recommended to leave Auto Exposure disabled if it doesn't make much of a difference in your scene.

The simplest way to use auto exposure is to make sure outdoor lights (or other strong lights) have energy beyond 1.0. This is done by tweaking their Energy multiplier (on the Light itself). To make it consistent, the Sky usually needs to use the energy multiplier too, to match with the directional light. Normally, values between 3.0 and 6.0 are enough to simulate indoor-outdoor conditions.

By combining Auto Exposure with Glow post-processing, pixels that go over the tonemap White will bleed to the glow buffer, creating the typical bloom effect in photography.

The user-controllable values in the Auto Exposure section come with sensible defaults, but you can still tweak them:

Scale: Value to scale the lighting. Higher values produce brighter images, and lower values produce darker ones.

Min Sensitivity / Min Exposure Value: Minimum luminance that auto exposure will aim to adjust for (in ISO when using CameraAttributesPractical, or in EV100 when using CameraAttributesPhysical). Luminance is the average of the light in all the pixels of the screen.

Max Sensitivity / Max Exposure Value: Maximum luminance that auto exposure will aim to adjust for (in ISO when using CameraAttributesPractical, or in EV100 when using CameraAttributesPhysical).

Speed: Speed at which luminance corrects itself. The higher the value, the faster luminance correction happens. High values may be more suited to fast-paced games, but can be distracting in some scenarios.

When using CameraAttributesPractical, exposure is set using sensitivity defined in ISO instead of an exposure value in EV100. Typical ISO values are between 50 and 3200, with higher values resulting in higher final exposure. In real life, daytime photography generally uses ISO values between 100 and 800.

See Physical light and camera units if you wish to use real world units to configure your camera's exposure, field of view and depth of field.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Exporting 3D scenes — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/assets_pipeline/exporting_3d_scenes.html

**Contents:**
- Exporting 3D scenes
- Overview
- Limitations
- User-contributed notes

In Godot, it is possible to export 3D scenes as a glTF 2.0 file. You can export as a glTF binary (.glb file) or glTF embedded with textures (gltf + .bin + textures). This allows you to create scenes in Godot, such as a CSG mesh blockout for a level, export it to clean it up in a program such as Blender, and then bring it back into Godot.

Only Blender 2.83 and newer can import glTF files exported by Godot.

To export a scene in the editor go to Scene > Export As... > glTF 2.0 Scene...

There are several limitations with glTF export.

No support for exporting particles since their implementation varies across engines.

ShaderMaterials cannot be exported.

No support for exporting 2D scenes.

3D scenes can be saved at runtime using runtime file loading and saving, including from an exported project.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Faking global illumination — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/global_illumination/faking_global_illumination.html

**Contents:**
- Faking global illumination
- Why fake global illumination?
- Faking DirectionalLight3D global illumination
- Faking positional light global illumination
- User-contributed notes

Godot provides several global illumination (GI) techniques, all with their advantages and drawbacks. Nonetheless, it remains possible to avoid using any GI technique and use a handmade approach instead. There are a few reasons for using a "handmade" approach to global illumination instead of VoxelGI, SDFGI or baked lightmaps:

You need to have good rendering performance, but can't afford going through a potentially cumbersome lightmap baking process.

You need an approach to GI that is fully real-time and works in procedurally generated levels.

You need an approach to GI that is fully real-time and does not suffer from significant light leaks.

The approaches described below only cover indirect diffuse lighting, not specular lighting. For specular lighting, consider using ReflectionProbes which are usually cheap enough to be used in conjunction with this fake GI approach.

Not sure if faking global illumination with lights is suited to your needs? See Which global illumination technique should I use? for a comparison of GI techniques available in Godot 4.

While the sky provides its own directional lighting, the scene's main DirectionalLight3D node typically emits a large amount of light. When using a GI technique, this light would be reflected on solid surfaces and would bounce back on most outdoors shaded surfaces.

We can fake this by adding a second DirectionalLight3D node with the following changes:

Rotate the light by 180 degrees. This allows it to represent lighting bounced by the main DirectionalLight3D node.

Set Shadows to Off. This reduces the secondary light's performance burden while also allowing shaded areas to receive some lighting (which is what we want here).

Set Energy to 10-40% of the original value. There is no "perfect" value, so experiment with various energy values depending on the light and your typical material colors.

Set Specular to 0.0. Indirect lighting shouldn't emit visible specular lobes, so we need to disable specular lighting entirely for the secondary light.

This approach works best in scenes that are mostly outdoors. When going indoors, the secondary DirectionalLight3D's light will still be visible as this light has shadows disabled.

This can be worked around by smoothly decreasing the secondary DirectionalLight3D's energy when entering an indoor area (and doing the opposite when leaving the indoor area). For instance, this can be achieved using an Area3D node and AnimationPlayer.

It's possible to follow the same approach as DirectionalLight3D for positional lights (OmniLight3D and SpotLight3D). However, this will require more manual work as this operation needs to be repeated for every positional light node in the scene to look good.

In an ideal scenario, additional OmniLight3Ds should be added at every location where a significant amount of light hits a bright enough surface. However, due to time constraints, this isn't always easily feasible (especially when performing procedural level generation).

If you're in a hurry, you can place a secondary OmniLight3D node at the same position as the main OmniLight3D node. You can add this node as a child of the main OmniLight3D node to make it easy to move and hide both nodes at the same time.

In the secondary OmniLight3D node, perform the following changes:

Increase the light's Range by 25-50%. This allows the secondary light to lighten what was previously not lit by the original light.

Set Shadows to Off. This reduces the secondary light's performance burden while also allowing shaded areas to receive some lighting (which is what we want here).

Set Energy to 10-40% of the original value. There is no "perfect" value, so experiment with various energy values depending on the light and its surroundings.

Set Specular to 0. Indirect lighting shouldn't emit visible specular lobes, so we need to disable specular lighting entirely for the secondary light.

For SpotLight3D, the same trick can be used. In this case, the secondary OmniLight3D should be placed in a way that reflects where most light will be bounced. This is usually close to the SpotLight3D's primary impact location.

In the example below, a SpotLight3D node is used to light up the room's floor. However, since there is no indirect lighting, the rest of the room remains entirely dark. In real life, the room's walls and ceiling would be lit up by light bouncing around. Using an OmniLight3D node positioned between the SpotLight3D's origin and the floor allows simulating this effect:

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Global illumination — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/global_illumination/index.html

**Contents:**
- Global illumination

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## High dynamic range lighting — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/high_dynamic_range.html

**Contents:**
- High dynamic range lighting
- Introduction
- Computer displays
- Scene linear & asset pipelines
  - sRGB transfer function to display linear ratios on image import
  - Hardware sRGB transfer function to display linear conversion
  - Scene linear to display-referred nonlinear
- Parameters of HDR
- User-contributed notes

The content of this page was not yet updated for Godot 4.5 and may be outdated. If you know how to improve this page or you can confirm that it's up to date, feel free to open a pull request.

Normally, an artist does all the 3D modeling, then all the texturing, looks at their awesome looking model in the 3D modeling software and says "looks fantastic, ready for integration!" then goes into the game, lighting is setup and the game runs.

So at what point does all this "HDR" business come into play? To understand the answer, we need to look at how displays behave.

Your display outputs linear light ratios from some maximum to some minimum intensity. Modern game engines perform complex math on linear light values in their respective scenes. So what's the problem?

The display has a limited range of intensity, depending on the display type. The game engine renders to an unlimited range of intensity values, however. While "maximum intensity" means something to an sRGB display, it has no bearing in the game engine; there is only a potentially infinitely wide range of intensity values generated per frame of rendering.

This means that some transformation of the scene light intensity, also known as scene-referred light ratios, need to be transformed and mapped to fit within the particular output range of the chosen display. This can be most easily understood if we consider virtually photographing our game engine scene through a virtual camera. Here, our virtual camera would apply a particular camera rendering transform to the scene data, and the output would be ready for display on a particular display type.

Godot does not support high dynamic range output yet. It can only perform lighting in HDR and tonemap the result to a low dynamic range image.

For advanced users, it is still possible to get a non-tonemapped image of the viewport with full HDR data, which can then be saved to an OpenEXR file.

Almost all displays require a nonlinear encoding for the code values sent to them. The display in turn, using its unique transfer characteristic, "decodes" the code value into linear light ratios of output, and projects the ratios out of the uniquely colored lights at each reddish, greenish, and blueish emission site.

For a majority of computer displays, the specifications of the display are outlined in accordance with IEC 61966-2-1, also known as the 1996 sRGB specification. This specification outlines how an sRGB display is to behave, including the color of the lights in the LED pixels as well as the transfer characteristics of the input (OETF) and output (EOTF).

Not all displays use the same OETF and EOTF as a computer display. For example, television broadcast displays use the BT.1886 EOTF. However, Godot currently only supports sRGB displays.

The sRGB standard is based around the nonlinear relationship between the current to light output of common desktop computing CRT displays.

The mathematics of a scene-referred model require that we multiply the scene by different values to adjust the intensities and exposure to different light ranges. The transfer function of the display can't appropriately render the wider dynamic range of the game engine's scene output using the simple transfer function of the display. A more complex approach to encoding is required.

Working in scene-linear sRGB is more complex than pressing a single switch. First, imported image assets must be converted to linear light ratios on import. Even when linearized, those assets may not be perfectly well-suited for use as textures, depending on how they were generated.

There are two ways to do this:

This is the easiest method of using sRGB assets, but it's not the most ideal. One issue with this is loss of quality. Using 8 bits per channel to represent linear light ratios is not sufficient to quantize the values correctly. These textures may also be compressed later, which can exacerbate the problem.

The GPU will do the conversion after reading the texel using floating-point. This works fine on PC and consoles, but most mobile devices don't support it, or they don't support it on compressed texture formats (iOS for example).

After all the rendering is done, the scene linear render requires transforming to a suitable output such as an sRGB display. To do this, enable sRGB conversion in the current Environment (more on that below).

Keep in mind that the sRGB -> Display Linear and Display Linear -> sRGB conversions must always be both enabled. Failing to enable one of them will result in horrible visuals suitable only for avant-garde experimental indie games.

HDR settings can be found in the Environment resource. Most of the time, these are found inside a WorldEnvironment node or set in a Camera node. For more information, see Environment and post-processing.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Importing 3D scenes — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/assets_pipeline/importing_3d_scenes/index.html

**Contents:**
- Importing 3D scenes

Godot supports importing 3D scenes from various file formats. This documentation section describes what those formats are, and how to use them, including exporting with the correct conventions and best practices, and how to customize the node type using a suffix in the node name. The import configuration article describes how to customize the imported data using the import dock, the advanced import settings dialog, and inherited scenes.

3D scenes can be loaded at runtime using runtime file loading and saving, including from an exported project.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Import configuration — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/assets_pipeline/importing_3d_scenes/import_configuration.html

**Contents:**
- Import configuration
- Import workflows
  - Using the Import dock
  - Using import scripts for automation
  - Using animation libraries
  - Filter script
- Scene inheritance
- User-contributed notes

Godot provides several ways to customize the imported data, such as the import dock, the advanced import setting dialog, and inherited scenes. This can be used to make further changes to the imported scene, such as adjusting meshes, adding physics information, and adding new nodes. You can also write a script that runs code at the end of the import process to perform arbitrary customization.

Note that, when applicable, modifying the original data should be preferred to configuring the scene after import. This helps minimize the differences between the 3D modeling application and the imported scene. See the Model export considerations and Node type customization using name suffixes articles for more information.

Since Godot can only save its own scene format (.tscn/.scn), Godot cannot save over the original 3D scene file (which uses a different format). This is also a safer approach as it avoids making accidental changes to the source file.

To allow customizing the scene and its materials, Godot's scene importer allows for different workflows regarding how data is imported.

Import dock after selecting a 3D scene in the FileSystem dock

This import process is customizable using 3 separate interfaces, depending on your needs:

The Import dock, after selecting the 3D scene by clicking it once in the FileSystem dock.

The Advanced Import Settings dialog, which can be accessed by double-clicking the 3D scene in the FileSystem dock or by clicking the Advanced… button in the Import dock. This allows you to customize per-object options in Godot, and preview models and animations. please see the Advanced Import Settings page for more information.

Import hints, which are special suffixes added to object names in the 3D modeling software. This allows you to customize per-object options in the 3D modeling software.

For basic customization, using the Import dock suffices. However, for more complex operations such as defining material overrides on a per-material basis, you'll need to use the Advanced Import Settings dialog, import hints, or possibly both.

The following options can be adjusted in the Import dock after selecting a 3D scene in the FileSystem dock:

Root Type: The node type to use as a root node. Using node types that inherit from Node3D is recommended. Otherwise, you'll lose the ability to position the node directly in the 3D editor.

Root Name: The name of the root node in the imported scene. This is generally not noticeable when instancing the scene in the editor (or drag-and-dropping from the FileSystem dock), as the root node is renamed to match the filename in this case.

Apply Root Scale: If enabled, Root Scale will be applied on the meshes and animations directly, while keeping the root node's scale to the default (1, 1, 1). This means that if you add a child node later on within the imported scene, it won't be scaled. If disabled, Root Scale will multiply the scale of the root node instead.

Ensure Tangents: If checked, generate vertex tangents using Mikktspace if the input meshes don't have tangent data. When possible, it's recommended to let the 3D modeling software generate tangents on export instead on relying on this option. Tangents are required for correct display of normal and height maps, along with any material/shader features that require tangents. If you don't need material features that require tangents, disabling this can reduce output file size and speed up importing if the source 3D file doesn't contain tangents.

Generate LODs: If checked, generates lower detail variants of the mesh which will be displayed in the distance to improve rendering performance. Not all meshes benefit from LOD, especially if they are never rendered from far away. Disabling this can reduce output file size and speed up importing. See Mesh level of detail (LOD) for more information.

Create Shadow Meshes: If checked, enables the generation of shadow meshes on import. This optimizes shadow rendering without reducing quality by welding vertices together when possible. This in turn reduces the memory bandwidth required to render shadows. Shadow mesh generation currently doesn't support using a lower detail level than the source mesh (but shadow rendering will make use of LODs when relevant).

Light Baking: Configures the meshes' global illumination mode in the 3D scene. If set to Static Lightmaps, sets the meshes' GI mode to Static and generates UV2 on import for lightmap baking.

Lightmap Texel Size: Only visible if Light Baking is set to Static Lightmaps. Controls the size of each texel on the baked lightmap. A smaller value results in more precise lightmaps, at the cost of larger lightmap sizes and longer bake times.

Use Named Skins: If checked, use named Skins for animation. The MeshInstance3D node contains 3 properties of relevance here: a skeleton NodePath pointing to the Skeleton3D node (usually ..), a mesh, and a skin:

The Skeleton3D node contains a list of bones with names, their pose and rest, a name and a parent bone.

The mesh is all of the raw vertex data needed to display a mesh. In terms of the mesh, it knows how vertices are weight-painted and uses some internal numbering often imported from 3D modeling software.

The skin contains the information necessary to bind this mesh onto this Skeleton3D. For every one of the internal bone IDs chosen by the 3D modeling software, it contains two things. Firstly, a Matrix known as the Bind Pose Matrix, Inverse Bind Matrix, or IBM for short. Secondly, the Skin contains each bone's name (if Use Named Skins is enabled), or the bone's index within the Skeleton3D list (if Use Named Skins is disabled).

Together, this information is enough to tell Godot how to use the bone poses in the Skeleton3D node to render the mesh from each MeshInstance3D. Note that each MeshInstance3D may share binds, as is common in models exported from Blender, or each MeshInstance3D may use a separate Skin object, as is common in models exported from other tools such as Maya.

Import: If checked, import animations from the 3D scene.

FPS: The number of frames per second to use for baking animation curves to a series of points with linear interpolation. It's recommended to configure this value to match the value you're using as a baseline in your 3D modeling software. Higher values result in more precise animation with fast movement changes, at the cost of higher file sizes and memory usage. Thanks to interpolation, there is usually not much benefit in going above 30 FPS (as the animation will still appear smooth at higher rendering framerates).

Trimming: Trim the beginning and end of animations if there are no keyframe changes. This can reduce output file size and memory usage with certain 3D scenes, depending on the contents of their animation tracks.

Remove Immutable Tracks: Remove animation tracks that only contain default values. This can reduce output file size and memory usage with certain 3D scenes, depending on the contents of their animation tracks.

Path: Path to an import script, which can run code after the import process has completed for custom processing. See Using import scripts for automation for more information.

Embedded Texture Handling: Controls how textures embedded within glTF scenes should be handled. Discard All Textures will not import any textures, which is useful if you wish to manually set up materials in Godot instead. Extract Textures extracts textures to external images, resulting in smaller file sizes and more control over import options. Embed as Basis Universal and Embed as Uncompressed keeps the textures embedded in the imported scene, with and without VRAM compression respectively.

Importer Which import method is used. ubfx handles fbx files as fbx files. FBX2glTF converts FBX files to glTF on import and requires additional setup. FBX2glTF is not recommended unless you have a specific rason to use it over ufbx or working with a different file format.

Allow Geometry Helper Nodes enables or disables geometry helper nodes

Embedded Texture Handling: Controls how textures embedded within fbx scenes should be handled. Discard All Textures will not import any textures, which is useful if you wish to manually set up materials in Godot instead. Extract Textures extracts textures to external images, resulting in smaller file sizes and more control over import options. Embed as Basis Universal and Embed as Uncompressed keeps the textures embedded in the imported scene, with and without VRAM compression respectively.

Blender-specific options

Only visible for .blend files.

Visible: All imports everything, even invisible objects. Visible Only only imports visible objects. Renderable only imports objects that are marked as renderable in Blender, regardless of whether they are actually visible. In Blender, renderability is toggled by clicking the camera icon next to each object in the Outliner, while visibility is toggled by the eye icon.

Active Collection Only: If checked, only imports nodes that are in the active collection in Blender.

Punctual Lights: If checked, imports lights (directional, omni, and spot) from Blender. "Punctual" is not to be confused with "positional", which is why directional lights are also included.

Cameras: If checked, imports cameras from Blender.

Custom Properties: If checked, imports custom properties from Blender as glTF extras. This data can then be used from an editor plugin that uses GLTFDocument.register_gltf_document_extension(), which can set node metadata on import (among other use cases).

Modifiers: If set to No Modifiers, object modifiers are ignored on import. If set to All Modifiers, applies modifiers to objects on import.

Colors: If checked, imports vertex colors from Blender.

UVs: If checked, imports vertex UV1 and UV2 from Blender.

Normals: If checked, imports vertex normals from Blender.

Export Geometry Nodes Instances: If checked, imports geometry node instances from Blender.

Tangents: If checked, imports vertex tangents from Blender.

Skins: None skips skeleton skin data import from Blender. 4 Influences (Compatible) imports skin data to be compatible with all renderers, at the cost of lower precision for certain rigs. All Influences imports skin data with all influences (up to 8 in Godot), which is more precise but may not be compatible with all renderers.

Export Bones Deforming Mesh Only: If checked, only imports bones that deform the mesh from Blender.

Unpack Enabled: If checked, unpacks the original images to the Godot filesystem and uses them. This allows changing image import settings like VRAM compression. If unchecked, allows Blender to convert the original images, such as repacking roughness and metallic into one roughness + metallic texture. In most cases, this option should be left checked, but if the .blend file's images aren't in the correct format, this must be disabled for correct behavior.

Export Materials: If set to Placeholder, does not import materials, but keeps surface slots so that separate materials can be assigned to different surfaces. If set to Export, imports materials as-is (note that procedural Blender materials may not work correctly). If set to Named Placeholder, imports materials, but doesn't import images that are packed into the .blend file. Textures will have to be reassigned manually in the imported materials.

Limit Playback: If checked, limits animation import to the playback range defined in Blender (the Start and End options at the right of the animation timeline in Blender). This can avoid including unused animation data, making the imported scene smaller and faster to load. However, this can also result in missing animation data if the playback range is not set correctly in Blender.

Always Sample: If checked, forces animation sampling on import to ensure consistency between how Blender and glTF perform animation interpolation, at the cost of larger file sizes. If unchecked, there may be differences in how animations are interpolated between what you see in Blender and the imported scene in Godot, due to different interpolation semantics between both.

Group Tracks: If checked, imports animations (actives and on NLA tracks) as separate tracks. If unchecked, all the currently assigned actions become one glTF animation.

A special script to process the whole scene after import can be provided. This is great for post-processing, changing materials, doing funny stuff with the geometry, and more.

Create a script that is not attached to any node by right-clicking in the FileSystem dock and choosing New > Script…. In the script editor, write the following:

The _post_import(scene: Node) function takes the imported scene as argument (the parameter is actually the root node of the scene). The scene that will finally be used must be returned (even if the scene can be entirely different).

To use your script, locate the script in the import tab's "Path" option under the "Import Script" category.

As of Godot 4.0, you can choose to import only animations from a glTF file and nothing else. This is used in some asset pipelines to distribute animations separately from models. For example, this allows you to use one set of animations for several characters, without having to duplicate animation data in every character.

To do so, select the glTF file in the FileSystem dock, then change the import mode to Animation Library in the Import dock:

Changing the import type to Animation Library in the Import dock

Click Reimport and restart the editor when prompted. After restarting, the glTF file will be imported as an AnimationLibrary instead of a PackedScene. This animation library can then be referenced in an AnimationPlayer node.

The import options that are visible after changing the import mode to Animation Library act the same as when using the Scene import mode. See Using the Import dock for more information.

It is possible to specify a filter script in a special syntax to decide which tracks from which animations should be kept.

The filter script is executed against each imported animation. The syntax consists of two types of statements, the first for choosing which animations to filter, and the second for filtering individual tracks within the matched animation. All name patterns are performed using a case-insensitive expression match, with support for ? and * wildcards (using String.matchn() under the hood).

The script must start with an animation filter statement (as denoted by the line beginning with an @). For example, if we would like to apply filters to all imported animations which have a name ending in "_Loop":

Similarly, additional patterns can be added to the same line, separated by commas. Here is a modified example to additionally include all animations with names that begin with "Arm_Left", but also exclude all animations which have names ending in "Attack":

Following the animation selection filter statement, we add track filtering patterns to indicate which animation tracks should be kept or discarded. If no track filter patterns are specified, then all tracks within the matched animations will be discarded!

It's important to note that track filter statements are applied in order for each track within the animation, this means that one line may include a track, a later rule can still discard it. Similarly, a track excluded by an early rule may then be re-included once again by a filter rule further down in the filter script.

For example: include all tracks in animations with names ending in "_Loop", but discard any tracks affecting a "Skeleton" which end in "Control", unless they have "Arm" in their name:

In the above example, tracks like "Skeleton:Leg_Control" would be discarded, while tracks such as "Skeleton:Head" or "Skeleton:Arm_Left_Control" would be retained.

Any track filter lines that do not begin with a + or - are ignored.

In many cases, it may be desired to make manual modifications to the imported scene. By default, this is not possible because if the source 3D asset changes, Godot will re-import the whole scene.

However, it is possible to make local modifications by using scene inheritance. If you try to open the imported scene using Scene > Open Scene… or Scene > Quick Open Scene…, the following dialog will appear:

Dialog when opening an imported 3D scene in the editor

In inherited scenes, the only limitations for modification are:

Nodes from the base scene can't be removed, but additional nodes can be added anywhere.

Subresources can't be edited. Instead, you need to save them externally as described above.

Other than that, everything is allowed.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
@tool # Needed so it runs in editor.
extends EditorScenePostImport

# This sample changes all node names.
# Called right after the scene is imported and gets the root node.
func _post_import(scene):
    # Change all node names to "modified_[oldnodename]"
    iterate(scene)
    return scene # Remember to return the imported scene

# Recursive function that is called on every node
# (for demonstration purposes; EditorScenePostImport only requires a `_post_import(scene)` function).
func iterate(node):
    if node != null:
        print_rich("Post-import: [b]%s[/b] -> [b]%s[/b]" % [node.name, "modified_" + node.name])
        node.name = "modified_" + node.name
        for child in node.get_children():
            iterate(child)
```

Example 2 (unknown):
```unknown
@+*_Loop, +Arm_Left*, -*Attack
```

Example 3 (unknown):
```unknown
@+*_Loop
+*
-Skeleton:*Control
+*Arm*
```

---

## Mesh level of detail (LOD) — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/mesh_lod.html

**Contents:**
- Mesh level of detail (LOD)
- Introduction
- Visual comparison
- Generating mesh LOD
- Comparing mesh LOD visuals and performance
- Configuring mesh LOD performance and quality
- Using mesh LOD with MultiMesh and particles
- User-contributed notes

Level of detail (LOD) is one of the most important ways to optimize rendering performance in a 3D project, along with Occlusion culling.

On this page, you'll learn:

How mesh LOD can improve your 3D project's rendering performance.

How to set up mesh LOD in Godot.

How to measure mesh LOD's effectiveness in your project (and alternatives you can explore if it doesn't meet your expectations).

You can see how mesh LOD works in action using the Occlusion Culling and Mesh LOD demo project.

Historically, level of detail in 3D games involved manually authoring meshes with lower geometry density, then configuring the distance thresholds at which these lower-detailed meshes should be drawn. This approach is still used today when increased control is needed.

However, in projects that have a large amount of detailed 3D assets, setting up LOD manually can be a very time-consuming process. As a result, automatic mesh decimation and LOD configuration is becoming increasingly popular.

Godot provides a way to automatically generate less detailed meshes for LOD usage on import, then use those LOD meshes when needed automatically. This is completely transparent to the user. The meshoptimizer library is used for LOD mesh generation behind the scenes.

Mesh LOD works with any node that draws 3D meshes. This includes MeshInstance3D, MultiMeshInstance3D, GPUParticles3D and CPUParticles3D.

Here is an example of LOD meshes generated on import. Lower detailed meshes will be used when the camera is far away from the object:

From most detailed (left) to least detailed (right), shaded view

Here's the same image with wireframe rendering to make the decimation easier to see:

From most detailed (left) to least detailed (right), wireframe view

If you need to manually configure level of detail with artist-created meshes, use Visibility ranges (HLOD) instead of automatic mesh LOD.

By default, mesh LOD generation happens automatically for imported 3D scenes (glTF, .blend, Collada, FBX). Once LOD meshes are generated, they will automatically be used when rendering the scene. You don't need to configure anything manually.

However, mesh LOD generation does not automatically happen for imported 3D meshes (OBJ). This is because OBJ files are not imported as full 3D scenes by default, but only as individual mesh resources to load into a MeshInstance3D node (or GPUParticles3D, CPUParticles3D, ...).

To make an OBJ file have mesh LOD generated for it, select it in the FileSystem dock, go to the Import dock, change its Import As option to Scene then click Reimport:

Changing the import type on an OBJ file in the Import dock

This will require restarting the editor after clicking Reimport.

The mesh LOD generation process is not perfect, and may occasionally introduce rendering issues (especially in skinned meshes). Mesh LOD generation can also take a while on complex meshes.

If mesh LOD causes a specific mesh to look broken, you can disable LOD generation for it in the Import dock. This will also speed up resource importing. This can be done globally in the 3D scene's import options, or on a per-mesh basis using the Advanced Import Settings dialog.

See Importing 3D scenes for more information.

To disable mesh LOD in the editor for comparison purposes, use the Disable Mesh LOD advanced debug draw mode. This can be done using the menu in the top-left corner of the 3D viewport (labeled Perspective or Orthogonal depending on camera mode):

Disabling mesh LOD in the 3D viewport's top-left menu

Enable View Frame Time in the same menu to view FPS in the top-right corner. Also enable View Information in the same menu to view the number of primitives (vertices + indices) rendered in the bottom-right corner.

If mesh LOD is working correctly in your scene and your camera is far away enough from the mesh, you should notice the number of drawn primitives decreasing and FPS increasing when mesh LOD is left enabled (unless you are CPU-bottlenecked).

To see mesh LOD decimation in action, change the debug draw mode to Display Wireframe in the menu specified above, then adjust the Rendering > Mesh LOD > LOD Change > Threshold Pixels project setting.

You can adjust how aggressive mesh LOD transitions should be in the root viewport by changing the Rendering > Mesh LOD > LOD Change > Threshold Pixels project setting. To change this value at runtime, set mesh_lod_threshold on the root viewport as follows:

Each viewport has its own mesh_lod_threshold property, which can be set independently from other viewports.

The default mesh LOD threshold of 1 pixel is tuned to look perceptually lossless; it provides a significant performance gain with an unnoticeable loss in quality. Higher values will make LOD transitions happen sooner when the camera moves away, resulting in higher performance, but lower quality.

If you need to perform per-object adjustments to mesh LOD, you can adjust how aggressive LOD transitions should be by adjusting the LOD Bias property on any node that inherits from GeometryInstance3D. Values above 1.0 will make LOD transitions happen later than usual (resulting in higher quality, but lower performance). Values below 1.0 will make LOD transitions happen sooner than usual (resulting in lower quality, but higher performance).

Additionally, ReflectionProbe nodes have their own Mesh LOD Threshold property that can be adjusted to improve rendering performance when the reflection probe updates. This is especially important for ReflectionProbes that use the Always update mode.

When rendering the scene, mesh LOD selection uses a screen-space metric. This means it automatically takes camera field of view and viewport resolution into account. Higher camera FOV and lower viewport resolutions will make LOD selection more aggressive; the engine will display heavily decimated models earlier when the camera moves away.

As a result, unlike Visibility ranges (HLOD), you don't need to do anything specific in your project to take camera FOV and viewport resolution into account.

For LOD selection, the point of the node's AABB that is the closest to the camera is used as a basis. This applies to any kind of mesh LOD (including for individual MeshInstance3D)s, but this has some implications for nodes that display multiple meshes at once, such as MultiMeshInstance3D, GPUParticles3D and GPUParticles3D. Most importantly, this means that all instances will be drawn with the same LOD level at a given time.

If you are noticing incorrect LOD selection with GPUParticles3D, make sure the node's visibility AABB is configured by selecting the GPUParticles3D node and using GPUParticles3D > Generate AABB at the top of the 3D viewport.

If you have instances in a MultiMesh that are far away from each other, they should be placed in a separate MultiMeshInstance3D node. Doing so will also improve rendering performance, as frustum and occlusion culling will be able to cull individual nodes (while they can't cull individual instances in a MultiMesh).

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
get_tree().root.mesh_lod_threshold = 4.0
```

Example 2 (unknown):
```unknown
GetTree().Root.MeshLodThreshold = 4.0f;
```

---

## Model export considerations — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/assets_pipeline/importing_3d_scenes/model_export_considerations.html

**Contents:**
- Model export considerations
- 3D asset direction conventions
- Exporting textures separately
- Exporting considerations
- Lighting considerations
- User-contributed notes

Before exporting a 3D model from a 3D modeling application, such as Blender, there are some considerations that should be taken into account to ensure that the model follows the conventions and best practices for Godot.

Godot uses a right-handed, Y-is-up coordinate system, with the -Z axis as the camera's forward direction. This is the same as OpenGL. This implies that +Z is back, +X is right, and -X is left for a camera.

The convention for 3D assets is to face the opposite direction as the camera, so that characters and other assets are facing the camera by default. This convention is extremely common in 3D modeling applications, and is codified in glTF as part of the glTF 2.0 specification. This means that for oriented 3D assets (such as characters), the +Z axis is the direction of the front, so -Z is the rear, +X is the left side, and -X is the right side for a 3D asset. In Blender, this means that +Y is rear and -Y is front for an asset.

When rotating an oriented 3D asset in Godot, use the use_model_front option on the look_at functions, and use the Vector3.MODEL_* constants to perform calculations in the oriented asset's local space.

For assets without an intrinsic front side or forward direction, such as a game map or terrain, take note of the cardinal directions instead. The convention in Godot and the vast majority of other applications is that +X is east and -X is west. Due to Godot's right-handed Y-is-up coordinate system, this implies that +Z is south and -Z is north. In Blender, this means that +Y is north and -Y is south.

While textures can be exported with a model in certain file formats, such as glTF 2.0, you can also export them separately. Godot uses PBR (physically based rendering) for its materials, so if a texturing program can export PBR textures, they can work in Godot. This includes the Substance suite, ArmorPaint (open source), and Material Maker (open source).

For more information on Godot's materials, see Standard Material 3D and ORM Material 3D.

Since GPUs can only render triangles, meshes that contain quads or N-gons have to be triangulated before they can be rendered. Godot can triangulate meshes on import, but results may be unpredictable or incorrect, especially with N-gons. Regardless of the target application, triangulating before exporting the scene will lead to more consistent results and should be done whenever possible.

To avoid issues with incorrect triangulation after importing in Godot, it is recommended to make the 3D modeling software triangulate objects on its own. In Blender, this can be done by adding a Triangulate modifier to your objects and making sure Apply Modifiers is checked in the export dialog. Alternatively, depending on the exporter, you may be able to find and enable a Triangulate Faces option in the export dialog.

To avoid issues with 3D selection in the editor, it is recommended to apply the object transform in the 3D modeling software before exporting the scene.

It is important that the mesh is not deformed by bones when exporting. Make sure that the skeleton is reset to its T-pose or default rest pose before exporting with your favorite 3D editor.

While it's possible to import lights from a 3D scene using the glTF, .blend or Collada formats, it's generally advised to design the scene's lighting in the Godot editor after importing the scene.

This allows you to get a more accurate feel for the final result, as different engines will render lights in a different manner. This also avoids any issues with lights appearing excessively strong or faint as a result of the import process.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Node type customization using name suffixes — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/assets_pipeline/importing_3d_scenes/node_type_customization.html

**Contents:**
- Node type customization using name suffixes
- Opting out
- Remove nodes and animations (-noimp)
- Create collisions (-col, -convcol, -colonly, -convcolonly)
- Create Occluder (-occ, -occonly)
- Create navigation (-navmesh)
- Create a VehicleBody (-vehicle)
- Create a VehicleWheel (-wheel)
- Rigid Body (-rigid)
- Animation loop (-loop, -cycle)

Many times, when editing a scene, there are common tasks that need to be done after exporting:

Adding collision detection to objects.

Setting objects as navigation meshes.

Deleting nodes that are not used in the game engine (like specific lights used for modeling).

To simplify this workflow, Godot offers several suffixes that can be added to the names of the objects in your 3D modeling software. When imported, Godot will detect suffixes in object names and will perform actions automatically.

All the suffixes described below can be used with -, $, and _ and are case-insensitive.

If you do not want Godot to perform any of the actions described below, you can set the nodes/use_node_type_suffixes import option to false. This will disable all node type suffixes, which keeps nodes the same type as the original file indicated. However, the -noimp suffix will still be respected, as well as non-node suffixes like -vcol or -loop.

Alternatively, you can completely opt out of all name suffixes by setting the nodes/use_name_suffixes import option to false. This will completely stop the general scene import code from looking at name suffixes. However, the format-specific import code may still look at name suffixes, such as the glTF importer checking for the -loop suffix.

Disabling these options makes editor-imported files more similar to the original files, and more similar to importing files at runtime. For an import workflow that works at runtime, gives more predictable results, and only has explicitly defined behavior, consider setting these options to false and using GLTFDocumentExtension instead.

Nodes and animations that have the -noimp suffix will be removed at import time no matter what their type is. They will not appear in the imported scene.

This is equivalent to enabling Skip Import for a node in the Advanced Import Settings dialog.

The option -col will work only for Mesh objects. If it is detected, a child static collision node will be added, using the same geometry as the mesh. This will create a triangle mesh collision shape, which is a slow, but accurate option for collision detection. This option is usually what you want for level geometry (but see also -colonly below).

The option -convcol will create a ConvexPolygonShape3D instead of a ConcavePolygonShape3D. Unlike triangle meshes which can be concave, a convex shape can only accurately represent a shape that doesn't have any concave angles (a pyramid is convex, but a hollow box is concave). Due to this, convex collision shapes are generally not suited for level geometry. When representing simple enough meshes, convex collision shapes can result in better performance compared to a triangle collision shape. This option is ideal for simple or dynamic objects that require mostly-accurate collision detection.

However, in both cases, the visual geometry may be too complex or not smooth enough for collisions. This can create physics glitches and slow down the engine unnecessarily.

To solve this, the -colonly modifier exists. It will remove the mesh upon importing and will create a StaticBody3D collision instead. This helps the visual mesh and actual collision to be separated.

The option -convcolonly works in a similar way, but will create a ConvexPolygonShape3D instead using convex decomposition.

With Collada files, the option -colonly can also be used with Blender's empty objects. On import, it will create a StaticBody3D with a collision node as a child. The collision node will have one of a number of predefined shapes, depending on Blender's empty draw type:

Choosing a draw type for an Empty on creation in Blender

Single arrow will create a SeparationRayShape3D.

Cube will create a BoxShape3D.

Image will create a WorldBoundaryShape3D.

Sphere (and the others not listed) will create a SphereShape3D.

When possible, try to use a few primitive collision shapes instead of triangle mesh or convex shapes. Primitive shapes often have the best performance and reliability.

For better visibility on Blender's editor, you can set the "X-Ray" option on collision empties and set some distinct color for them by changing Edit > Preferences > Themes > 3D Viewport > Empty.

If using Blender 2.79 or older, follow these steps instead: User Preferences > Themes > 3D View > Empty.

See Collision shapes (3D) for a comprehensive overview of collision shapes.

If a mesh is imported with the -occ suffix an Occluder3D node will be created based on the geometry of the mesh, it does not replace the mesh. A mesh node with the -occonly suffix will be converted to an Occluder3D on import.

A mesh node with the -navmesh suffix will be converted to a navigation mesh. The original Mesh object will be removed at import-time.

A mesh node with the -vehicle suffix will be imported as a child to a VehicleBody3D node.

A mesh node with the -wheel suffix will be imported as a child to a VehicleWheel3D node.

A mesh node with the -rigid suffix will be imported as a RigidBody3D.

Animation clips in the source 3D file that start or end with the token loop or cycle will be imported as a Godot Animation with the loop flag set. Unlike the other suffixes described above, this does not require a hyphen.

In Blender, this requires using the NLA Editor and naming the Action with the loop or cycle prefix or suffix.

A material with the -alpha suffix will be imported with the TRANSPARENCY_ALPHA transparency mode.

A material with the -vcol suffix will be imported with the FLAG_ALBEDO_FROM_VERTEX_COLOR and FLAG_SRGB_VERTEX_COLOR flags set.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Occlusion culling — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/occlusion_culling.html

**Contents:**
- Occlusion culling
- Why use occlusion culling
- How occlusion culling works in Godot
- Setting up occlusion culling
  - Automatically baking occluders (recommended)
  - Manually placing occluders
- Previewing occlusion culling
- Performance considerations
  - Design your levels to take advantage of occlusion culling
  - Avoid moving OccluderInstance3D nodes during gameplay

In a 3D rendering engine, occlusion culling is the process of performing hidden geometry removal.

On this page, you'll learn:

What are the advantages and pitfalls of occlusion culling.

How to set up occlusion culling in Godot.

Troubleshooting common issues with occlusion culling.

You can see how occlusion culling works in action using the Occlusion Culling and Mesh LOD demo project.

In this example scene with hundreds of rooms stacked next to each other, a dynamic object (red sphere) is hidden behind the wall in the lit room (on the left of the door):

Example scene with an occlusion culling-friendly layout

With occlusion culling disabled, all the rooms behind the lit room have to be rendered. The dynamic object also has to be rendered:

Example scene with occlusion culling disabled (wireframe)

With occlusion culling enabled, only the rooms that are actually visible have to be rendered. The dynamic object is also occluded by the wall, and therefore no longer has to be rendered:

Example scene with occlusion culling enabled (wireframe)

Since the engine has less work to do (fewer vertices to render and fewer draw calls), performance will increase as long as there are enough occlusion culling opportunities in the scene. This means occlusion culling is most effective in indoor scenes, preferably with many smaller rooms instead of fewer larger rooms. Combine this with Mesh level of detail (LOD) and Visibility ranges (HLOD) to further improve performance gains.

When using the Forward+ renderer, the engine already performs a depth prepass. This consists in rendering a depth-only version of the scene before rendering the scene's actual materials. This is used to ensure each opaque pixel is only shaded once, reducing the cost of overdraw significantly.

The greatest performance benefits can be observed when using the Mobile renderer, as it does not feature a depth prepass for performance reasons. As a result, occlusion culling will actively decrease shading overdraw with that renderer.

Nonetheless, even when using a depth prepass, there is still a noticeable benefit to occlusion culling in complex 3D scenes. However, in scenes with few occlusion culling opportunities, occlusion culling may not be worth the added setup and CPU usage.

"occluder" refers to the shape blocking the view, while "occludee" refers to the object being hidden.

In Godot, occlusion culling works by rasterizing the scene's occluder geometry to a low-resolution buffer on the CPU. This is done using the software raytracing library Embree.

The engine then uses this low-resolution buffer to test the occludee's AABB against the occluder shapes. The occludee's AABB must be fully occluded by the occluder shape to be culled.

As a result, smaller objects are more likely to be effectively culled than larger objects. Larger occluders (such as walls) also tend to be much more effective than smaller ones (such as decoration props).

The first step to using occlusion culling is to enable the Rendering > **Occlusion Culling > Use Occlusion Culling project setting. (Make sure the Advanced toggle is enabled in the Project Settings dialog to be able to see it.)

This project setting applies immediately, so you don't need to restart the editor.

After enabling the project setting, you still need to create some occluders. For performance reasons, the engine doesn't automatically use all visible geometry as a basis for occlusion culling. Instead, the engine requires a simplified representation of the scene with only static objects to be baked.

There are two ways to set up occluders in a scene:

Only MeshInstance3D nodes are currently taken into account in the occluder baking process. MultiMeshInstance3D, GPUParticles3D, CPUParticles3D and CSG nodes are not taken into account when baking occluders. If you wish those to be treated as occluders, you have to manually create occluder shapes that (roughly) match their geometry.

Since Godot 4.4, CSG nodes can be taken into account in the baking process if they are converted to a MeshInstance3D before baking occluders.

This restriction does not apply to occludees. Any node type that inherits from GeometryInstance3D can be occluded.

After enabling the occlusion culling project setting mentioned above, add an OccluderInstance3D node to the scene containing your 3D level.

Select the OccluderInstance3D node, then click Bake Occluders at the top of the 3D editor viewport. After baking, the OccluderInstance3D node will contain an Occluder3D resource that stores a simplified version of your level's geometry. This occluder geometry appears as purple wireframe lines in the 3D view (as long as View Gizmos is enabled in the Perspective menu). This geometry is then used to provide occlusion culling for both static and dynamic occludees.

After baking, you may notice that your dynamic objects (such as the player, enemies, etc…) are included in the baked mesh. To prevent this, set the Bake > Cull Mask property on the OccluderInstance3D to exclude certain visual layers from being baked.

For example, you can disable layer 2 on the cull mask, then configure your dynamic objects' MeshInstance3D nodes to be located on the visual layer 2 (instead of layer 1). To do so, select the MeshInstance3D node in question, then on the VisualInstance3D > Layers property, uncheck layer 1 then check layer 2. After configuring both cull mask and layers, bake occluders again by following the above process.

This approach is more suited for specialized use cases, such as creating occlusion for MultiMeshInstance3D setups or CSG nodes (due to the aforementioned limitation).

After enabling the occlusion culling project setting mentioned above, add an OccluderInstance3D node to the scene containing your 3D level. Select the OccluderInstance3D node, then choose an occluder type to add in the Occluder property:

QuadOccluder3D (a single plane)

BoxOccluder3D (a cuboid)

SphereOccluder3D (a sphere-shaped occluder)

PolygonOccluder3D (a 2D polygon with as many points as you want)

There is also ArrayOccluder3D, whose points can't be modified in the editor but can be useful for procedural generation from a script.

You can enable a debug draw mode to preview what the occlusion culling is actually "seeing". In the top-left corner of the 3D editor viewport, click the Perspective button (or Orthogonal depending on your current camera mode), then choose Display Advanced… > Occlusion Culling Buffer. This will display the low-resolution buffer that is used by the engine for occlusion culling.

In the same menu, you can also enable View Information and View Frame Time to view the number of draw calls and rendered primitives (vertices + indices) in the bottom-right corner, along with the number of frames per second rendered in the top-right corner.

If you toggle occlusion culling in the project settings while this information is displayed, you can see how much occlusion culling improves performance in your scene. Note that the performance benefit highly depends on the 3D editor camera's view angle, as occlusion culling is only effective if there are occluders in front of the camera.

To toggle occlusion culling at runtime, set use_occlusion_culling on the root viewport as follows:

Toggling occlusion culling at runtime is useful to compare performance on a running project.

This is the most important guideline. A good level design is not just about what the gameplay demands; it should also be built with occlusion in mind.

For indoor environments, add opaque walls to "break" the line of sight at regular intervals and ensure not too much of the scene can be seen at once.

For large open scenes, use a pyramid-like structure for the terrain's elevation when possible. This provides the greatest culling opportunities compared to any other terrain shape.

This includes moving the parents of OccluderInstance3D nodes, as this will cause the nodes themselves to move in global space, therefore requiring the BVH to be rebuilt.

Toggling an OccluderInstance3D's visibility (or one of its parents' visibility) is not as expensive, as the update only needs to happen once (rather than continuously).

For example, if you have a sliding or rotating door, you can make the OccluderInstance3D node not be a child of the door itself (so that the occluder never moves), but you can hide the OccluderInstance3D visibility once the door starts opening. You can then reshow the OccluderInstance3D once the door is fully closed.

If you absolutely have to move an OccluderInstance3D node during gameplay, use a primitive Occluder3D shape for it instead of a complex baked shape.

If you notice low performance or stuttering in complex 3D scenes, it may mean that the CPU is overloaded as a result of rendering detailed occluders. Select the OccluderInstance3D node, increase the Bake > Simplification property then bake occluders again.

Remember to keep the simplification value reasonable. Values that are too high for the level's geometry may cause incorrect occlusion culling to occur, as in My occludee is being culled when it shouldn't be.

If this still doesn't lead to low enough CPU usage, you can try adjusting the Rendering > Occlusion Culling > BVH Build Quality project setting and/or decreasing Rendering > Occlusion Culling > Occlusion Rays Per Thread. You'll need to enable the Advanced toggle in the Project Settings dialog to see those settings.

On the occluder side:

First, double-check that the Bake > Cull Mask property in the OccluderInstance3D is set to allow baking the meshes you'd like. The visibility layer of the MeshInstance3D nodes must be present within the cull mask for the mesh to be included in the bake.

Also note that occluder baking only takes meshes with opaque materials into account. Surfaces will transparent materials will not be included in the bake, even if the texture applied on them is fully opaque.

Lastly, remember that MultiMeshInstance3D, GPUParticles3D, CPUParticles3D and CSG nodes are not taken into account when baking occluders. As a workaround, you can add OccluderInstance3D nodes for those manually.

On the occludee side:

Make sure Extra Cull Margin is set as low as possible (it should usually be 0.0), and that Ignore Occlusion Culling is disabled in the object's GeometryInstance3D section.

Also, check the AABB's size (which is represented by an orange box when selecting the node). This axis-aligned bounding box must be fully occluded by the occluder shapes for the occludee to be hidden.

The most likely cause for this is that objects that were included in the occluder bake have been moved after baking occluders. For instance, this can occur when moving your level geometry around or rearranging its layout. To fix this, select the OccluderInstance3D node and bake occluders again.

This can also happen because dynamic objects were included in the bake, even though they shouldn't be. Use the occlusion culling debug draw mode to look for occluder shapes that shouldn't be present, then adjust the bake cull mask accordingly.

The last possible cause for this is overly aggressive mesh simplification during the occluder baking process. Select the OccluderInstance3D node, decrease the Bake > Simplification property then bake occluders again.

As a last resort, you can enable the Ignore Occlusion Culling property on the occludee. This will negate the performance improvements of occlusion culling for that object, but it makes sense to do this for objects that will never be culled (such as a first-person view model).

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
get_tree().root.use_occlusion_culling = true
```

Example 2 (unknown):
```unknown
GetTree().Root.UseOcclusionCulling = true;
```

---

## Optimization using MultiMeshes — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/performance/using_multimesh.html

**Contents:**
- Optimization using MultiMeshes
- MultiMeshes
- Multimesh example
- User-contributed notes

The content of this page was not yet updated for Godot 4.5 and may be outdated. If you know how to improve this page or you can confirm that it's up to date, feel free to open a pull request.

For large amount of instances (in the thousands), that need to be constantly processed (and certain amount of control needs to be retained), using servers directly is the recommended optimization.

When the amount of objects reach the hundreds of thousands or millions, none of these approaches are efficient anymore. Still, depending on the requirements, there is one more optimization possible.

A MultiMesh is a single draw primitive that can draw up to millions of objects in one go. It's extremely efficient because it uses the GPU hardware to do this.

The only drawback is that there is no screen or frustum culling possible for individual instances. This means, that millions of objects will be always or never drawn, depending on the visibility of the whole MultiMesh. It is possible to provide a custom visibility rect for them, but it will always be all-or-none visibility.

If the objects are simple enough (just a couple of vertices), this is generally not much of a problem as most modern GPUs are optimized for this use case. A workaround is to create several MultiMeshes for different areas of the world.

It is also possible to execute some logic inside the vertex shader (using the INSTANCE_ID or INSTANCE_CUSTOM built-in constants). For an example of animating thousands of objects in a MultiMesh, see the Animating thousands of fish tutorial. Information to the shader can be provided via textures (there are floating-point Image formats which are ideal for this).

Another alternative is to use a GDExtension and C++, which should be extremely efficient (it's possible to set the entire state for all objects using linear memory via the RenderingServer.multimesh_set_buffer() function). This way, the array can be created with multiple threads, then set in one call, providing high cache efficiency.

Finally, it's not required to have all MultiMesh instances visible. The amount of visible ones can be controlled with the MultiMesh.visible_instance_count property. The typical workflow is to allocate the maximum amount of instances that will be used, then change the amount visible depending on how many are currently needed.

Here is an example of using a MultiMesh from code. Languages other than GDScript may be more efficient for millions of objects, but for a few thousands, GDScript should be fine.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
extends MultiMeshInstance3D


func _ready():
    # Create the multimesh.
    multimesh = MultiMesh.new()
    # Set the format first.
    multimesh.transform_format = MultiMesh.TRANSFORM_3D
    # Then resize (otherwise, changing the format is not allowed).
    multimesh.instance_count = 10000
    # Maybe not all of them should be visible at first.
    multimesh.visible_instance_count = 1000

    # Set the transform of the instances.
    for i in multimesh.visible_instance_count:
        multimesh.set_instance_transform(i, Transform3D(Basis(), Vector3(i * 20, 0, 0)))
```

Example 2 (unknown):
```unknown
using Godot;

public partial class MyMultiMeshInstance3D : MultiMeshInstance3D
{
    public override void _Ready()
    {
        // Create the multimesh.
        Multimesh = new MultiMesh();
        // Set the format first.
        Multimesh.TransformFormat = MultiMesh.TransformFormatEnum.Transform3D;
        // Then resize (otherwise, changing the format is not allowed)
        Multimesh.InstanceCount = 1000;
        // Maybe not all of them should be visible at first.
        Multimesh.VisibleInstanceCount = 1000;

        // Set the transform of the instances.
        for (int i = 0; i < Multimesh.VisibleInstanceCount; i++)
        {
            Multimesh.SetInstanceTransform(i, new Transform3D(Basis.Identity, new Vector3(i * 20, 0, 0)));
        }
    }
}
```

---

## Optimizing 3D performance — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/performance/optimizing_3d_performance.html

**Contents:**
- Optimizing 3D performance
- Culling
  - Occlusion culling
- Transparent objects
- Level of detail (LOD)
  - Billboards and imposters
  - Use automatic instancing
  - Use manual instancing (MultiMesh)
- Bake lighting
- Animation and skinning

Godot will automatically perform view frustum culling in order to prevent rendering objects that are outside the viewport. This works well for games that take place in a small area, however things can quickly become problematic in larger levels.

Walking around a town for example, you may only be able to see a few buildings in the street you are in, as well as the sky and a few birds flying overhead. As far as a naive renderer is concerned however, you can still see the entire town. It won't just render the buildings in front of you, it will render the street behind that, with the people on that street, the buildings behind that. You quickly end up in situations where you are attempting to render 10× or 100× more than what is visible.

Things aren't quite as bad as they seem, because the Z-buffer usually allows the GPU to only fully shade the objects that are at the front. This is called depth prepass and is enabled by default in Godot when using the Forward+ or Compatibility rendering methods. However, unneeded objects are still reducing performance.

One way we can potentially reduce the amount to be rendered is to take advantage of occlusion. Godot 4.0 and later offers a new approach to occlusion culling using occluder nodes. See Occlusion culling for instructions on setting up occlusion culling in your scene.

In some cases, you may have to adapt your level design to add more occlusion opportunities. For example, you may have to add more walls to prevent the player from seeing too far away, which would decrease performance due to the lost opportunities for occlusion culling.

Godot sorts objects by Material and Shader to improve performance. This, however, can not be done with transparent objects. Transparent objects are rendered from back to front to make blending with what is behind work. As a result, try to use as few transparent objects as possible. If an object has a small section with transparency, try to make that section a separate surface with its own material.

For more information, see the GPU optimizations doc.

In some situations, particularly at a distance, it can be a good idea to replace complex geometry with simpler versions. The end user will probably not be able to see much difference. Consider looking at a large number of trees in the far distance. There are several strategies for replacing models at varying distance. You could use lower poly models, or use transparency to simulate more complex geometry.

Godot 4 offers several ways to control level of detail:

An automatic approach on mesh import using Mesh level of detail (LOD).

A manual approach configured in the 3D node using Visibility ranges (HLOD).

Decals and lights can also benefit from level of detail using their respective Distance Fade properties.

While they can be used independently, these approaches are most effective when used together. For example, you can set up visibility ranges to hide particle effects that are too far away from the player to notice. At the same time, you can rely on mesh LOD to make the particle effect's meshes rendered with less detail at a distance.

Visibility ranges are also a good way to set up impostors for distant geometry (see below).

The simplest version of using transparency to deal with LOD is billboards. For example, you can use a single transparent quad to represent a tree at distance. This can be very cheap to render, unless of course, there are many trees in front of each other. In this case, transparency may start eating into fill rate (for more information on fill rate, see GPU optimization).

An alternative is to render not just one tree, but a number of trees together as a group. This can be especially effective if you can see an area but cannot physically approach it in a game.

You can make imposters by pre-rendering views of an object at different angles. Or you can even go one step further, and periodically re-render a view of an object onto a texture to be used as an imposter. At a distance, you need to move the viewer a considerable distance for the angle of view to change significantly. This can be complex to get working, but may be worth it depending on the type of project you are making.

This is only implemented in the Forward+ renderer, not Mobile or Compatibility.

If you have many identical objects in your scene, you can use automatic instancing to reduce the number of draw calls. This automatically happens for MeshInstance3D nodes that use the same mesh and material: no manual setup is required.

For automatic instancing to be effective, the material must be opaque or alpha-tested (alpha scissor or alpha hash). Alpha-blended or depth pre-pass materials are never instanced this way. Instead, you must use MultiMesh as described below.

If several identical objects have to be drawn in the same place or nearby, try using MultiMesh instead. MultiMesh allows the drawing of many thousands of objects at very little performance cost, making it ideal for flocks, grass, particles, and anything else where you have thousands of identical objects.

See also the Using MultiMesh documentation.

Lighting objects is one of the most costly rendering operations. Realtime lighting, shadows (especially multiple lights), and global illumination are especially expensive. They may simply be too much for lower power mobile devices to handle.

Consider using baked lighting, especially for mobile. This can look fantastic, but has the downside that it will not be dynamic. Sometimes, this is a tradeoff worth making.

See Using Lightmap global illumination for instructions on using baked lightmaps. For best performance, you should set lights' bake mode to Static as opposed to the default Dynamic, as this will skip real-time lighting on meshes that have baked lighting.

The downside of lights with the Static bake mode is that they can't cast shadows onto meshes with baked lighting. This can make scenes with outdoor environments and dynamic objects look flat. A good balance between performance and quality is to keep Dynamic for the DirectionalLight3D node, and use Static for most (if not all) omni and spot lights.

Animation and vertex animation such as skinning and morphing can be very expensive on some platforms. You may need to lower the polycount considerably for animated models, or limit the number of them on screen at any given time. You can also reduce the animation rate for distant or occluded meshes, or pause the animation entirely if the player is unlikely to notice the animation being stopped.

The VisibleOnScreenEnabler3D and VisibleOnScreenNotifier3D nodes can be useful for this purpose.

If you are making large worlds, there are different considerations than what you may be familiar with from smaller games.

Large worlds may need to be built in tiles that can be loaded on demand as you move around the world. This can prevent memory use from getting out of hand, and also limit the processing needed to the local area.

There may also be rendering and physics glitches due to floating point error in large worlds. This can be resolved using Large world coordinates. If using large world coordinates is not an option, you may be able to use techniques such as orienting the world around the player (rather than the other way around), or shifting the origin periodically to keep things centred around Vector3(0, 0, 0).

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Particle sub-emitters — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/particles/subemitters.html

**Contents:**
- Particle sub-emitters
- Emitter mode
- Limitations
- User-contributed notes

Sometimes a visual effect cannot be created with a single particle system alone. Sometimes a particle system needs to be spawned as a response to something that happens in another particle system. Fireworks are a good example of that. They usually consist of several stages of explosions that happen in sequence. Sub-emitters are a good way to achieve this kind of effect.

Click to assign a sub-emitter...

...and select one from the scene

A sub-emitter is a particle system that spawns as a child of another particle system. You can add sub-emitters to sub-emitters, chaining particle effects as deep as you like.

To create a sub-emitter, you need at least two particle systems in the same scene. One of them will be the parent and one will be set as the child. Find the Sub Emitter property on the parent and click the box next to it to assign the sub-emitter. You will see a list of available particle systems in the scene. Select one and click the confirmation button.

Particle systems from instanced scenes can be set as sub-emitters too, as long as the Editable Children property is enabled on the instanced scene. This also works the other way around: You can assign a sub-emitter to a particle system in an instanced scene, even one coming from a different instanced scene.

When you set a particle system as the sub-emitter of another, the system stops emitting, even if the Emitting property was checked. Don't worry, it didn't break. This happens to every particle system as soon as it becomes a sub-emitter. You also won't be able to re-enable the property as long as the particle system is used as a sub-emitter.

Even though the parent particle system can be selected from the list of available particle systems, a particle system which is its own sub-emitter does not work in Godot. It will simply not spawn. The same is true for any other kind of recursive or self-referential sub-emitter setup.

When you assign a sub-emitter, you don't see it spawn right away. Emitting is disabled by default and needs to be enabled first. Set the Mode property in the Sub Emitter group of the ParticleProcessMaterial to something other than Disabled.

The emitter mode also determines how many sub-emitter particles are spawned. Constant spawns a single particle at a frequency set by the Frequency property. For At End and At Collision you can set the amount directly with the Amount At End and the Amount At Collision properties.

One thing to keep in mind is that the total number of active particles from the sub-emitter is always capped by the Amount property on the sub-emitter particle system. If you find that there are not enough particles spawned from the sub-emitter, you might have to increase the amount in the particle system.

Some emitter properties are ignored when a particle system is spawned as a sub-emitter. The Explosiveness property, for example, has no effect. Depending on the emitter mode, the particles are either spawned sequentially at fixed intervals or explosively all at once.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Particle systems (3D) — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/particles/index.html

**Contents:**
- Particle systems (3D)
- Introduction
  - Particles
  - Emitters
  - Node overview
- Basic usage
- Advanced topics

This section of the tutorial covers (3D) GPU-accelerated particle systems. Most of the things discussed here apply to CPU particles as well.

You can use particle systems to simulate complex physical effects like fire, sparks, smoke, magical effects, and many more. They are very well suited for creating dynamic and organic behavior and adding "life" to your scenes.

The idea is that a particle is emitted at a fixed interval and with a fixed lifetime. During its lifetime, every particle will have the same base behavior. What makes each particle different from the others and creates the organic look is the randomness that you can add to most of its parameters and behaviors.

Every particle system you create in Godot consists of two main parts: particles and emitters.

A particle is the visible part of a particle system. It's what you see on the screen when a particle system is active: The tiny specks of dust, the flames of a fire, the glowing orbs of a magical effect. You can have anywhere between a couple hundred and tens of thousands of particles in a single system. You can randomize a particle's size, its speed and movement direction, and change its color over the course of its lifetime. When you think of a fire, you can think of all the little embers flying away from it as individual particles.

An emitter is what's creating the particles. Emitters are usually not visible, but they can have a shape. That shape controls where and how particles are spawned, for example whether they should fill a room like dust or shoot away from a single point like a fountain. Going back to the fire example, an emitter would be the heat at the center of the fire that creates the embers and the flames.

All 3D particle nodes available in Godot

There are two types of 3D particle systems in Godot: GPUParticles3D, which are processed on the GPU, and CPUParticles3D, which are processed on the CPU.

CPU particle systems are less flexible than their GPU counterpart, but they work on a wider range of hardware and provide better support for older devices and mobile phones. Because they are processed on the CPU, they are not as performant as GPU particle systems and can't render as many individual particles. In addition they currently do not have all the available options GPU particles have for control.

GPU particle systems run on the GPU and can render hundreds of thousands of particles on modern hardware. You can write custom particle shaders for them, which makes them very flexible. You can also make them interact with the environment by using attractor and collision nodes.

There are three particle attractor nodes: GPUParticlesAttractorBox3D, GPUParticlesAttractorSphere3D, and GPUParticlesAttractorVectorField3D. An attractor node applies a force to all particles in its reach and pulls them closer or pushes them away based on the direction of that force.

There are several particle collision nodes. GPUParticlesCollisionBox3D and GPUParticlesCollisionSphere3D are the simple ones. You can use them to create basic shapes like boxes, a floor, or a wall that particles collide with. The other two nodes provide more complex collision behavior. The GPUParticlesCollisionSDF3D is useful when you want indoor scenes to collide with particles without having to create all the individual box and sphere colliders by hand. If you want particles to collide with large outdoor scenes, you would use the GPUParticlesCollisionHeightField3D node. It creates a heightmap of your world and the objects in it and uses that for large-scale particle collisions.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Particle turbulence — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/particles/turbulence.html

**Contents:**
- Particle turbulence
- Noise properties
- Influence properties
- Displacement properties
- User-contributed notes

Turbulence uses a noise texture to add variation and interesting patterns to particle movement. It can be combined with particle attractors and collision nodes to create even more complex looking behavior.

Particle turbulence properties

There are two things you have to do before turbulence has any effect on a particle system. First you must add movement to the particle system. Turbulence modifies a particle's movement direction and speed, but it doesn't create any. It is enough to give the particle system some gravity, but you can just as well create a number of attractors if you want the particles to follow a more complex movement path. Second, you need to enable turbulence in the particle process material. Once enabled, you have access to all the turbulence properties.

Turbulence makes use of 3D noise, which has a high performance cost on the GPU. Only enable turbulence on a few particle systems on screen at most. Using turbulence is not recommended when targeting mobile/web platforms.

The basis for particle turbulence is a noise pattern. There are several properties that allow you to manipulate different attributes of this pattern.

The Noise Strength property controls the pattern's contrast, which affects the overall turbulence sharpness. A lower value creates a softer pattern where individual movement paths are not as sharply separated from another. Set this to a higher number to make the pattern more distinct.

At a value of 1 (left), the noise strength produces softer turbulence patterns than at 20 (right)

The Noise Scale property controls the pattern's frequency. It basically changes the noise texture's UV scale where a smaller value produces finer detail, but repeating patterns become noticeable faster. A larger value results in a weaker turbulence pattern overall, but the particle system can cover a larger area before repetition starts to become an issue.

Turbulence noise scale produces finer details at a value of 1.5 (left) than at 6 (right)

The Noise Speed property takes a vector and controls the noise panning speed and direction. This allows you to move the noise pattern over time, which adds another layer of movement variation to the particle system.

Don't mix up particle movement speed and noise panning speed! They are two different things. Particle movement is determined by a number of properties, including the turbulence noise. The Noise Speed property moves the pattern itself, which in turn changes where the noise affects the particles.

At a value of (X=0,Y=0,Z=0), the noise pattern doesn't move at all. The influence on particle movement stays the same at any given point. Set the speed to (X=1,Y=0,Z=0) instead, and the noise pattern moves along the X-axis.

Different noise speed values. Left: (X=0,Y=0,Z=0), middle: (X=0.5,Y=0.5,Z=0.5), right: (X=0,Y=-2,Z=0).

The Noise Speed Random property adds some randomness to the noise panning speed. This helps with breaking up visible patterns, especially at higher panning speeds when repetition becomes noticeable faster.

The influence properties determine how much each particle is affected by turbulence. Use Influence Min to set a minimum value and Influence Max to set a maximum value. When a particle spawns, the influence is randomly chosen from within this range. You can also set up a curve with the Influence Over Life property that modifies that value over each particle's lifetime. These three properties together control the strength of the turbulence's effect on the particle system as described before.

Since these properties affect the overall influence of the turbulence over a particle system, both movement direction and speed change as you set different values. A stronger influence causes a particle to move faster and all particles to follow along narrower paths as a result of that.

Notice how the particle paths are more narrow and less spread out at high influence values (right)

Displacement changes a particle's starting position. Use Initial Displacement Min to set a lower limit and Initial Displacement Max to set an upper limit. When a particle spawns, the amount of displacement is randomly chosen from within this range and multiplied by a random direction.

Displacement is very useful to break up regular shapes or to create complex shapes from simpler ones. The only difference between the particle systems in the screenshot below is the value given to the displacement properties.

No displacement (left), displacement value of 5 (middle), displacement range [-20, 20] (right)

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Physical light and camera units — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/physical_light_and_camera_units.html

**Contents:**
- Physical light and camera units
- Why use physical light and camera units?
  - Advantages of physical units
  - Disadvantages of physical units
- Setting up physical light units
  - Enable the project setting
  - Configure the camera
  - Configure the environment
  - Configure the light nodes
- Setting up physical camera units

Godot uses arbitrary units for many physical properties that apply to light like color, energy, camera field of view, and exposure. By default, these properties use arbitrary units, because using accurate physical units comes with a few tradeoffs that aren't worth it for many games. As Godot favors ease of use by default, physical light units are disabled by default.

If you aim for photorealism in your project, using real world units as a basis can help make things easier to adjust. References for real world materials, lights and scene brightness are wildly available on websites such as Physically Based.

Using real world units in Godot can also be useful when porting a scene from other 3D software that uses physical light units (such as Blender).

The biggest disadvantage of using physical light units is you will have to pay close attention to the dynamic range in use at a given time. You can run into floating point precision errors when mixing very high light intensities with very low light intensities.

In practice, this means that you will have to manually manage your exposure settings to ensure that you aren't over-exposing or under-exposing your scene too much. Auto-exposure can help you balance the light in a scene to bring it into a normal range, but it can't recover lost precision from a dynamic range that is too high.

Using physical light and camera units will not automatically make your project look better. Sometimes, moving away from realism can actually make a scene look better to the human eye. Also, using physical units requires a greater amount of rigor compared to non-physical units. Most benefits of physical units can only be obtained if the units are correctly set to match real world reference.

Physical light units are only available in 3D rendering, not 2D.

Physical light units can be enabled separately from physical camera units.

To enable physical light units correctly, there are 4 steps required:

Enable the project setting.

Configure the camera.

Configure the environment.

Configure Light3D nodes.

Since physical light and camera units only require a handful of calculations to handle unit conversion, enabling them doesn't have any noticeable performance impact on the CPU. However, on the GPU side, physical camera units currently enforce depth of field. This has a moderate performance impact. To alleviate this performance impact, depth of field quality can be decreased in the advanced Project Settings.

Open the Project Settings, enable the Advanced toggle then enable Rendering > Lights And Shadows > Use Physical Light Units. Restart the editor.

When physical light units are enabled and if you have a WorldEnvironment node in your scene (i.e. the editor Environment is disabled), you must have a CameraAttributes resource assigned to the WorldEnvironment node. Otherwise, the 3D editor viewport will appear extremely bright if you have a visible DirectionalLight3D node.

On the Camera3D node, you can add a CameraAttributes resource to its Attributes property. This resource is used to control the camera's depth of field and exposure. When using CameraAttributesPhysical, its focal length property is also used to adjust the camera's field of view.

When physical light units are enabled, the following additional properties become available in CameraAttributesPhysical's Exposure section:

Aperture: The size of the aperture of the camera, measured in f-stops. An f-stop is a unitless ratio between the focal length of the camera and the diameter of the aperture. A high aperture setting will result in a smaller aperture which leads to a dimmer image and sharper focus. A low aperture results in a wide aperture which lets in more light resulting in a brighter, less-focused image.

Shutter Speed: The time for shutter to open and close, measured in inverse seconds (1/N). A lower value will let in more light leading to a brighter image, while a higher value will let in less light leading to a darker image. When getting or setting this property with a script, the unit is in seconds instead of inverse seconds.

Sensitivity: The sensitivity of camera sensors, measured in ISO. A higher sensitivity results in a brighter image. When auto exposure is enabled, this can be used as a method of exposure compensation. Doubling the value will increase the exposure value (measured in EV100) by 1 stop.

Multiplier: A non-physical exposure multiplier. Higher values will increase the scene's brightness. This can be used for post-processing adjustments or for animation purposes.

The default Aperture value of 16 f-stops is appropriate for outdoors at daytime (i.e. for use with a default DirectionalLight3D). For indoor lighting, a value between 2 and 4 is more appropriate.

Typical shutter speed used in photography and movie production is 1/50 (0.02 seconds). Night-time photography generally uses a shutter around 1/10 (0.1 seconds), while sports photography uses a shutter speed between 1/250 (0.004 seconds) and 1/1000 (0.001 seconds) to reduce motion blur.

In real life, sensitivity is usually set between 50 ISO and 400 ISO for daytime outdoor photography depending on weather conditions. Higher values are used for indoor or night-time photography.

Unlike real life cameras, the adverse effects of increasing ISO sensitivity or decreasing shutter speed (such as visible grain or light trails) are not simulated in Godot.

See Setting up physical camera units for a description of CameraAttributesPhysical properties that are also available when not using physical light units.

The default configuration is designed for daytime outdoor scenes. Night-time and indoor scenes will need adjustments to the DirectionalLight3D and WorldEnvironment background intensity to look correct. Otherwise, positional lights will be barely visible at their default intensity.

If you haven't added a WorldEnvironment and Camera3D node to the current scene yet, do so now by clicking the 3 vertical dots at the top of the 3D editor viewport. Click Add Sun to Scene, open the dialog again then click Add Environment to Scene.

After enabling physical light units, a new property becomes available to edit in the Environment resource:

Background Intensity: The background sky's intensity in nits (candelas per square meter). This also affects ambient and reflected light if their respective modes are set to Background. If a custom Background Energy is set, this energy is multiplied by the intensity.

After enabling physical light units, 2 new properties become available in Light3D nodes:

Intensity: The light's intensity in lux (DirectionalLight3D) or lumens (OmniLight3D/SpotLight3D). If a custom Energy is set, this energy is multiplied by the intensity.

Temperature: The light's color temperature defined in Kelvin. If a custom Color is set, this color is multiplied by the color temperature.

OmniLight3D/SpotLight3D intensity

Lumens are a measure of luminous flux, which is the total amount of visible light emitted by a light source per unit of time.

For SpotLight3Ds, we assume that the area outside the visible cone is surrounded by a perfect light absorbing material. Accordingly, the apparent brightness of the cone area does not change as the cone increases and decreases in size.

A typical household lightbulb can range from around 600 lumens to 1200 lumens. A candle is about 13 lumens, while a streetlight can be approximately 60000 lumens.

DirectionalLight3D intensity

Lux is a measure pf luminous flux per unit area, it is equal to one lumen per square metre. Lux is the measure of how much light hits a surface at a given time.

With DirectionalLight3D, on a clear sunny day, a surface in direct sunlight may receive approximately 100000 lux. A typical room in a home may receive approximately 50 lux, while the moonlit ground may receive approximately 0.1 lux.

6500 Kelvin is white. Higher values result in colder (bluer) colors, while lower values result in warmer (more orange) colors.

The sun on a cloudy day is approximately 6500 Kelvin. On a clear day, the sun is between 5500 to 6000 Kelvin. On a clear day at sunrise or sunset, the sun ranges to around 1850 Kelvin.

Color temperature chart from 1,000 Kelvin (left) to 12,500 Kelvin (right)

Other Light3D properties such as Energy and Color remain editable for animation purposes, and when you occasionally need to create lights with non-realistic properties.

Physical camera units can be enabled separately from physical light units.

After adding a CameraAttributesPhysical resource to the Camera Attributes property of a Camera3D nodes, some properties such as FOV will no longer be editable. Instead, these properties are now governed by the CameraAttributesPhysical's properties, such as focal length and aperture.

CameraAttributesPhysical offers the following properties in its Frustum section:

Focus Distance: Distance from camera of object that will be in focus, measured in meters. Internally, this will be clamped to be at least 1 millimeter larger than the Focal Length.

Focal Length: Distance between camera lens and camera aperture, measured in millimeters. Controls field of view and depth of field. A larger focal length will result in a smaller field of view and a narrower depth of field meaning fewer objects will be in focus. A smaller focal length will result in a wider field of view and a larger depth of field, which means more objects will be in focus. This property overrides the Camera3D's FOV and Keep Aspect properties, making them read-only in the inspector.

Near/Far: The near and far clip distances in meters. These behave the same as the Camera3D properties of the same name. Lower Near values allow the camera to display objects that are very close, at the cost of potential precision (Z-fighting) issues in the distance. Higher Far values allow the camera to see further away, also at the cost of potential precision (Z-fighting) issues in the distance.

The default focal length of 35 mm corresponds to a wide angle lens. It still results in a field of view that is noticeably narrower compared to the default "practical" vertical FOV of 75 degrees. This is because non-gaming use cases such as filmmaking and photography favor using a narrower field of view for a more cinematic appearance.

Common focal length values used in filmmaking and photography are:

Fisheye (ultrawide angle): Below 15 mm. Nearly no depth of field visible.

Wide angle: Between 15 mm and 50 mm. Reduced depth of field.

Standard: Between 50 mm and 100 mm. Standard depth of field.

Telephoto: Greater than 100 mm. Increased depth of field.

Like when using the Keep Height aspect mode, the effective field of view depends on the viewport's aspect ratio, with wider aspect ratios automatically resulting in a wider horizontal field of view.

Automatic exposure adjustment based on the camera's average brightness level can also be enabled in the Auto Exposure section, with the following properties:

Min Sensitivity: The darkest brightness the camera is allowed to get to, measured in EV100.

Max Sensitivity: The brightest the camera is allowed to get to, measured in EV100.

Speed: The speed of the auto exposure effect. Affects the time needed for the camera to perform auto exposure. Higher values allow for faster transitions, but the resulting adjustments may look distracting depending on the scene.

Scale: The scale of the auto exposure effect. Affects the intensity of auto exposure.

EV100 is an exposure value (EV) measured at an ISO sensitivity of 100. See this table for common EV100 values found in real life.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Procedural geometry — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/procedural_geometry/index.html

**Contents:**
- Procedural geometry
- What is geometry?
- What is a Mesh?
- What a Mesh is
  - Surfaces
  - Surface array
- Tools
  - ArrayMesh
  - MeshDataTool
  - SurfaceTool

There are many ways to procedurally generate geometry in Godot. In this tutorial series, we will explore a few of them. Each technique has its own benefits and drawbacks, so it is best to understand each one and how it can be useful in a given situation.

All the procedural geometry generation methods described here run on the CPU. Godot doesn't support generating geometry on the GPU yet.

Geometry is a fancy way of saying shape. In computer graphics, geometry is typically represented by an array of positions called "vertices". In Godot, geometry is represented by Meshes.

Many things in Godot have mesh in their name: the Mesh, the ArrayMesh, the ImmediateMesh, the MeshInstance3D, the MultiMesh, and the MultiMeshInstance3D. While they are all related, they have slightly different uses.

Meshes and ArrayMeshes are resources that are drawn using a MeshInstance3D node. Resources like Meshes and ArrayMeshes cannot be added to the scene directly. A MeshInstance3D represents one instance of a mesh in your scene. You can reuse a single mesh in multiple MeshInstance3Ds to draw it in different parts of your scene with different materials or transformations (scale, rotation, position etc.).

If you are going to draw the same object many times, it can be helpful to use a MultiMesh with a MultiMeshInstance3D. MultiMeshInstance3Ds draw meshes thousands of times very cheaply by taking advantage of hardware instancing. The drawback with using a MultiMeshInstance3D is that each of your mesh's surfaces are limited to one material for all instances. It uses an instance array to store different colors and transformations for each instance, but all the instances of each surface use the same material.

A Mesh is composed of one or more surfaces. A surface is an array composed of multiple sub-arrays containing vertices, normals, UVs, etc. Normally the process of constructing surfaces and meshes is hidden from the user in the RenderingServer, but with ArrayMeshes, the user can construct a Mesh manually by passing in an array containing the surface information.

Each surface has its own material. Alternatively, you can override the material for all surfaces in the Mesh when you use a MeshInstance3D using the material_override property.

The surface array is an array of length ArrayMesh.ARRAY_MAX. Each position in the array is filled with a sub-array containing per-vertex information. For example, the array located at ArrayMesh.ARRAY_NORMAL is a PackedVector3Array of vertex normals. See Mesh.ArrayType for more information.

The surface array can be indexed or non-indexed. Creating a non-indexed array is as easy as not assigning an array at the index ArrayMesh.ARRAY_INDEX. A non-indexed array stores unique vertex information for every triangle, meaning that when two triangles share a vertex, the vertex is duplicated in the array. An indexed surface array only stores vertex information for each unique vertex and then also stores an array of indices which maps out how to construct the triangles from the vertex array. In general, using an indexed array is faster, but it means you have to share vertex data between triangles, which is not always desired (e.g. when you want per-face normals).

Godot provides different ways of accessing and working with geometry. More information on each will be provided in the following tutorials.

The ArrayMesh resource extends Mesh to add a few different quality of life functions and, most importantly, the ability to construct a Mesh surface through scripting.

For more information about the ArrayMesh, please see the ArrayMesh tutorial.

The MeshDataTool is a resource that converts Mesh data into arrays of vertices, faces, and edges that can be modified at runtime.

For more information about the MeshDataTool, please see the MeshDataTool tutorial.

The SurfaceTool allows the creation of Meshes using an OpenGL 1.x immediate mode style interface.

For more information about the SurfaceTool, please see the SurfaceTool tutorial.

ImmediateMesh is a mesh that uses an immediate mode style interface (like SurfaceTool) to draw objects. The difference between ImmediateMesh and the SurfaceTool is that ImmediateMesh is drawn directly with code dynamically, while the SurfaceTool is used to generate a Mesh that you can do whatever you want with.

ImmediateMesh is useful for prototyping because of its straightforward API, but it is slow because the geometry is rebuilt each time you make a change. It is most useful for adding simple geometry for visual debugging (e.g. by drawing lines to visualize physics raycasts etc.).

For more information about ImmediateMesh, please see the ImmediateMesh tutorial.

Which approach you use depends on what you are trying to do and what kind of procedure you are comfortable with.

Both SurfaceTool and ArrayMesh are excellent for generating static geometry (meshes) that don't change over time.

Using an ArrayMesh is slightly faster than using a SurfaceTool, but the API is a little more challenging. Additionally, SurfaceTool has a few quality of life methods such as generate_normals() and index().

ImmediateMesh is more limited than both ArrayMesh and SurfaceTool. However, if you need the geometry to change every frame anyway, it provides a much easier interface that can be slightly faster than generating an ArrayMesh every frame.

The MeshDataTool is not fast, but it gives you access to all kinds of properties of the mesh that you don't get with the others (edges, faces, etc.). It is incredibly useful when you need that sort of data to transform the mesh, but it is not a good idea to use it if that extra information is not needed. The MeshDataTool is best used if you are going to be using an algorithm that requires access to the face or edge array.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Process material properties — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/particles/process_material_properties.html

**Contents:**
- Process material properties
- Time
- Particle flags
- Spawn
- Emission shape
- Angle
- Direction
- Initial velocity
- Accelerations
- Gravity

Min, max, and curve properties

The properties in this material control how particles behave and change over their lifetime. A lot of them have Min, Max, and Curve values that allow you to fine-tune their behavior. The relationship between these values is this: When a particle is spawned, the property is set with a random value between Min and Max. If Min and Max are the same, the value will always be the same for every particle. If the Curve is also set, the value of the property will be multiplied by the value of the curve at the current point in a particle's lifetime. Use the curve to change a property over the particle lifetime. Very complex behavior can be expressed this way.

This page covers how to use ParticleProcessMaterial for 3D scenes specifically. For information on how to use it in a 2D Scene see ParticleProcessMaterial 2D Usage.

The Lifetime Randomness property controls how much randomness to apply to each particle's lifetime. A value of 0 means there is no randomness at all and all particles live for the same amount of time, set by the Lifetime property. A value of 1 means that a particle's lifetime is completely random within the range of [0.0, Lifetime].

The Align Y property aligns each particle's Y-axis with its velocity. Enabling this property is the same as setting the Transform Align property to Y to Velocity.

The Rotate Y property works with the properties in the Angle and Angular Velocity groups to control particle rotation. Rotate Y has to be enabled if you want to apply any rotation to particles. The exception to this is any particle that uses the Standard Material where the Billboard property is set to Particle Billboard. In that case, particles rotate even without Rotate Y enabled.

When the Disable Z property is enabled, particles will not move along the Z-axis. Whether that is going to be the particle system's local Z-axis or the world Z-axis is determined by the Local Coords property.

The Damping as Friction property changes the behavior of damping from a constant deceleration to a deceleration based on speed.

Particles can emit from a single point in space or in a way that they fill out a shape. The Shape property controls that shape. Point is the default value. All particles emit from a single point in the center of the particle system. When set to Sphere or Box, particles emit in a way that they fill out a sphere or a box shape evenly. You have full control over the size of these shapes. Sphere Surface works like Sphere, but instead of filling it out, all particles spawn on the sphere's surface.

Particles emitting from a point (left), in a sphere (middle), and in a box (right)

A ring-shaped particle system

The Ring emission shape makes particles emit in the shape of a ring. You can control the ring's direction by changing the Ring Axis property. Ring Height controls the thickness of the ring along its axis. Ring Radius and Ring Inner Radius control how wide the ring is and how large the hole in the middle should be. The image shows a particle system with a radius of 2 and an inner radius of 1.5, the axis points along the global Z-axis.

In addition to these relatively simple shapes, you can select the Points or Directed Points option to create highly complex emission shapes. See the Complex emission shapes section for a detailed explanation of how to set these up.

The Angle property controls a particle's starting rotation as described above. In order to have an actual effect on the particle, you have to enable one of two properties: Rotate Y rotates the particle around the particle system's Y-axis. The Billboard property in the Standard Material, if it is set to Particle Billboard, rotates the particle around the axis that points from the particle to the camera.

The Direction property alone is not enough to see any particle movement. Whatever values you set here only take effect once velocity or acceleration properties are set, too.

The Direction property is a vector that controls each particle's direction of movement at the moment it is spawned. A value of (X=1,Y=0,Z=0) would make all particles move sideways along the X-axis. For something like a fountain where particles shoot out up in the air, a value of (X=0,Y=1,Z=0) would be a good starting point.

Different direction values: Y-axis only (left), equal values for X and Y (middle), X and Y with gravity enabled (right)

After setting a direction, you will notice that all particles move in the same direction in a straight line. The Spread property adds some variation and randomness to each particle's direction. The higher the value, the stronger the deviation from the original path. A value of 0 means there is no spread at all while a value of 180 makes particles shoot out in every direction. You could use this for something like pieces of debris during an explosion effect.

No spread (left), 45 degree angle (middle), full 180 degrees (right)

The Flatness property limits the spread along the Y-axis. A value of 0 means there is no limit and a value of 1 will eliminate all particle movement along the Y-axis. The particles will spread out completely "flat".

You won't see any actual movement until you also set some values for the velocity and acceleration properties below, so let's take a look at those next.

While the Direction property controls a particle's movement direction, the Initial Velocity controls how fast it goes. It's separated into Velocity Min and Velocity Max, both set to 0 by default, which is why you don't see any movement initially. As soon as you set values for either of these properties as described above, the particles begin to move. The direction is multiplied by these values, so you can make particles move in the opposite direction by setting a negative velocity.

The next few property groups work closely together to control particle movement and rotation. Gravity drags particles in the direction it points at, which is straight down at the strength of Earth's gravity by default. Gravity affects all particle movement. If your game uses physics and the world's gravity can change at runtime, you can use this property to keep the game's gravity in sync with particle gravity. A Gravity value of (X=0,Y=0,Z=0) means no particle will ever move at all if none of the other movement properties are set.

Left: (X=0,Y=-9.8,Z=0), middle: (X=0,Y=9.8,Z=0), right: (X=4,Y=2,Z=0).

Angular Velocity controls a particle's speed of rotation as described above. You can reverse the direction by using negative numbers for Velocity Min or Velocity Max. Like the Angle property, the rotation will only be visible if the Rotate Y flag is set or the Particle Billboard mode is selected in the Standard Material.

The Damping property has no effect on the angular velocity.

A particle's velocity is a constant value: once it's set, it doesn't change and the particle will always move at the same speed. You can use the Linear Accel property to change the speed of movement over a particle's lifetime as described above. Positive values will speed up the particle and make it move faster. Negative values will slow it down until it stops and starts moving in the other direction.

Negative (top) and positive (bottom) linear acceleration

It's important to keep in mind that when we change acceleration, we're not changing the velocity directly, we're changing the change in velocity. A value of 0 on the acceleration curve does not stop the particle's movement, it stops the change in the particle's movement. Whatever its velocity was at that moment, it will keep moving at that velocity until the acceleration is changed again.

The Radial Accel property adds a gravity-like force to all particles, with the origin of that force at the particle system's current location. Negative values make particles move towards the center, like the force of gravity from a planet on objects in its orbit. Positive values make particles move away from the center.

Negative (left) and positive (right) radial acceleration

Tangents on a circle

This property adds particle acceleration in the direction of the tangent to a circle on the particle system's XZ-plane with the origin at the system's center and a radius the distance between each particle's current location and the system's center projected onto that plane.

A tangent to a circle is a straight line that "touches" the circle in a right angle to the circle's radius at the touch point. A circle on the particle system's XZ-plane is the circle that you see when you look straight down at the particle system from above.

Tangential acceleration from above

Tangential Accel is always limited to that plane and never move particles along the system's Y-axis. A particle's location is enough to define such a circle where the distance to the system's center is the radius if we ignore the vector's Y component.

The Tangential Accel property will make particles orbit the particle system's center, but the radius will increase constantly. Viewed from above, particles will move away from the center in a spiral. Negative values reverse the direction.

The Damping property gradually stops all movement. Each frame, a particle's movement is slowed down a little unless the total acceleration is greater than the damping effect. If it isn't, the particle will keep slowing down until it doesn't move at all. The greater the value, the less time it takes to bring particles to a complete halt.

If you want the particle system to interact with particle attractors, you have to check the Enabled property. When it is disabled, the particle system ignores all particle attractors.

Scale controls a particle's size as described above. You can set different values for Scale Min and Scale Max to randomize each particle's size. Negative values are not allowed, so you won't be able to flip particles with this property. If you emit particles as billboards, the Keep Size property on the Standard Material in your draw passes has to be enabled for any scaling to have an effect.

The Color property controls a particle's initial color. It will have an effect only after the Use As Albedo property in the Vertex Color group of the Standard Material is enabled. This property is multiplied with color coming from the particle material's own Color or Texture property.

Setting up a color ramp

There are two Ramp properties in the Color group. These allow you to define a range of colors that are used to set the particle's color. The Color Ramp property changes a particle's color over the course of its lifetime. It moves through the entire range of colors you defined. The Color Initial Ramp property selects the particle's initial color from a random position on the color ramp.

To set up a color ramp, click on the box next to the property name and from the dropdown menu select New GradientTexture1D. Click on the box again to open the texture's details. Find the Gradient property, click on the box next to it and select New Gradient. Click on that box again and you will see a color range. Click anywhere on that range to insert a new marker. You can move the marker with the mouse and delete it by clicking the right mouse button. When a marker is selected, you can use the color picker next to the range to change its color.

Like the Color property, Hue Variation controls a particle's color, but in a different way. It does so not by setting color values directly, but by shifting the color's hue.

Hue describes a color's pigment: red, orange, yellow, green and so on. It does not tell you anything about how bright or how saturated the color is. The Hue Variation property controls the range of available hues as described above.

It works on top of the particle's current color. The values you set for Variation Min and Variation Max control how far the hue is allowed to shift in either direction. A higher value leads to more color variation while a low value limits the available colors to the closest neighbors of the original color.

Different values for hue variation, both times with blue as base color: 0.6 (left) and 0.1 (right)

The Animation property group controls the behavior of sprite sheet animations in the particle's Standard Material. The Min, Max, and Curve values work as described above.

An animated sprite sheet is a texture that contains several smaller images aligned on a grid. The images are shown one after the other so fast that they combine to play a short animation, like a flipbook. You can use them for animated particles like smoke or fire. These are the steps to create an animated particle system:

An 8x8 animated smoke sprite sheet

Import a sprite sheet texture into the engine. If you don't have one at hand, you can download the high-res version of the example image.

Set up a particle system with at least one draw pass and assign a Standard Material to the mesh in that draw pass.

Assign the sprite sheet to the Texture property in the Albedo group

Set the material's Billboard property to Particle Billboard. Doing so makes the Particles Anim group available in the material.

Set H Frames to the number of columns and V Frames to the number of rows in the sprite sheet.

Check Loop if you want the animation to keep repeating.

That's it for the Standard Material. You won't see any animation right away. This is where the Animation properties come in. The Speed properties control how fast the sprite sheet animates. Set Speed Min and Speed Max to 1 and you should see the animation playing. The Offset properties control where the animation starts on a newly spawned particle. By default, it will always be the first image in the sequence. You can add some variety by changing Offset Min and Offset Max to randomize the starting position.

Three different particle systems using the same smoke sprite sheet

Depending on how many images your sprite sheet contains and for how long your particle is alive, the animation might not look smooth. The relationship between particle lifetime, animation speed, and number of images in the sprite sheet is this:

At an animation speed of 1.0, the animation will reach the last image in the sequence just as the particle's lifetime ends.

If your sprite sheet contains 64 (8x8) images and the particle's lifetime is set to 1 second, the animation will be very smooth at 64 FPS (1 second / 64 images). if the lifetime is set to 2 seconds, it will still be fairly smooth at 32 FPS. But if the particle is alive for 8 seconds, the animation will be visibly choppy at 8 FPS. In order to make the animation smooth again, you need to increase the animation speed to something like 3 to reach an acceptable framerate.

The same particle system at different lifetimes: 1 second (left), 2 seconds (middle), 8 seconds (right)

Note that the GPUParticles3D node's Fixed FPS also affects animation playback. For smooth animation playback, it's recommended to set it to 0 so that the particle is simulated on every rendered frame. If this is not an option for your use case, set Fixed FPS to be equal to the effective framerate used by the flipbook animation (see above for the formula).

Turbulence adds noise to particle movement, creating interesting and lively patterns. Check the box next to the Enabled property to activate it. A number of new properties show up that control the movement speed, noise pattern and overall influence on the particle system. You can find a detailed explanation of these in the section on particle turbulence.

The Mode property controls how and if emitters collide with particle collision nodes. Set it to Disabled to disable any collision for this particle system. Set it to Hide On Contact if you want particles to disappear as soon as they collide. Set it to Constant to make particles collide and bounce around. You will see two new properties appear in the inspector. They control how particles behave during collision events.

A high Friction value will reduce sliding along surfaces. This is especially helpful if particles collide with sloped surfaces and you want them to stay in place instead of sliding all the way to the bottom, like snow falling on a mountain. A high Bounce value will make particles bounce off surfaces they collide with, like rubber balls on a solid floor.

If the Use Scale property is enabled, the collision base size is multiplied by the particle's current scale. You can use this to make sure that the rendered size and the collision size match for particles with random scale or scale that varies over time.

You can learn more about particle collisions in the Collisions section in this manual.

The available sub-emitter modes

The Mode property controls how and when sub-emitters are spawned. Set it to Disabled and no sub-emitters will ever be spawned. Set it to Constant to make sub-emitters spawn continuously at a constant rate. The Frequency property controls how often that happens within the span of one second. Set the mode to At End to make the sub-emitter spawn at the end of the parent particle's lifetime, right before it is destroyed. The Amount At End property controls how many sub-emitters will be spawned. Set the mode to At Collision to make sub-emitters spawn when a particle collides with the environment. The Amount At Collision property controls how many sub-emitters will be spawned.

When the Keep Velocity property is enabled, the newly spawned sub-emitter starts off with the parent particle's velocity at the time the sub-emitter is created.

See the Sub-emitters section in this manual for a detailed explanation of how to add a sub-emitter to a particle system.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Prototyping levels with CSG — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/csg_tools.html

**Contents:**
- Prototyping levels with CSG
- Introduction to CSG nodes
  - CSG tools features
  - CSGPolygon
  - Custom meshes
    - Making an existing mesh manifold with Blender
  - CSGCombiner3D
  - Processing order
- Prototyping a level
- Using prototype textures

The content of this page was not yet updated for Godot 4.5 and may be outdated. If you know how to improve this page or you can confirm that it's up to date, feel free to open a pull request.

CSG stands for Constructive Solid Geometry, and is a tool to combine basic shapes or custom meshes to create more complex shapes. In 3D modeling software, CSG is mostly known as "Boolean Operators".

Level prototyping is one of the main uses of CSG in Godot. This technique allows users to create the most common shapes by combining primitives. Interior environments can be created by using inverted primitives.

The CSG nodes in Godot are mainly intended for prototyping. There is no built-in support for UV mapping or editing 3D polygons (though extruded 2D polygons can be used with the CSGPolygon3D node).

If you're looking for an easy to use level design tool for a project, you may want to use FuncGodot or Cyclops Level Builder instead.

You can check how to use CSG nodes to build various shapes (such as stairs or roads) using the Constructive Solid Geometry demo project.

Like other features of Godot, CSG is supported in the form of nodes. These are the CSG nodes:

CSGCylinder3D (also supports cone)

Every CSG node supports 3 kinds of boolean operations:

Union: Geometry of both primitives is merged, intersecting geometry is removed.

Intersection: Only intersecting geometry remains, the rest is removed.

Subtraction: The second shape is subtracted from the first, leaving a dent with its shape.

The CSGPolygon3D node extrude along a Polygon drawn in 2D (in X, Y coordinates) in the following ways:

Depth: Extruded back a given amount.

Spin: Extruded while spinning around its origin.

Path: Extruded along a Path node. This operation is commonly called lofting.

The Path mode must be provided with a Path3D node to work. In the Path node, draw the path and the polygon in CSGPolygon3D will extrude along the given path.

Custom meshes can be used for CSGMesh3D as long as the mesh is manifold. The mesh can be modeled in other software and imported into Godot. Multiple materials are supported.

For a mesh to be used as a CSG mesh, it is required to:

have each edge connect to only two faces

And it is recommended to avoid:

Godot uses the manifold library to implement CSG meshes. The technical definition of "manifold" used by Godot is the following, adapted from that library's definition of "manifold":

Every edge of every triangle must contain the same two vertices (by index) as exactly one other triangle edge, and the start and end vertices must switch places between these two edges. The triangle vertices must appear in clockwise order when viewed from the outside of the Godot Engine manifold mesh.

If you have an existing mesh that is not already manifold, you can make it manifold using Blender.

In Blender, install and enable the 3D Print Toolbox addon.

Select the mesh you want to make manifold. Open the sidebar by clicking on the arrow:

In the 3D Print tab, under Clean Up, click the Make Manifold button:

The mesh should now be manifold, and can be used as a custom mesh.

The CSGCombiner3D node is an empty shape used for organization. It will only combine children nodes.

Every CSG node will first process its children nodes and their operations: union, intersection, or subtraction, in tree order, and apply them to itself one after the other.

In the interest of performance, make sure CSG geometry remains relatively simple, as complex meshes can take a while to process. If adding objects together (such as table and room objects), create them as separate CSG trees. Forcing too many objects in a single tree will eventually start affecting performance. Only use binary operations where you actually need them.

We will prototype a room to practice the use of CSG tools.

Working in Orthogonal projection gives a better view when combining the CSG shapes.

Our level will contain these objects:

Create a scene with a Node3D node as root node.

The default lighting of the environment doesn't provide clear shading at some angles. Change the display mode using Display Overdraw in the 3D viewport menu, or add a DirectionalLight node to help you see clearly.

Create a CSGBox3D and name it room, enable Invert Faces and change the dimensions of your room.

Next, create a CSGCombiner3D and name it desk.

A desk has one surface and 4 legs:

Create 1 CSGBox3D children node in Union mode for the surface and adjust the dimensions.

Create 4 CSGBox3D children nodes in Union mode for the legs and adjust the dimensions.

Adjust their placement to resemble a desk.

CSG nodes inside a CSGCombiner3D will only process their operation within the combiner. Therefore, CSGCombiner3Ds are used to organize CSG nodes.

Create a CSGCombiner3D and name it bed.

Our bed consists of 3 parts: the bed, the mattress and a pillow. Create a CSGBox3D and adjust its dimension for the bed. Create another CSGBox3D and adjust its dimension for the mattress.

We will create another CSGCombiner3D named pillow as the child of bed. The scene tree should look like this:

We will combine 3 CSGSphere3D nodes in Union mode to form a pillow. Scale the Y axis of the spheres and enable Smooth Faces.

Select the pillow node and switch the mode to Subtraction; the combined spheres will cut a hole into the mattress.

Try to re-parent the pillow node to the root Node3D node; the hole will disappear.

This is to illustrate the effect of CSG processing order. Since the root node is not a CSG node, the CSGCombiner3D nodes are the end of the operations; this shows the use of CSGCombiner3D to organize the CSG scene.

Undo the re-parent after observing the effect. The bed you've built should look like this:

Create a CSGCombiner3D and name it lamp.

A lamp consists of 3 parts: the stand, the pole and the lampshade. Create a CSGCylinder3D, enable the Cone option and make it the stand. Create another CSGCylinder3D and adjust the dimensions to use it as a pole.

We will use a CSGPolygon3D for the lampshade. Use the Spin mode for the CSGPolygon3D and draw a trapezoid while in Front View (numeric keypad 1); this shape will extrude around the origin and form the lampshade.

Adjust the placement of the 3 parts to make it look like a lamp.

Create a CSGCombiner3D and name it bookshelf.

We will use 3 CSGBox3D nodes for the bookshelf. Create a CSGBox3D and adjust its dimensions; this will be the size of the bookshelf.

Duplicate the CSGBox3D and shorten the dimensions of each axis and change the mode to Subtraction.

You've almost built a shelf. Create one more CSGBox3D for dividing the shelf into two levels.

Position your furniture in your room as you like and your scene should look this:

You've successfully prototyped a room level with the CSG tools in Godot. CSG tools can be used for designing all kinds of levels, such as a maze or a city; explore its limitations when designing your game.

Godot's Standard Material 3D and ORM Material 3D supports triplanar mapping, which can be used to automatically apply a texture to arbitrary objects without distortion. This is handy when using CSG as Godot doesn't support editing UV maps on CSG nodes yet. Triplanar mapping is relatively slow, which usually restricts its usage to organic surfaces like terrain. Still, when prototyping, it can be used to quickly apply textures to CSG-based levels.

If you need some textures for prototyping, Kenney made a set of CC0-licensed prototype textures.

There are two ways to apply a material to a CSG node:

Applying it to a CSGCombiner3D node as a material override (Geometry > Material Override in the Inspector). This will affect its children automatically, but will make it impossible to change the material in individual children.

Applying a material to individual nodes (Material in the Inspector). This way, each CSG node can have its own appearance. Subtractive CSG nodes will apply their material to the nodes they're "digging" into.

To apply triplanar mapping to a CSG node, select it, go to the Inspector, click the [empty] text next to Material Override (or Material for individual CSG nodes). Choose New StandardMaterial3D. Click the newly created material's icon to edit it. Unfold the Albedo section and load a texture into the Texture property. Now, unfold the Uv1 section and check Triplanar. You can change the texture offset and scale on each axis by playing with the Scale and Offset properties just above. Higher values in the Scale property will cause the texture to repeat more often.

You can copy a StandardMaterial3D to reuse it across CSG nodes. To do so, click the dropdown arrow next to a material property in the Inspector and choose Copy. To paste it, select the node you'd like to apply the material onto, click the dropdown arrow next to its material property then choose Paste.

Since Godot 4.4, you can convert a CSG node and its children to a MeshInstance3D node.

This has several benefits:

Bake lightmaps, since UV2 can be generated on a MeshInstance3D.

Bake occlusion culling, since the occlusion culling bake process only takes MeshInstance3D into account.

Faster loading times, since the CSG mesh no longer needs to be rebuilt when the scene loads.

Better performance when updating the node's transform if using the mesh within another CSG node.

To convert a CSG node to a MeshInstance3D node, select it, then choose CSG > Bake Mesh Instance in the toolbar. The MeshInstance3D node will be created as a sibling. Note that the CSG node that was used for baking is not hidden automatically, so remember to hide it to prevent its geometry from overlapping with the newly created MeshInstance3D.

You can also create a trimesh collision shape using CSG > Bake Collision Shape. The generated CollisionShape3D node must be a child of a StaticBody3D or AnimatableBody3D node to be effective.

Remember to keep the original CSG node in the scene tree, so that you can perform changes to the geometry later if needed. To make changes to the geometry, remove the MeshInstance3D node and make the root CSG node visible again.

It can be useful to block out a level using CSG, then export it as a 3d model, to import into 3D modeling software. You can do this by selecting Scene > Export As... > glTF 2.0 Scene.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Reflection probes — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/global_illumination/reflection_probes.html

**Contents:**
- Reflection probes
- Visual comparison
- Setting up a ReflectionProbe
- ReflectionProbe properties
- ReflectionProbe blending
- Limitations
- User-contributed notes

As stated in the Standard Material 3D and ORM Material 3D, objects can show reflected and/or diffuse light. Reflection probes are used as a source of reflected and ambient light for objects inside their area of influence. They can be used to provide more accurate reflections than VoxelGI and SDFGI while being fairly cheap on system resources.

Since reflection probes can also store ambient light, they can be used as a low-end alternative to VoxelGI and SDFGI when baked lightmaps aren't viable (e.g. in procedurally generated levels).

Reflection probes can also be used at the same time as screen-space reflections to provide reflections for off-screen objects. In this case, Godot will blend together the screen-space reflections and reflections from reflection probes.

Not sure if ReflectionProbe is suited to your needs? See Which global illumination technique should I use? for a comparison of GI techniques available in Godot 4.

Reflection probe disabled. Environment sky is used as a fallback.

Reflection probe enabled.

Reflection probe enabled with LightmapGI used at the same time. The lightmap appears in the reflection.

By combining reflection probes with screen-space reflections, you can get the best of both worlds: high-quality reflections for general room structure (that remain present when off-screen), while also having real-time reflections for small details.

Reflections in a room using ReflectionProbe only. Notice how small details don't have any reflections.

Reflections in a room using screen-space reflections only. Notice how the reflection on the sides of the room's walls is partly missing due to being off-screen.

Reflections in a room using ReflectionProbe and screen-space reflections together. The screen-space reflections are blended with the reflection probe, acting as a fallback in situations where the reflection probe fails to display any reflection.

Add a ReflectionProbe node.

Configure the ReflectionProbe's extents in the inspector to fit your scene. To get reasonably accurate reflections, you should generally have one ReflectionProbe node per room (sometimes more for large rooms).

Remember that ReflectionProbe extents don't have to be square, and you can even rotate the ReflectionProbe node to fit rooms that aren't aligned with the X/Z grid. Use this to your advantage to better cover rooms without having to place too many ReflectionProbe nodes.

Update Mode: Controls when the reflection probe updates. Once only renders the scene once every time the ReflectionProbe is moved. This makes it much faster to render compared to the Always update mode, which forces the probe to re-render everything around it every frame. Leave this property on Once (default) unless you need the reflection probe to update every frame.

Intensity: The brightness of the reflections and ambient lighting. This usually doesn't need to be changed from its default value of 1.0, but you can decrease it 1.0 if you find that reflections look too strong.

Max Distance: Controls the maximum distance used by the ReflectionProbe's internal camera. The distance is always at least equal to the Extents, but this can be increased to make objects located outside the extents visible in reflections. This property does not affect the maximum distance at which the ReflectionProbe itself is visible.

Extents: The area that will be affected by the ReflectionProbe's lighting and reflections.

Origin Offset: The origin to use for the internal camera used for reflection probe rendering. This must always be constrained within the Extents. If needed, adjust this to prevent the reflection from being obstructed by a solid object located exactly at the center of the ReflectionProbe.

Box Projection: Controls whether parallax correction should be used when rendering the reflection probe. This adjusts the reflection's appearance depending on the camera's position (relative to the reflection probe). This has a small performance cost, but the quality increase is often worth it in box-shaped rooms. Note that this effect doesn't work quite as well in rooms with less regular shapes (such as ellipse-shaped rooms).

Interior: If enabled, ambient lighting will not be sourced from the environment sky, and the background sky won't be rendered onto the reflection probe.

Enable Shadows: Controls whether real-time light shadows should be rendered within the reflection probe. Enable this to improve reflection quality at the cost of performance. This should be left disabled for reflection probes with the Always mode, as it's very expensive to render reflections with shadows every frame. Fully baked light shadows are not affected by this setting and will be rendered in the reflection probe regardless.

Cull Mask: Controls which objects are visible in the reflection. This can be used to improve performance by excluding small objects from the reflection. This can also be used to prevent an object from having self-reflection artifacts in situations where Origin Offset can't be used.

Mesh LOD Threshold: The automatic level of detail threshold to use for rendering meshes within the reflection. This only affects meshes that have automatic LODs generated for them. Higher values can improve performance by using less detailed geometry, especially for objects that are far away from the reflection's origin. The visual difference of using less detailed objects is usually not very noticeable during gameplay, especially in rough reflections.

The Ambient category features several properties to adjust ambient lighting rendered by the ReflectionProbe:

Mode: If set to Disabled, no ambient light is added by the probe. If set to Environment, the ambient light color is automatically sampled from the environment sky (if Interior is disabled) and the reflection's average color. If set to Constant Color, the color specified in the Color property is used instead. The Constant Color mode can be used as an approximation of area lighting.

Color: The color to use when the ambient light mode is set to Constant Mode.

Color Energy: The multiplier to use for the ambient light custom Color. This only has an effect when the ambient light mode is Custom Color.

To make transitions between reflection sources smoother, Godot supports automatic probe blending:

Up to 4 ReflectionProbes can be blended together at a given location. A ReflectionProbe will also fade out smoothly back to environment lighting when it isn't touching any other ReflectionProbe node.

SDFGI and VoxelGI will blend in smoothly with ReflectionProbes if used. This allows placing ReflectionProbes strategically to get more accurate (or fully real-time) reflections where needed, while still having rough reflections available in the VoxelGI or SDFGI's area of influence.

To make several ReflectionProbes blend with each other, you need to have part of each ReflectionProbe overlap each other's area. The extents should only overlap as little possible with other reflection probes to improve rendering performance (typically a few units in 3D space).

When using the Forward+ renderer, Godot uses a clustering approach for reflection probe rendering. As many reflection probes as desired can be added (as long as performance allows). However, there's still a default limit of 512 clustered elements that can be present in the current camera view. A clustered element is an omni light, a spot light, a decal or a reflection probe. This limit can be increased by adjusting Max Clustered Elements in Project Settings > Rendering > Limits > Cluster Builder.

When using the Mobile renderer, only 8 reflection probes can be applied on each individual Mesh resource. If there are more reflection probes affecting a single mesh, not all of them will be rendered on the mesh.

Similarly, when using the Compatibility renderer, up to 2 reflection probes can be applied per mesh. If more than 2 reflection probes affect a single mesh, additional probes will not be rendered.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Resolution scaling — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/resolution_scaling.html

**Contents:**
- Resolution scaling
- Why use resolution scaling?
- Resolution scaling options
  - Scaling mode
  - Rendering scale
  - FSR Sharpness
  - Mipmap bias
- Troubleshooting
  - Performance does not increase much when decreasing resolution scale
- User-contributed notes

With the ever-increasing rendering complexity of modern games, rendering at native resolution isn't always viable anymore, especially on lower-end GPUs.

Resolution scaling is one of the most direct ways to influence the GPU requirements of a scene. In scenes that are bottlenecked by the GPU (rather than by the CPU), decreasing the resolution scale can improve performance significantly. Resolution scaling is particularly important on mobile GPUs where performance and power budgets are limited.

While resolution scaling is an important tool to have, remember that resolution scaling is not intended to be a replacement for decreasing graphics settings on lower-end hardware. Consider exposing both resolution scale and graphics settings in your in-game menus.

You can compare resolution scaling modes and factors in action using the 3D Antialiasing demo project.

Resolution scaling is currently not available for 2D rendering, but it can be simulated using the viewport stretch mode. See Multiple resolutions for more information.

In the advanced Project Settings' Rendering > Scaling 3D section, you can find several options for 3D resolution scaling:

Bilinear: Standard bilinear filtering (default). This is used as a fallback when the current renderer doesn't support FSR 1.0 or FSR 2.2. Available in all renderers.

FSR 1.0: AMD FidelityFX Super Resolution 1.0. Slower, but higher quality compared to bilinear scaling. On very slow GPUs, the cost of FSR1 may be too expensive to be worth using it over bilinear scaling. Only available when using the Forward+ renderer.

FSR 2.2: AMD FidelityFX Super Resolution 2.2 (since Godot 4.2). Slowest, but even higher quality compared to FSR1 and bilinear scaling. On slow GPUs, the cost of FSR2 may be too expensive to be worth using it over bilinear scaling or FSR1. To match FSR2 performance with FSR1, you need to use a lower resolution scale factor. Only available when using the Forward+ renderer.

Here are comparison images between native resolution, bilinear scaling with 50% resolution scale, FSR1, and FSR2 scaling with 50% resolution scale:

FSR1 upscaling works best when coupled with another form of antialiasing. Temporal antialiasing (TAA) or multisample antialiasing (MSAA) should preferably be used in this case, as FXAA does not add temporal information and introduces more blurring to the image.

On the other hand, FSR2 provides its own temporal antialiasing. This means you don't need to enable other antialiasing methods for the resulting image to look smooth. The Use TAA project setting is ignored when FSR2 is used as the 3D scaling method, since FSR2's temporal antialiasing takes priority.

Here's the same comparison, but with 4× MSAA enabled on all images:

Notice how the edge upscaling of FSR1 becomes much more convincing once 4× MSAA is enabled. However, FSR2 doesn't benefit much from enabling MSAA since it already performs temporal antialiasing.

The Rendering > Scaling 3D > Scale setting adjusts the resolution scale. 1.0 represents the full resolution scale, with the 3D rendering resolution matching the 2D rendering resolution. Resolution scales below 1.0 can be used to speed up rendering, at the cost of a blurrier final image and more aliasing.

The rendering scale can be adjusted at runtime by changing the scaling_3d_scale property on a Viewport node.

Resolution scales above 1.0 can be used for supersample antialiasing (SSAA). This will provide antialiasing at a very high performance cost, and is not recommended for most use cases. See 3D antialiasing for more information.

The tables below list common screen resolutions, the resulting 3D rendering resolution and the number of megapixels that need to be rendered each frame depending on the rendering scale option. Rows are sorted from fastest to slowest in each table.

The resolution scale is defined on a per-axis basis. For example, this means that halving the resolution scale factor will reduce the number of rendered megapixels per frame by a factor of 4, not 2. Therefore, very low or very high resolution scale factors can have a greater performance impact than expected.

Resolution scale factor

3D rendering resolution

Megapixels rendered per frame

Resolution scale factor

3D rendering resolution

Megapixels rendered per frame

3840×2160 (Ultra HD "4K")

Resolution scale factor

3D rendering resolution

Megapixels rendered per frame

This is only available in the Forward+ renderer, not the Mobile or Compatibility renderers.

When using the FSR1 or FSR2 scaling modes, the sharpness can be controlled using the Rendering > Scaling 3D > FSR Sharpness advanced project setting.

The intensity is inverted compared to most other sharpness sliders: lower values will result in a sharper final image, while higher values will reduce the impact of the sharpening filter. 0.0 is the sharpest, while 2.0 is the least sharp. The default value of 0.2 provides a balance between preserving the original image's sharpness and avoiding additional aliasing due to oversharpening.

If you wish to use sharpening when rendering at native resolution, Godot currently doesn't allow using the sharpening component of FSR1 (RCAS) independently from the upscaling component (EASU).

As a workaround, you can set the 3D rendering scale to 0.99, set the scaling mode to FSR 1.0 then adjust FSR sharpness as needed. This allows using FSR1 while rendering at a near-native resolution.

Alternatively, you can set the scaling mode to FSR 2.2 with the 3D rendering scale set to 1.0 if you have enough GPU headroom. This also provides high-quality temporal antialiasing. The FSR Sharpness setting remains functional in this case.

This is only available in the Forward+ and Mobile renderers, not the Compatibility renderer.

Godot automatically uses a negative texture mipmap bias when the 3D resolution scale is set below 1.0. This allows for better preservation of texture detail at the cost of a grainy appearance on detailed textures.

The texture LOD bias currently affects both 2D and 3D rendering in the same way. However, keep in mind it only has an effect on textures with mipmaps enabled. Textures used in 2D don't have mipmaps enabled by default, which means only 3D rendering is affected unless you enabled mipmaps on 2D textures in the Import dock.

The formula used to determine the texture mipmap bias is: log2f(min(scaling_3d_scale, 1.0)) + custom_texture_mipmap_bias

To counteract the blurriness added by some antialiasing methods, Godot also adds a -0.25 offset when FXAA is enabled, and a -0.5 offset when TAA is enabled. If both are enabled at the same time, a -0.75 offset is used. This mipmap bias offset is applied before the resolution scaling offset, so it does not change depending on resolution scale.

The texture LOD bias can manually be changed by adjusting the Rendering > Textures > Default Filters > Texture Mipmap Bias advanced project setting. It can also be changed at runtime on Viewports by adjusting the texture_mipmap_bias property.

Adjusting the mipmap LOD bias manually can be useful in certain scenarios, but this should be done carefully to prevent the final image from looking grainy in motion.

Negative mipmap LOD bias can also decrease performance due to higher-resolution mips having to be sampled further away. Recommended values for a manual offset are between -0.5 and 0.0.

Positive mipmap LOD bias will make mipmapped textures appear blurrier than intended. This may improve performance slightly, but is otherwise not recommended as the loss in visual quality is usually not worth the performance gain.

The example below shows an extreme case, with a mipmap LOD bias of -1.0 and anisotropic filtering disabled to make the difference more noticeable:

If performance doesn't increase much when decreasing resolution scale to a value like 0.5, it likely means the performance bottleneck is elsewhere in your scene. For example, your scene could have too many draw calls, causing a CPU bottleneck to occur. Likewise, you may have too many graphics effects enabled for your GPU to handle (such as SDFGI, SSAO or SSR).

See the Performance tutorials for more information.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Retargeting 3D Skeletons — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/assets_pipeline/retargeting_3d_skeletons.html

**Contents:**
- Retargeting 3D Skeletons
- To share animations among multiple Skeletons
- Options for Retargeting
  - Bone Map
  - Remove Tracks
    - Except Bone Transform
    - Unimportant Positions
    - Unmapped Bones
  - Bone Renamer
    - Rename Bones

Godot has Position/Rotation/Scale 3D tracks (which this document calls "Transform" tracks) with Nodepaths to bones for Skeleton bone animation. This means you can't share animations between multiple Skeletons just by using the same bone names.

Godot allows each bone to have a parent-child relationship and can have rotation and scale as well as position, which means that bones that share a name can still have different Transform values.

The Skeleton stores the Transform values necessary for the default pose as Bone Rest. If Bone Pose is equal to Bone Rest, it means that the Skeleton is in the default pose.

Godot 3.x and Godot 4.0+ have different Bone Pose behaviors. In Godot 3.x, Bone Pose is relative to Bone Rest, but in Godot 4.0+, it includes Bone Rest. See this article.

Skeletal models have different Bone Rests depending on the environment from which they were exported. For example, the bones of a glTF model output from Blender have "Edit Bone Orientation" as the Bone Rest rotation. However, there are skeletal models without any Bone Rest rotations, such as the glTF model output from Maya.

To share animations in Godot, it is necessary to match Bone Rests as well as Bone Names to remove unwanted tracks in some cases. In Godot 4.0+, you can do that using the scene importer.

When you select the Skeleton3D node in the advanced scene import menu, a menu will appear on the right-hand side containing the "Retarget" section. The Retarget section has a single property bone_map.

With the Skeleton node selected, first set up a new BoneMap and SkeletonProfile. Godot has a preset called SkeletonProfileHumanoid for humanoid models. This tutorial proceeds with the assumption that you are using SkeletonProfileHumanoid.

If you need a profile that is different from SkeletonProfileHumanoid, you can export a SkeletonProfile from the editor by selecting a Skeleton3D and using the Skeleton3D menu in the 3D viewport's toolbar.

When you use SkeletonProfileHumanoid, auto-mapping will be performed when the SkeletonProfile is set. If the auto-mapping does not work well, you can map bones manually.

Any missing, duplicate or incorrect parent-child relationship mappings will be indicated by a magenta / red button (depending on the editor setting). It does not block the import process, but it warns that animations may not be shared correctly.

The auto-mapping uses pattern matching for the bone names. So we recommend to use common English names for bones.

After you set up the bone_map, several options are available in the sections below.

If you import resources as an AnimationLibrary that will be shared, we recommend to enable these options. However, if you import resources as scenes, these should be disabled in some cases. For example, if you import a character with animated accessories, these options may cause the accessories to not animate.

Removes any tracks except the bone Transform track from the animations.

Removes Position tracks other than root_bone and scale_base_bone defined in SkeletonProfile from the animations. In SkeletonProfileHumanoid, this means that to remove Position tracks other than "Root" and "Hips". Since Godot 4.0+, animations include Bone Rest in the Transform value. If you disable this option, the animation may change the body shape unpredictably.

Removes unmapped bone Transform tracks from the animations.

Rename the mapped bones.

Makes Skeleton a unique node with the name specified in the skeleton_name. This allows the animation track paths to be unified independent of the scene hierarchy.

Reference poses defined in SkeletonProfileHumanoid have the following rules:

The humanoid is T-pose

The humanoid is facing +Z in the Right-Handed Y-UP Coordinate System

The humanoid should not have a Transform as Node

Directs the +Y axis from the parent joint to the child joint

+X rotation bends the joint like a muscle contracting

These rules are convenient definitions for blend animation and Inverse Kinematics (IK). If your model does not match this definition, you need to fix it with these options.

If the asset is not exported correctly for sharing, the imported Skeleton may have a Transform as a Node. For example, a glTF exported from Blender with no "Apply Transform" executed is one such case. It looks like the model matches the definition, but the internal Transforms are different from the definition. This option fixes such models by applying Transforms on import.

If the imported scene contains objects other than Skeletons, this option may have a negative effect.

Position track is used mostly for model movement, but sharing the moving animation between models with different heights may cause the appearance of slipping due to the difference in stride length. This option normalizes the Position track values based on the scale_base_bone height. The scale_base_bone height is stored in the Skeleton as the motion_scale, and the normalized Position track values is multiplied by that value on playback. If this option is disabled, the Position tracks is not normalized and the Skeleton's motion_scale is always imported as 1.0.

With SkeletonProfileHumanoid, scale_base_bone is "Hips", therefore the Hips' height is used as the motion_scale.

Unifies the models' Bone Rests by overwriting it to match the reference poses defined in the SkeletonProfile.

This is the most important option for sharing animations in Godot 4.0+, but be aware that this option can produce horrible results if the original Bone Rest set externally is important. If you want to share animations with keeping the original Bone Rest, consider to use the Realtime Retarget Module.

Attempts to make the model's silhouette match that of the reference poses defined in the SkeletonProfile, such as T-Pose. This cannot fix silhouettes which are too different, and it may not work for fixing bone roll.

With SkeletonProfileHumanoid, this option does not need to be enabled for T-pose models, but should be enabled for A-pose models. However in that case, the fixed foot results may be bad depending on the heel height of the model, so it may be necessary to add the SkeletonProfile bone names you do not want fixed in the filter array, as in the below example.

Also, for models with bent knees or feet, it may be necessary to adjust the scale_base_bone height. For that, you can use base_height_adjustment option.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Signed distance field global illumination (SDFGI) — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/global_illumination/using_sdfgi.html

**Contents:**
- Signed distance field global illumination (SDFGI)
- Visual comparison
- Setting up SDFGI
- Environment SDFGI properties
- SDFGI interaction with lights and objects
- Adjusting SDFGI performance and quality
- SDFGI caveats
- User-contributed notes

Signed distance field global illumination (SDFGI) is a novel technique available in Godot 4.0. It provides semi-real-time global illumination that scales to any world size and works with procedurally generated levels.

SDFGI supports dynamic lights, but not dynamic occluders or dynamic emissive surfaces. Therefore, SDFGI provides better real-time ability than baked lightmaps, but worse real-time ability than VoxelGI.

From a performance standpoint, SDFGI is one of the most demanding global illumination techniques in Godot. Like with VoxelGI, there are still many settings available to tweak its performance requirements at the cost of quality.

SDFGI is only supported when using the Forward+ renderer, not the Mobile or Compatibility renderers.

Not sure if SDFGI is suited to your needs? See Which global illumination technique should I use? for a comparison of GI techniques available in Godot 4.

In Godot, SDFGI is the global illumination technique with the fewest required steps to enable:

Make sure your MeshInstance nodes have their Global Illumination > Mode property set to Static in the inspector.

For imported 3D scenes, the bake mode can be configured in the Import dock after selecting the 3D scene file in the FileSystem dock.

Add a WorldEnvironment node and create an Environment resource for it.

Edit the Environment resource, scroll down to the SDFGI section and unfold it.

Enable SDFGI > Enabled. SDFGI will automatically follow the camera when it moves, so you do not need to configure extents (unlike VoxelGI).

In the Environment resource, there are several properties available to adjust SDFGI appearance and quality:

Use Occlusion: If enabled, SDFGI will throw additional rays to find and reduce light leaks. This has a performance cost, so only enable this property if you actually need it.

Read Sky Light: If enabled, the environment lighting is represented in the global illumination. This should be enabled in outdoor scenes and disabled in fully indoor scenes.

Bounce Feedback: By default, indirect lighting only bounces once when using SDFGI. Setting this value above 0.0 will cause SDFGI to bounce more than once, which provides more realistic indirect lighting at a small performance cost. Sensible values are usually between 0.3 and 1.0 depending on the scene. Note that in some scenes, values above 0.5 can cause infinite feedback loops to happen, causing the scene to become extremely bright in a few seconds' time. If your indirect lighting looks "splotchy", consider increasing this value above 0.0 to get more uniform-looking lighting. If your lighting ends up looking too bright as a result, decrease Energy to compensate.

Cascades: Higher values result in more detailed GI information (and/or greater maximum distance), but are significantly more expensive on the CPU and GPU. The performance cost of having more cascades especially increases when the camera moves fast, so consider decreasing this to 4 or lower if your camera moves fast.

Min Cell Size: The minimum SDFGI cell size to use for the nearest, most detailed cascade. Lower values result in more accurate indirect lighting and reflection at the cost of lower performance. Adjusting this setting also affects Cascade 0 Distance and Max Distance automatically.

Cascade 0 Distance: The distance at which the nearest, most detailed cascade ends. Greater values make the nearest cascade transition less noticeable, at the cost of reducing the level of detail in the nearest cascade. Adjusting this setting also affects Min Cell Size and Max Distance automatically.

Max Distance: Controls how far away the signed distance field will be computed (for the least detailed cascade). SDFGI will not have any effect past this distance. This value should always be set below the Camera's Far value, as there is no benefit in computing SDFGI past the viewing distance. Adjusting this setting also affects Min Cell Size and Cascade 0 Distance automatically.

Y Scale: Controls how far apart SDFGI probes are spread vertically. By default, vertical spread is the same as horizontal. However, since most game scenes aren't highly vertical, setting the Y Scale to 75% or even 50% can provide better quality and reduce light leaks without impacting performance.

Energy: The brightness multiplier for SDFGI's indirect lighting.

Normal Bias: The normal bias to use for SDFGI's probe ray bounces. Unlike Probe Bias, this only increases the value in relation to the mesh's normals. This makes the bias adjustment more nuanced and avoids increasing the bias too much for no reason. Increase this value if you notice striping artifacts in indirect lighting or reflections.

Probe Bias: The bias to use for SDFGI's probe ray bounces. Increase this value if you notice striping artifacts in indirect lighting or reflections.

The amount of indirect energy emitted by a light is governed by its color, energy and indirect energy properties. To make a specific light emit more or less indirect energy without affecting the amount of direct light emitted by the light, adjust the Indirect Energy property in the Light3D inspector.

To ensure correct visuals when using SDFGI, you must configure your meshes and lights' global illumination properties according to their purpose in the scene (static or dynamic).

There are 3 global illumination modes available for meshes:

Disabled: The mesh won't be taken into account in SDFGI generation. The mesh will receive indirect lighting from the scene, but it will not contribute indirect lighting to the scene.

Static (default): The mesh will be taken into account in SDFGI generation. The mesh will both receive and contribute indirect lighting to the scene. If the mesh is changed in any way after SDFGI is generated, the camera must move away from the object then move back close to it for SDFGI to regenerate. Alternatively, SDFGI can be toggled off and back on. If neither is done, indirect lighting will look incorrect.

Dynamic (not supported with SDFGI): The mesh won't be taken into account in SDFGI generation. The mesh will receive indirect lighting from the scene, but it will not contribute indirect lighting to the scene. This acts identical to the **Disabled* bake mode when using SDFGI.*

Additionally, there are 3 bake modes available for lights (DirectionalLight3D, OmniLight3D and SpotLight3D):

Disabled: The light won't be taken into account for SDFGI baking. The light won't contribute indirect lighting to the scene.

Static: The light will be taken into account for SDFGI baking. The light will contribute indirect lighting to the scene. If the light is changed in any way after baking, indirect lighting will look incorrect until the camera moves away from the light and back (which causes SDFGI to be baked again). will look incorrect. If in doubt, use this mode for level lighting.

Dynamic (default): The light won't be taken into account for SDFGI baking, but it will still contribute indirect lighting to the scene in real-time. This option is slower compared to Static. Only use the Dynamic global illumination mode on lights that will change significantly during gameplay.

The amount of indirect energy emitted by a light depends on its color, energy and indirect energy properties. To make a specific light emit more or less indirect energy without affecting the amount of direct light emitted by the light, adjust the Indirect Energy property in the Light3D inspector.

See Which global illumination mode should I use on meshes and lights? for general usage recommendations.

Since SDFGI is relatively demanding, it will perform best on systems with recent dedicated GPUs. On older dedicated GPUs and integrated graphics, tweaking the settings is necessary to achieve reasonable performance.

In the Project Settings' Rendering > Global Illumination section, SDFGI quality can also be adjusted in several ways:

Sdfgi > Probe Ray Count: Higher values result in better quality, at the cost of higher GPU usage. If this value is set too low, this can cause surfaces to have visible "splotches" of indirect lighting on them due to the number of rays thrown being very low.

Sdfgi > Frames To Converge: Higher values result in better quality, but GI will take more time to fully converge. The effect of this setting is especially noticeable when first loading a scene, or when lights with a bake mode other than Disabled are moving fast. If this value is set too low, this can cause surfaces to have visible "splotches" of indirect lighting on them due to the number of rays thrown being very low. If your scene's lighting doesn't have fast-moving lights that contribute to GI, consider setting this to 30 to improve quality without impacting performance.

Sdfgi > Frames To Update Light: Lower values result in moving lights being reflected faster, at the cost of higher GPU usage. If your scene's lighting doesn't have fast-moving lights that contribute to GI, consider setting this to 16 to improve performance.

Gi > Use Half Resolution: If enabled, both SDFGI and VoxelGI will have their GI buffer rendering at halved resolution. For instance, when rendering in 3840×2160, the GI buffer will be computed at a 1920×1080 resolution. Enabling this option saves a lot of GPU time, but it can introduce visible aliasing around thin details.

SDFGI rendering performance also depends on the number of cascades and the cell size chosen in the Environment resource (see above).

SDFGI has some downsides due to its cascaded nature. When the camera moves, cascade shifts may be visible in indirect lighting. This can be alleviated by adjusting the cascade size, but also by adding fog (which will make distant cascade shifts less noticeable).

Additionally, performance will suffer if the camera moves too fast. This can be fixed in two ways:

Ensuring the camera doesn't move too fast in any given situation.

Temporarily disabling SDFGI in the Environment resource if the camera needs to be moved at a high speed, then enabling SDFGI once the camera speed slows down.

When SDFGI is enabled, it will also take some time for global illumination to be fully converged (25 frames by default). This can create a noticeable transition effect while GI is still converging. To hide this, you can use a ColorRect node that spans the whole viewport and fade it out when switching scenes using an AnimationPlayer node.

The signed distance field is only updated when the camera moves in and out of a cascade. This means that if geometry is modified in the distance, the global illumination appearance will be correct once the camera gets closer. However, if a nearby object with a bake mode set to Static or Dynamic is moved (such as a door), the global illumination will appear incorrect until the camera moves away from the object.

SDFGI's sharp reflections are only visible on opaque materials. Transparent materials will only use rough reflections, even if the material's roughness is lower than 0.2.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Spatial shaders — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/shaders/shader_reference/spatial_shader.html

**Contents:**
- Spatial shaders
- Render modes
- Built-ins
- Global built-ins
- Vertex built-ins
- Fragment built-ins
- Light built-ins
- User-contributed notes

Spatial shaders are used for shading 3D objects. They are the most complex type of shader Godot offers. Spatial shaders are highly configurable with different render modes and different rendering options (e.g. Subsurface Scattering, Transmission, Ambient Occlusion, Rim lighting etc). Users can optionally write vertex, fragment, and light processor functions to affect how objects are drawn.

For visual examples of these render modes, see Standard Material 3D and ORM Material 3D.

Mix blend mode (alpha is transparency), default.

Subtractive blend mode.

Multiplicative blend mode.

Premultiplied alpha blend mode (fully transparent = add, fully opaque = mix).

Only draw depth for opaque geometry (not transparent).

Always draw depth (opaque and transparent).

Do opaque depth pre-pass for transparent geometry.

Disable depth testing.

Subsurface Scattering mode for skin (optimizes visuals for human skin, e.g. boosted red channel).

Cull back-faces (default).

Culling disabled (double sided).

Result is just albedo. No lighting/shading happens in material, making it faster to render.

Geometry draws using lines (useful for troubleshooting).

Directional shadows are drawn using different colors for each split (useful for troubleshooting).

Burley (Disney PBS) for diffuse (default).

Lambert shading for diffuse.

Lambert-wrap shading (roughness-dependent) for diffuse.

Toon shading for diffuse.

Schlick-GGX for direct light specular lobes (default).

Toon for direct light specular lobes.

Disable direct light specular lobes. Doesn't affect reflected light (use SPECULAR = 0.0 instead).

skip_vertex_transform

VERTEX, NORMAL, TANGENT, and BITANGENT need to be transformed manually in the vertex() function.

VERTEX, NORMAL, TANGENT, and BITANGENT are modified in world space instead of model space.

ensure_correct_normals

Use when non-uniform scale is applied to mesh (note: currently unimplemented).

Disable computing shadows in shader. The shader will not receive shadows, but can still cast them.

ambient_light_disabled

Disable contribution from ambient light and radiance map.

Lighting modifies the alpha so shadowed areas are opaque and non-shadowed areas are transparent. Useful for overlaying shadows onto a camera feed in AR.

Use vertex-based lighting instead of per-pixel lighting.

Enables the trails when used on particles geometry.

Alpha antialiasing mode, see here for more.

alpha_to_coverage_and_one

Alpha antialiasing mode, see here for more.

Disable receiving depth-based or volumetric fog. Useful for blend_add materials like particles.

Values marked as in are read-only. Values marked as out can optionally be written to and will not necessarily contain sensible values. Values marked as inout provide a sensible default value, and can optionally be written to. Samplers cannot be written to so they are not marked.

Not all built-ins are available in all processing functions. To access a vertex built-in from the fragment() function, you can use a varying. The same applies for accessing fragment built-ins from the light() function.

Global built-ins are available everywhere, including custom functions.

Global time since the engine has started, in seconds. It repeats after every 3,600 seconds (which can be changed with the rollover setting). It's affected by time_scale but not by pausing. If you need a TIME variable that is not affected by time scale, add your own global shader uniform and update it each frame.

A PI constant (3.141592). A ratio of a circle's circumference to its diameter and amount of radians in half turn.

A TAU constant (6.283185). An equivalent of PI * 2 and amount of radians in full turn.

An E constant (2.718281). Euler's number and a base of the natural logarithm.

in bool OUTPUT_IS_SRGB

true when output is in sRGB color space (this is true in the Compatibility renderer, false in Forward+ and Mobile).

in float CLIP_SPACE_FAR

Clip space far z value. In the Forward+ or Mobile renderers, it's 0.0. In the Compatibility renderer, it's -1.0.

Vertex data (VERTEX, NORMAL, TANGENT, and BITANGENT) are presented in model space (also called local space). If not written to, these values will not be modified and be passed through as they came, then transformed into view space to be used in fragment().

They can optionally be presented in world space by using the world_vertex_coords render mode.

Users can disable the built-in modelview transform (projection will still happen later) and do it manually with the following code:

Other built-ins, such as UV, UV2, and COLOR, are also passed through to the fragment() function if not modified.

Users can override the modelview and projection transforms using the POSITION built-in. If POSITION is written to anywhere in the shader, it will always be used, so the user becomes responsible for ensuring that it always has an acceptable value. When POSITION is used, the value from VERTEX is ignored and projection does not happen. However, the value passed to the fragment shader still comes from VERTEX.

For instancing, the INSTANCE_CUSTOM variable contains the instance custom data. When using particles, this information is usually:

x: Rotation angle in radians.

y: Phase during lifetime (0.0 to 1.0).

This allows you to easily adjust the shader to a particle system using default particles material. When writing a custom particle shader, this value can be used as desired.

in vec2 VIEWPORT_SIZE

Size of viewport (in pixels).

World space to view space transform.

in mat4 INV_VIEW_MATRIX

View space to world space transform.

in mat4 MAIN_CAM_INV_VIEW_MATRIX

View space to world space transform of camera used to draw the current viewport.

in mat4 INV_PROJECTION_MATRIX

Clip space to view space transform.

in vec3 NODE_POSITION_WORLD

Node position, in world space.

in vec3 NODE_POSITION_VIEW

Node position, in view space.

in vec3 CAMERA_POSITION_WORLD

Camera position, in world space. Represents the midpoint of the two eyes when in multiview/stereo rendering.

in vec3 CAMERA_DIRECTION_WORLD

Camera direction, in world space.

in uint CAMERA_VISIBLE_LAYERS

Cull layers of the camera rendering the current pass.

Instance ID for instancing.

in vec4 INSTANCE_CUSTOM

Instance custom data (for particles, mostly).

The view that we are rendering. VIEW_MONO_LEFT (0) for Mono (not multiview) or left eye, VIEW_RIGHT (1) for right eye.

in int VIEW_MONO_LEFT

Constant for Mono or left eye, always 0.

Constant for right eye, always 1.

Position offset for the eye being rendered, in view space. Only applicable for multiview rendering.

Position of the vertex, in model space. In world space if world_vertex_coords is used.

The index of the current vertex in the vertex buffer.

Normal in model space. In world space if world_vertex_coords is used.

Tangent in model space. In world space if world_vertex_coords is used.

Binormal in model space. In world space if world_vertex_coords is used.

If written to, overrides final vertex position in clip space.

UV secondary channel.

Roughness for vertex lighting.

inout float POINT_SIZE

Point size for point rendering.

inout mat4 MODELVIEW_MATRIX

Model/local space to view space transform (use if possible).

inout mat3 MODELVIEW_NORMAL_MATRIX

Model/local space to world space transform.

in mat3 MODEL_NORMAL_MATRIX

inout mat4 PROJECTION_MATRIX

View space to clip space transform.

in uvec4 BONE_INDICES

Custom value from vertex primitive. When using extra UVs, xy is UV3 and zw is UV4.

Custom value from vertex primitive. When using extra UVs, xy is UV5 and zw is UV6.

Custom value from vertex primitive. When using extra UVs, xy is UV7 and zw is UV8.

Custom value from vertex primitive.

MODELVIEW_MATRIX combines both the MODEL_MATRIX and VIEW_MATRIX and is better suited when floating point issues may arise. For example, if the object is very far away from the world origin, you may run into floating point issues when using the separated MODEL_MATRIX and VIEW_MATRIX.

INV_VIEW_MATRIX is the matrix used for rendering the object in that pass, unlike MAIN_CAM_INV_VIEW_MATRIX, which is the matrix of the camera in the scene. In the shadow pass, INV_VIEW_MATRIX's view is based on the camera that is located at the position of the light.

The default use of a Godot fragment processor function is to set up the material properties of your object and to let the built-in renderer handle the final shading. However, you are not required to use all these properties, and if you don't write to them, Godot will optimize away the corresponding functionality.

in vec2 VIEWPORT_SIZE

Size of viewport (in pixels).

Coordinate of pixel center in screen space. xy specifies position in window. Origin is lower left. z specifies fragment depth. It is also used as the output value for the fragment depth unless DEPTH is written to.

true if current face is front facing, false otherwise.

Normalized vector from fragment position to camera (in view space). This is the same for both perspective and orthogonal cameras.

UV that comes from the vertex() function.

UV2 that comes from the vertex() function.

COLOR that comes from the vertex() function.

Point coordinate for drawing points with POINT_SIZE.

Model/local space to world space transform.

in mat3 MODEL_NORMAL_MATRIX

Model/local space to world space transform for normals. This is the same as MODEL_MATRIX by default unless the object is scaled non-uniformly, in which case this is set to transpose(inverse(mat3(MODEL_MATRIX))).

World space to view space transform.

in mat4 INV_VIEW_MATRIX

View space to world space transform.

in mat4 PROJECTION_MATRIX

View space to clip space transform.

in mat4 INV_PROJECTION_MATRIX

Clip space to view space transform.

in vec3 NODE_POSITION_WORLD

Node position, in world space.

in vec3 NODE_POSITION_VIEW

Node position, in view space.

in vec3 CAMERA_POSITION_WORLD

Camera position, in world space. Represents the midpoint of the two eyes when in multiview/stereo rendering.

in vec3 CAMERA_DIRECTION_WORLD

Camera direction, in world space.

in uint CAMERA_VISIBLE_LAYERS

Cull layers of the camera rendering the current pass.

Position of the fragment (pixel), in view space. It is the VERTEX value from vertex() interpolated between the face's vertices and transformed into view space. If skip_vertex_transform is enabled, it may not be in view space.

inout vec3 LIGHT_VERTEX

A writable version of VERTEX that can be used to alter light and shadows. Writing to this will not change the position of the fragment.

The view that we are rendering. Used to distinguish between views in multiview/stereo rendering. VIEW_MONO_LEFT (0) for Mono (not multiview) or left eye, VIEW_RIGHT (1) for right eye.

in int VIEW_MONO_LEFT

Constant for Mono or left eye, always 0.

Constant for right eye, always 1.

Position offset for the eye being rendered, in view space. Only applicable for multiview rendering.

sampler2D SCREEN_TEXTURE

Removed in Godot 4. Use a sampler2D with hint_screen_texture instead.

Screen UV coordinate for current pixel.

sampler2D DEPTH_TEXTURE

Removed in Godot 4. Use a sampler2D with hint_depth_texture instead.

Custom depth value (range of [0.0, 1.0]). If DEPTH is being written to in any shader branch, then you are responsible for setting the DEPTH for all other branches. Otherwise, the graphics API will leave them uninitialized.

Normal that comes from the vertex() function, in view space. If skip_vertex_transform is enabled, it may not be in view space.

Tangent that comes from the vertex() function, in view space. If skip_vertex_transform is enabled, it may not be in view space.

Binormal that comes from the vertex() function, in view space. If skip_vertex_transform is enabled, it may not be in view space.

Set normal here if reading normal from a texture instead of NORMAL.

out float NORMAL_MAP_DEPTH

Depth from NORMAL_MAP. Defaults to 1.0.

Albedo (default white). Base color.

Alpha (range of [0.0, 1.0]). If read from or written to, the material will go to the transparent pipeline.

out float ALPHA_SCISSOR_THRESHOLD

If written to, values below a certain amount of alpha are discarded.

out float ALPHA_HASH_SCALE

Alpha hash scale when using the alpha hash transparency mode. Defaults to 1.0. Higher values result in more visible pixels in the dithering pattern.

out float ALPHA_ANTIALIASING_EDGE

The threshold below which alpha to coverage antialiasing should be used. Defaults to 0.0. Requires the alpha_to_coverage render mode. Should be set to a value lower than ALPHA_SCISSOR_THRESHOLD to be effective.

out vec2 ALPHA_TEXTURE_COORDINATE

The texture coordinate to use for alpha-to-coverge antialiasing. Requires the alpha_to_coverage render mode. Typically set to UV * vec2(albedo_texture_size) where albedo_texture_size is the size of the albedo texture in pixels.

out float PREMUL_ALPHA_FACTOR

Premultiplied alpha factor. Only effective if render_mode blend_premul_alpha; is used. This should be written to when using a shaded material with premultiplied alpha blending for interaction with lighting. This is not required for unshaded materials.

Metallic (range of [0.0, 1.0]).

Specular (not physically accurate to change). Defaults to 0.5. 0.0 disables reflections.

Roughness (range of [0.0, 1.0]).

Rim (range of [0.0, 1.0]). If used, Godot calculates rim lighting. Rim size depends on ROUGHNESS.

Rim Tint, range of 0.0 (white) to 1.0 (albedo). If used, Godot calculates rim lighting.

Small specular blob added on top of the existing one. If used, Godot calculates clearcoat.

out float CLEARCOAT_GLOSS

Gloss of clearcoat. If used, Godot calculates clearcoat.

For distorting the specular blob according to tangent space.

out vec2 ANISOTROPY_FLOW

Distortion direction, use with flowmaps.

out float SSS_STRENGTH

Strength of subsurface scattering. If used, subsurface scattering will be applied to the object.

out vec4 SSS_TRANSMITTANCE_COLOR

Color of subsurface scattering transmittance. If used, subsurface scattering transmittance will be applied to the object.

out float SSS_TRANSMITTANCE_DEPTH

Depth of subsurface scattering transmittance. Higher values allow the effect to reach deeper into the object.

out float SSS_TRANSMITTANCE_BOOST

Boosts the subsurface scattering transmittance if set above 0.0. This makes the effect show up even on directly lit surfaces

Color of backlighting (works like direct light, but it's received even if the normal is slightly facing away from the light). If used, backlighting will be applied to the object. Can be used as a cheaper approximation of subsurface scattering.

Strength of ambient occlusion. For use with pre-baked AO.

out float AO_LIGHT_AFFECT

How much ambient occlusion affects direct light (range of [0.0, 1.0], default 0.0).

Emission color (can go over (1.0, 1.0, 1.0) for HDR).

If written to, blends final pixel color with FOG.rgb based on FOG.a.

If written to, blends environment map radiance with RADIANCE.rgb based on RADIANCE.a.

If written to, blends environment map irradiance with IRRADIANCE.rgb based on IRRADIANCE.a.

Shaders going through the transparent pipeline when ALPHA is written to may exhibit transparency sorting issues. Read the transparency sorting section in the 3D rendering limitations page for more information and ways to avoid issues.

Writing light processor functions is completely optional. You can skip the light() function by using the unshaded render mode. If no light function is written, Godot will use the material properties written to in the fragment() function to calculate the lighting for you (subject to the render mode).

The light() function is called for every light in every pixel. It is called within a loop for each light type.

Below is an example of a custom light() function using a Lambertian lighting model:

If you want the lights to add together, add the light contribution to DIFFUSE_LIGHT using +=, rather than overwriting it.

The light() function won't be run if the vertex_lighting render mode is enabled, or if Rendering > Quality > Shading > Force Vertex Shading is enabled in the Project Settings. (It's enabled by default on mobile platforms.)

in vec2 VIEWPORT_SIZE

Size of viewport (in pixels).

Coordinate of pixel center in screen space. xy specifies position in window, z specifies fragment depth if DEPTH is not used. Origin is lower-left.

Model/local space to world space transform.

in mat4 INV_VIEW_MATRIX

View space to world space transform.

World space to view space transform.

in mat4 PROJECTION_MATRIX

View space to clip space transform.

in mat4 INV_PROJECTION_MATRIX

Clip space to view space transform.

Normal vector, in view space.

Screen UV coordinate for current pixel.

UV that comes from the vertex() function.

UV2 that comes from the vertex() function.

View vector, in view space.

Light vector, in view space.

Light color multiplied by light energy multiplied by PI. The PI multiplication is present because physically-based lighting models include a division by PI.

in float SPECULAR_AMOUNT

For OmniLight3D and SpotLight3D, 2.0 multiplied by light_specular. For DirectionalLight3D, 1.0.

in bool LIGHT_IS_DIRECTIONAL

true if this pass is a DirectionalLight3D.

Attenuation based on distance or shadow.

out vec3 DIFFUSE_LIGHT

Diffuse light result.

out vec3 SPECULAR_LIGHT

Specular light result.

Alpha (range of [0.0, 1.0]). If written to, the material will go to the transparent pipeline.

Shaders going through the transparent pipeline when ALPHA is written to may exhibit transparency sorting issues. Read the transparency sorting section in the 3D rendering limitations page for more information and ways to avoid issues.

Transparent materials also cannot cast shadows or appear in hint_screen_texture and hint_depth_texture uniforms. This in turn prevents those materials from appearing in screen-space reflections or refraction. SDFGI sharp reflections are not visible on transparent materials (only rough reflections are visible on transparent materials).

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
shader_type spatial;
render_mode skip_vertex_transform;

void vertex() {
    VERTEX = (MODELVIEW_MATRIX * vec4(VERTEX, 1.0)).xyz;
    NORMAL = normalize((MODELVIEW_MATRIX * vec4(NORMAL, 0.0)).xyz);
    BINORMAL = normalize((MODELVIEW_MATRIX * vec4(BINORMAL, 0.0)).xyz);
    TANGENT = normalize((MODELVIEW_MATRIX * vec4(TANGENT, 0.0)).xyz);
}
```

Example 2 (unknown):
```unknown
void light() {
    DIFFUSE_LIGHT += clamp(dot(NORMAL, LIGHT), 0.0, 1.0) * ATTENUATION * LIGHT_COLOR / PI;
}
```

---

## Standard Material 3D and ORM Material 3D — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/standard_material_3d.html

**Contents:**
- Standard Material 3D and ORM Material 3D
- Introduction
- BaseMaterial 3D settings
- Transparency
  - Alpha Antialiasing
  - Blend Mode
  - Cull Mode
  - Depth Draw Mode
  - No Depth Test
- Shading

StandardMaterial3D and ORMMaterial3D (Occlusion, Roughness, Metallic) are default 3D materials that aim to provide most of the features artists look for in a material, without the need for writing shader code. However, they can be converted to shader code if additional functionality is needed.

This tutorial explains the parameters present in both materials.

There are 4 ways to add these materials to an object. A material can be added in the Material property of the mesh. It can be added in the Material property of the node using the mesh (such as a MeshInstance3D node), the Material Override property of the node using the mesh, and the Material Overlay.

If you add a material to the mesh itself, every time that mesh is used it will have that material. If you add a material to the node using the mesh, the material will only be used by that node, it will also override the material property of the mesh. If a material is added in the Material Override property of the node, it will only be used by that node. It will also override the regular material property of the node and the material property of the mesh.

The Material Overlay property will render a material over the current one being used by the mesh. As an example, this can be used to put a transparent shield effect on a mesh.

StandardMaterial3D has many settings that determine the look of a material. All of these are under the BaseMaterial3D category

ORM materials are almost exactly the same with one difference. Instead of separate settings and textures for occlusion, roughness, and metallic, there is a single ORM texture. The different color channels of that texture are used for each parameter. Programs such as Substance Painter and Armor Paint will give you the option to export in this format, for these two programs it's with the export preset for unreal engine, which also uses ORM textures.

By default, materials in Godot are opaque. This is fast to render, but it means the material can't be seen through even if you use a transparent texture in the Albedo > Texture property (or set Albedo > Color to a transparent color).

To be able to see through a material, the material needs to be made transparent. Godot offers several transparency modes:

Disabled: Material is opaque. This is the fastest to render, with all rendering features supported.

Alpha: Material is transparent. Semi-transparent areas are drawn with blending. This is slow to render, but it allows for partial transparency (also known as translucency). Materials using alpha blending also can't cast shadows, and are not visible in screen-space reflections.

Alpha is a good fit for particle effects and VFX.

Alpha Scissor: Material is transparent. Semi-transparent areas whose opacity is below Alpha Scissor Threshold are not drawn (above this opacity, these are drawn as opaque). This is faster to render than Alpha and doesn't exhibit transparency sorting issues. The downside is that this results in "all or nothing" transparency, with no intermediate values possible. Materials using alpha scissor can cast shadows.

Alpha Scissor is ideal for foliage and fences, since these have hard edges and require correct sorting to look good.

Alpha Hash: Material is transparent. Semi-transparent areas are drawn using dithering. This is also "all or nothing" transparency, but dithering helps represent partially opaque areas with limited precision depending on viewport resolution. Materials using alpha hash can cast shadows.

Alpha Hash is suited for realistic-looking hair, although stylized hair may work better with alpha scissor.

Depth Pre-Pass: This renders the object's fully opaque pixels via the opaque pipeline first, then renders the rest with alpha blending. This allows transparency sorting to be mostly correct (albeit not fully so, as partially transparent regions may still exhibit incorrect sorting). Materials using depth prepass can cast shadows.

Godot will automatically force the material to be transparent with alpha blending if any of these conditions is met:

Setting the transparency mode to Alpha (as described here).

Setting a blend mode other than the default Mix

Enabling Refraction, Proximity Fade, or Distance Fade.

Comparison between alpha blending (left) and alpha scissor (right) transparency:

Alpha-blended transparency has several limitations:

Alpha-blended materials are significantly slower to render, especially if they overlap.

Alpha-blended materials may exhibit sorting issues when transparent surfaces overlap each other. This means that surfaces may render in the incorrect order, with surfaces in the back appearing to be in front of those which are actually closer to the camera.

Alpha-blended materials don't cast shadows, although they can receive shadows.

Alpha-blended materials don't appear in any reflections (other than reflection probes).

Screen-space reflections and sharp SDFGI reflections don't appear on alpha-blended materials. When SDFGI is enabled, rough reflections are used as a fallback regardless of material roughness.

Before using the Alpha transparency mode, always consider whether another transparency mode is more suited for your needs.

This property is only visible when the transparency mode is Alpha Scissor or Alpha Hash.

While alpha scissor and alpha hash materials are faster to render than alpha-blended materials, they exhibit hard edges between opaque and transparent regions. While it's possible to use post-processing-based antialiasing techniques such as FXAA and TAA, this is not always desired as these techniques tend to make the final result look blurrier or exhibit ghosting artifacts.

There are 3 alpha antialiasing modes available:

Disabled: No alpha antialiasing. Edges of transparent materials will appear aliased unless a post-processing-based antialiasing solution is used.

Alpha Edge Blend: Results in a smooth transition between opaque and transparent areas. Also known as "alpha to coverage".

Alpha Edge Clip: Results in a sharp, but still antialiased transition between opaque and transparent areas. Also known as "alpha to coverage + alpha to one".

When the alpha antialiasing mode is set to Alpha Edge Blend or Alpha Edge Clip, a new Alpha Antialiasing Edge property becomes visible below in the inspector. This property controls the threshold below which pixels should be made transparent. While you've already defined an alpha scissor threshold (when using Alpha Scissor only), this additional threshold is used to smoothly transition between opaque and transparent pixels. Alpha Antialiasing Edge must always be set to a value that is strictly below the alpha scissor threshold. The default of 0.3 is a sensible value with an alpha scissor of threshold of 0.5, but remember to adjust this alpha antialiasing edge when modifying the alpha scissor threshold.

If you find the antialiasing effect not effective enough, try increasing Alpha Antialiasing Edge while making sure it's below Alpha Scissor Threshold (if the material uses alpha scissor). On the other hand, if you notice the texture's appearance visibly changing as the camera moves closer to the material, try decreasing Alpha Antialiasing Edge.

For best results, MSAA 3D should be set to at least 2× in the Project Settings when using alpha antialiasing. This is because this feature relies on alpha to coverage, which is a feature provided by MSAA.

Without MSAA, a fixed dithering pattern is applied on the material's edges, which isn't very effective at smoothing out edges (although it can still help a little).

Controls the blend mode for the material. Keep in mind that any mode other than Mix forces the object to go through the transparent pipeline.

Mix: Default blend mode, alpha controls how much the object is visible.

Add: The final color of the object is added to the color of the screen, nice for flares or some fire-like effects.

Sub: The final color of the object is subtracted from the color of the screen.

Mul: The final color of the object is multiplied with the color of the screen.

Premultiplied Alpha: The color of the object is expected to have already been multiplied by the alpha. This behaves like Add when the alpha is 0.0 (fully transparent) and like Mix when the alpha is 1.0 (opaque).

Determines which side of the object is not drawn when backfaces are rendered:

Back: The back of the object is culled when not visible (default).

Front: The front of the object is culled when not visible.

Disabled: Used for objects that are double-sided (no culling is performed).

By default, Blender has backface culling disabled on materials and will export materials to match how they render in Blender. This means that materials in Godot will have their cull mode set to Disabled. This can decrease performance since backfaces will be rendered, even when they are being culled by other faces. To resolve this, enable Backface Culling in Blender's Materials tab, then export the scene to glTF again.

Specifies when depth rendering must take place.

Opaque Only (default): Depth is only drawn for opaque objects.

Always: Depth draw is drawn for both opaque and transparent objects.

Never: No depth draw takes place (do not confuse this with the No Depth Test option below).

Depth Pre-Pass: For transparent objects, an opaque pass is made first with the opaque parts, then transparency is drawn above. Use this option with transparent grass or tree foliage.

In order for close objects to appear over far away objects, depth testing is performed. Disabling it has the result of objects appearing over (or under) everything else.

Disabling this makes the most sense for drawing indicators in world space, and works very well with the Render Priority property of Material (see the bottom of this page).

Materials support three shading modes: Per-Pixel, Per-Vertex, and Unshaded.

The Per-Pixel shading mode calculates lighting for each pixel, and is a good fit for most use cases. However, in some cases you may want to increase performance by using another shading mode.

The Per-Vertex shading mode, often called "vertex shading" or "vertex lighting", instead calculates lighting once for each vertex, and interpolates the result between each pixel.

On low-end or mobile devices, using per-vertex lighting can considerably increase rendering performance. When rendering several layers of transparency, such as when using particle systems, using per-vertex shading can improve performance, especially when the camera is close to particles.

You can also use per-vertex lighting to achieve a retro look.

Texture from AmbientCG

The Unshaded shading mode does not calculate lighting at all. Instead, the Albedo color is output directly. Lights will not affect the material at all, and unshaded materials will tend to appear considerably brighter than shaded materials.

Rendering unshaded is useful for some specific visual effects. If maximum performance is needed, it can also be used for particles, or low-end or mobile devices.

Specifies the algorithm used by diffuse scattering of light when hitting the object. The default is Burley. Other modes are also available:

Burley: Default mode, the original Disney Principled PBS diffuse algorithm.

Lambert: Is not affected by roughness.

Lambert Wrap: Extends Lambert to cover more than 90 degrees when roughness increases. Works great for hair and simulating cheap subsurface scattering. This implementation is energy conserving.

Toon: Provides a hard cut for lighting, with smoothing affected by roughness. It is recommended you disable sky contribution from your environment's ambient light settings or disable ambient light in the StandardMaterial3D to achieve a better effect.

Specifies how the specular blob will be rendered. The specular blob represents the shape of a light source reflected in the object.

SchlickGGX: The most common blob used by PBR 3D engines nowadays.

Toon: Creates a toon blob, which changes size depending on roughness.

Disabled: Sometimes the blob gets in the way. Begone!

Makes the object not receive any kind of ambient lighting that would otherwise light it.

Makes the object unaffected by depth-based or volumetric fog. This is useful for particles or other additively blended materials that would otherwise show the shape of the mesh (even in places where it would be invisible without the fog).

Makes the object not have its reflections reduced where they would usually be occluded.

This setting allows choosing what is done by default to vertex colors that come from your 3D modeling application. By default, they are ignored.

Choosing this option means vertex color is used as albedo color.

Most 3D modeling software will likely export vertex colors as sRGB, so toggling this option on will help them look correct.

Albedo is the base color for the material, on which all the other settings operate. When set to Unshaded, this is the only color that is visible. In previous versions of Godot, this channel was named Diffuse. The change of name mainly happened because, in PBR (Physically Based Rendering), this color affects many more calculations than just the diffuse lighting path.

Albedo color and texture can be used together as they are multiplied.

Alpha channel in albedo color and texture is also used for the object transparency. If you use a color or texture with alpha channel, make sure to either enable transparency or alpha scissoring for it to work.

Godot uses a metallic model over competing models due to its simplicity. This parameter defines how reflective the material is. The more reflective, the less diffuse/ambient light affects the material and the more light is reflected. This model is called "energy-conserving".

The Specular parameter is a general amount for the reflectivity (unlike Metallic, this is not energy-conserving, so leave it at 0.5 and don't touch it unless you need to).

The minimum internal reflectivity is 0.04, so it's impossible to make a material completely unreflective, just like in real life.

Roughness affects the way reflection happens. A value of 0 makes it a perfect mirror while a value of 1 completely blurs the reflection (simulating natural microsurfacing). Most common types of materials can be achieved with the right combination of Metallic and Roughness.

Emission specifies how much light is emitted by the material (keep in mind this does not include light surrounding geometry unless VoxelGI or SDFGI are used). This value is added to the resulting final image and is not affected by other lighting in the scene.

Normal mapping allows you to set a texture that represents finer shape detail. This does not modify geometry, only the incident angle for light. In Godot, only the red and green channels of normal maps are used for better compression and wider compatibility.

Godot requires the normal map to use the X+, Y+ and Z+ coordinates, this is known as OpenGL style. If you've imported a material made to be used with another engine it may be DirectX style, in which case the normal map needs to be converted so its Y axis is flipped.

More information about normal maps (including a coordinate order table for popular engines) can be found here.

A bent normal map describes the average direction of ambient lighting. Unlike a regular normal map, this is used to improve how a material reacts to lighting rather than add surface detail.

This is achieved in two ways:

Indirect diffuse lighting is made to match global illumination more closely.

If specular occlusion is enabled, it is calculated using the bent normals and ambient occlusion instead of just from ambient light. This includes screen-space ambient occlusion (SSAO) and other sources of ambient occlusion.

Godot only uses the red and green channels of a bent normal map for better compression and wider compatibility.

When creating a bent normal map, there are three things required for it to work correctly in Godot:

A cosine distribution of rays has to be used when baking.

The texture must be created in tangent space.

The bent normal map needs to use the X+, Y+, and Z+ coordinates, this is known as OpenGL style. If you've imported a material made to be used with another engine it may be DirectX style, in which case the bent normal map needs to be converted so its Y axis is flipped. This can be achieved by setting the green channel under the Channel Remap section to Inverted Green in the import dock.

A bent normal map is different from a regular normal map. The two are not interchangeable.

Some fabrics have small micro-fur that causes light to scatter around it. Godot emulates this with the Rim parameter. Unlike other rim lighting implementations, which just use the emission channel, this one actually takes light into account (no light means no rim). This makes the effect considerably more believable.

Rim size depends on roughness, and there is a special parameter to specify how it must be colored. If Tint is 0, the color of the light is used for the rim. If Tint is 1, then the albedo of the material is used. Using intermediate values generally works best.

The Clearcoat parameter is used to add a secondary pass of transparent coat to the material. This is common in car paint and toys. In practice, it's a smaller specular blob added on top of the existing material.

This changes the shape of the specular blob and aligns it to tangent space. Anisotropy is commonly used with hair, or to make materials such as brushed aluminum more realistic. It works especially well when combined with flowmaps.

It is possible to specify a baked ambient occlusion map. This map affects how much ambient light reaches each surface of the object (it does not affect direct light by default). While it is possible to use Screen-Space Ambient Occlusion (SSAO) to generate ambient occlusion, nothing beats the quality of a well-baked AO map. It is recommended to bake ambient occlusion whenever possible.

Setting a height map on a material produces a ray-marched search to emulate the proper displacement of cavities along the view direction. This only creates an illusion of depth, and does not add real geometry — for a height map shape used for physics collision (such as terrain), see HeightMapShape3D. It may not work for complex objects, but it produces a realistic depth effect for textures. For best results, Height should be used together with normal mapping.

This is only available in the Forward+ renderer, not the Mobile or Compatibility renderers.

This effect emulates light that penetrates an object's surface, is scattered, and then comes out. It is useful to create realistic skin, marble, colored liquids, etc.

This controls how much light from the lit side (visible to light) is transferred to the dark side (opposite from the light). This works well for thin objects such as plant leaves, grass, human ears, etc.

When refraction is enabled, Godot attempts to fetch information from behind the object being rendered. This allows distorting the transparency in a way similar to refraction in real life.

Remember to use a transparent albedo texture (or reduce the albedo color's alpha channel) to make refraction visible, as refraction relies on transparency to have a visible effect.

Refraction also takes the material roughness into account. Higher roughness values will make the objects behind the refraction look blurrier, which simulates real life behavior. If you can't see behind the object when refraction is enabled and albedo transparency is reduced, decrease the material's Roughness value.

A normal map can optionally be specified in the Refraction Texture property to allow distorting the refraction's direction on a per-pixel basis.

Refraction is implemented as a screen-space effect and forces the material to be transparent. This makes the effect relatively fast, but this results in some limitations:

Transparency sorting issues may occur.

The refractive material cannot refract onto itself, or onto other transparent materials. A refractive material behind another transparent material will be invisible.

Off-screen objects cannot appear in the refraction. This is most noticeable with high refraction strength values.

Opaque materials in front of the refractive material will appear to have "refracted" edges, even though they shouldn't.

Godot allows using secondary albedo and normal maps to generate a detail texture, which can be blended in many ways. By combining this with secondary UV or triplanar modes, many interesting textures can be achieved.

There are several settings that control how detail is used.

Mask: The detail mask is a black and white image used to control where the blending takes place on a texture. White is for the detail textures, Black is for the regular material textures, different shades of gray are for partial blending of the material textures and detail textures.

Blend Mode: These four modes control how the textures are blended together.

Mix: Combines pixel values of both textures. At black, only show the material texture, at white, only show the detail texture. Values of gray create a smooth blend between the two.

Add: Adds pixel values of one Texture with the other. Unlike mix mode both textures are completely mixed at white parts of a mask and not at gray parts. The original texture is mostly unchanged at black

Sub: Subtracts pixel values of one texture with the other. The second texture is completely subtracted at white parts of a mask with only a little subtraction in black parts, gray parts being different levels of subtraction based on the exact texture.

Mul: Multiplies the RGB channel numbers for each pixel from the top texture with the values for the corresponding pixel from the bottom texture.

Albedo: This is where you put an albedo texture you want to blend. If nothing is in this slot it will be interpreted as white by default.

Normal: This is where you put a normal texture you want to blend. If nothing is in this slot it will be interpreted as a flat normal map. This can still be used even if the material does not have normal map enabled.

Godot supports two UV channels per material. Secondary UV is often useful for ambient occlusion or emission (baked light). UVs can be scaled and offset, which is useful when using repeating textures.

Triplanar mapping is supported for both UV1 and UV2. This is an alternative way to obtain texture coordinates, sometimes called "Autotexture". Textures are sampled in X, Y and Z and blended by the normal. Triplanar mapping can be performed in either world space or object space.

In the image below, you can see how all primitives share the same material with world triplanar, so the brick texture continues smoothly between them.

When using triplanar mapping, it is computed in object local space. This option makes it use world space instead.

The filtering method for the textures used by the material. See this page for a full list of options and their description.

if the textures used by the material repeat, and how they repeat. See this page for a full list of options and their description.

Makes the object not receive any kind of shadow that would otherwise be cast onto it.

Lighting modifies the alpha so shadowed areas are opaque and non-shadowed areas are transparent. Useful for overlaying shadows onto a camera feed in AR.

Enables billboard mode for drawing materials. This controls how the object faces the camera:

Disabled: Billboard mode is disabled.

Enabled: Billboard mode is enabled. The object's -Z axis will always face the camera's viewing plane.

Y-Billboard: The object's X axis will always be aligned with the camera's viewing plane.

Particle Billboard: Most suited for particle systems, because it allows specifying flipbook animation.

The Particles Anim section is only visible when the billboard mode is Particle Billboard.

Enables scaling a mesh in billboard mode.

Grows the object vertices in the direction pointed by their normals:

This is commonly used to create cheap outlines. Add a second material pass, make it black and unshaded, reverse culling (Cull Front), and add some grow:

For Grow to work as expected, the mesh must have connected faces with shared vertices, or "smooth shading". If the mesh has disconnected faces with unique vertices, or "flat shading", the mesh will appear to have gaps when using Grow.

This causes the object to be rendered at the same size no matter the distance. This is useful mostly for indicators (no depth test and high render priority) and some types of billboards.

This option is only effective when the geometry rendered is made of points (generally it's made of triangles when imported from 3D modeling software). If so, then those points can be resized (see below).

When drawing points, specify the point size in pixels.

If true, enables parts of the shader required for GPUParticles3D trails to function. This also requires using a mesh with appropriate skinning, such as RibbonTrailMesh or TubeTrailMesh. Enabling this feature outside of materials used in GPUParticles3D meshes will break material rendering.

Scales the object being rendered towards the camera to avoid clipping into things like walls. This is intended to be used for objects that are fixed with respect to the camera like player arms, tools, etc. Lighting and shadows will continue to work correctly when this setting is adjusted, but screen-space effects like SSAO and SSR may break with lower scales. Therefore, try to keep this setting as close to 1.0 as possible.

Overrides the Camera3D's field of view angle (in degrees).

This behaves as if the field of view is set on a Camera3D with Camera3D.keep_aspect set to Camera3D.KEEP_HEIGHT. Additionally, it may not look correct on a non-perspective camera where the field of view setting is ignored.

Godot allows materials to fade by proximity to each other as well as depending on the distance from the viewer. Proximity fade is useful for effects such as soft particles or a mass of water with a smooth blending to the shores.

Distance fade is useful for light shafts or indicators that are only present after a given distance.

Keep in mind enabling proximity fade or distance fade with Pixel Alpha mode enables alpha blending. Alpha blending is more GPU-intensive and can cause transparency sorting issues. Alpha blending also disables many material features such as the ability to cast shadows.

To hide a character when they get too close to the camera, consider using Pixel Dither or better, Object Dither (which is even faster than Pixel Dither).

Pixel Alpha mode: The actual transparency of a pixel of the object changes with distance to the camera. This is the most effect, but forces the material into the transparency pipeline (which leads, for example, to no shadows).

Pixel Dither mode: What this does is sort of approximate the transparency by only having a fraction of the pixels rendered.

Object Dither mode: Like the previous mode, but the calculated transparency is the same across the entire object's surface.

The rendering order of objects can be changed, although this is mostly useful for transparent objects (or opaque objects that perform depth draw but no color draw, such as cracks on the floor).

Objects are sorted by an opaque/transparent queue, then render_priority, with higher priority being drawn later. Transparent objects are also sorted by depth.

Depth testing overrules priority. Priority alone cannot force opaque objects to be drawn over each other.

Setting next_pass on a material will cause an object to be rendered again with that next material.

Materials are sorted by an opaque/transparent queue, then render_priority, with higher priority being drawn later.

Depth will test equal between both materials unless the grow setting or other vertex transformations are used. Multiple transparent passes should use render_priority to ensure correct ordering.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Third-person camera with spring arm — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/spring_arm.html

**Contents:**
- Third-person camera with spring arm
- Introduction
- What is a spring arm?
- Spring arm with a camera
- Setting up the spring arm and camera
- User-contributed notes

3D games will often have a third-person camera that follows and rotates around something such as a player character or a vehicle.

In Godot, this can be done by setting a Camera3D as a child of a node. However, if you try this without any extra steps, you'll notice that the camera clips through geometry and hides the scene.

This is where the SpringArm3D node comes in.

A spring arm has two main components that affect its behavior.

The "length" of the spring arm is how far from its global position to check for collisions:

The "shape" of the spring arm is what it uses to check for collisions. The spring arm will "sweep" this shape from its origin out towards its length.

The spring arm tries to keep all of its children at the end of its length. When the shape collides with something, the children are instead placed at or near that collision point:

When a camera is placed as a child of a spring arm, a pyramid representing the camera will be used as the shape.

This pyramid represents the near plane of the camera:

If the spring arm is given a specific shape, then that shape will always be used.

The camera's shape is only used if the camera is a direct child of the spring arm.

If no shape is provided and the camera is not a direct child, the spring arm will fall back to using a ray cast which is inaccurate for camera collisions and not recommended.

Every physics process frame, the spring arm will perform a motion cast to check if anything is collided with:

When the shape hits something, the camera will be placed at or near the collision point:

Let's add a spring arm camera setup to the platformer demo.

You can download the Platformer 3D demo on GitHub or using the Asset Library.

In general, for a third-person camera setup, you will have three nodes as children of the node that you're following:

Node3D (the "pivot point" for the camera)

Open the player/player.tscn scene. Set these up as children of our player and give them unique names so we can find them in our script. Make sure to delete the existing camera node!

Let's move the pivot point up by 2 on the Y-axis so that it's not on the ground:

Give the spring arm a length of 3 so that it is placed behind the character:

Leave the Shape of the spring arm as <empty>. This way, it will use the camera's pyramid shape.

If you want, you can also try other shapes - a sphere is a common choice since it slides smoothly along edges.

Update the top of player/player.gd to grab the camera and the pivot points by their unique names:

Add an _unhandled_input function to check for camera movement and then rotate the pivot point accordingly:

By rotating the pivot point, the spring arm will also be rotated and it will change where the camera is positioned. Run the game and notice that mouse movement now rotates the camera around the character. If the camera moves into a wall, it collides with it.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
# Comment out this existing camera line.
# @onready var _camera := $Target/Camera3D as Camera3D

@onready var _camera := %Camera3D as Camera3D
@onready var _camera_pivot := %CameraPivot as Node3D
```

Example 2 (gdscript):
```gdscript
@export_range(0.0, 1.0) var mouse_sensitivity = 0.01
@export var tilt_limit = deg_to_rad(75)


func _unhandled_input(event: InputEvent) -> void:
    if event is InputEventMouseMotion:
        _camera_pivot.rotation.x -= event.relative.y * mouse_sensitivity
        # Prevent the camera from rotating too far up or down.
        _camera_pivot.rotation.x = clampf(_camera_pivot.rotation.x, -tilt_limit, tilt_limit)
        _camera_pivot.rotation.y += -event.relative.x * mouse_sensitivity
```

---

## Using 3D transforms — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/using_transforms.html

**Contents:**
- Using 3D transforms
- Introduction
- Problems of Euler angles
  - Axis order
  - Interpolation
  - Say no to Euler angles
- Introducing transforms
  - Manipulating transforms
  - Precision errors
  - Obtaining information

If you have never made 3D games before, working with rotations in three dimensions can be confusing at first. Coming from 2D, the natural way of thinking is along the lines of "Oh, it's just like rotating in 2D, except now rotations happen in X, Y and Z".

At first, this seems easy. For simple games, this way of thinking may even be enough. Unfortunately, it's often incorrect.

Angles in three dimensions are most commonly referred to as "Euler Angles".

Euler angles were introduced by mathematician Leonhard Euler in the early 1700s.

This way of representing 3D rotations was groundbreaking at the time, but it has several shortcomings when used in game development (which is to be expected from a guy with a funny hat). The idea of this document is to explain why, as well as outlining best practices for dealing with transforms when programming 3D games.

While it may seem intuitive that each axis has a rotation, the truth is that it's just not practical.

The main reason for this is that there isn't a unique way to construct an orientation from the angles. There isn't a standard mathematical function that takes all the angles together and produces an actual 3D rotation. The only way an orientation can be produced from angles is to rotate the object angle by angle, in an arbitrary order.

This could be done by first rotating in X, then Y and then in Z. Alternatively, you could first rotate in Y, then in Z and finally in X. Anything works, but depending on the order, the final orientation of the object will not necessarily be the same. Indeed, this means that there are several ways to construct an orientation from 3 different angles, depending on the order of the rotations.

Following is a visualization of rotation axes (in X, Y, Z order) in a gimbal (from Wikipedia). As you can see, the orientation of each axis depends on the rotation of the previous one:

You may be wondering how this affects you. Let's look at a practical example:

Imagine you are working on a first-person controller (e.g. an FPS game). Moving the mouse left and right controls your view angle parallel to the ground, while moving it up and down moves the player's view up and down.

In this case to achieve the desired effect, rotation must be applied first in the Y axis ("up" in this case, since Godot uses a "Y-Up" orientation), followed by rotation in the X axis.

If we were to apply rotation in the X axis first, and then in Y, the effect would be undesired:

Depending on the type of game or effect desired, the order in which you want axis rotations to be applied may differ. Therefore, applying rotations in X, Y, and Z is not enough: you also need a rotation order.

Another problem with using Euler angles is interpolation. Imagine you want to transition between two different camera or enemy positions (including rotations). One logical way to approach this is to interpolate the angles from one position to the next. One would expect it to look like this:

But this does not always have the expected effect when using angles:

The camera actually rotated the opposite direction!

There are a few reasons this may happen:

Rotations don't map linearly to orientation, so interpolating them does not always result in the shortest path (i.e., to go from 270 to 0 degrees is not the same as going from 270 to 360, even though the angles are equivalent).

Gimbal lock is at play (first and last rotated axis align, so a degree of freedom is lost). See Wikipedia's page on Gimbal Lock for a detailed explanation of this problem.

The result of all this is that you should not use the rotation property of Node3D nodes in Godot for games. It's there to be used mainly in the editor, for coherence with the 2D engine, and for simple rotations (generally just one axis, or even two in limited cases). As much as you may be tempted, don't use it.

Instead, there is a better way to solve your rotation problems.

Godot uses the Transform3D datatype for orientations. Each Node3D node contains a transform property which is relative to the parent's transform, if the parent is a Node3D-derived type.

It is also possible to access the world coordinate transform via the global_transform property.

A transform has a Basis (transform.basis sub-property), which consists of three Vector3 vectors. These are accessed via the transform.basis property and can be accessed directly by transform.basis.x, transform.basis.y, and transform.basis.z. Each vector points in the direction its axis has been rotated, so they effectively describe the node's total rotation. The scale (as long as it's uniform) can also be inferred from the length of the axes. A basis can also be interpreted as a 3x3 matrix and used as transform.basis[x][y].

A default basis (unmodified) is akin to:

This is also an analog of a 3x3 identity matrix.

Following the OpenGL convention, X is the Right axis, Y is the Up axis and Z is the Forward axis.

Together with the basis, a transform also has an origin. This is a Vector3 specifying how far away from the actual origin (0, 0, 0) this transform is. Combining the basis with the origin, a transform efficiently represents a unique translation, rotation, and scale in space.

One way to visualize a transform is to look at an object's 3D gizmo while in "local space" mode.

The gizmo's arrows show the X, Y, and Z axes (in red, green, and blue respectively) of the basis, while the gizmo's center is at the object's origin.

For more information on the mathematics of vectors and transforms, please read the Vector math tutorials.

Of course, transforms are not as straightforward to manipulate as angles and have problems of their own.

It is possible to rotate a transform, either by multiplying its basis by another (this is called accumulation), or by using the rotation methods.

A method in Node3D simplifies this:

This rotates the node relative to the parent node.

To rotate relative to object space (the node's own transform), use the following:

The axis should be defined in the local coordinate system of the object. For example, to rotate around the object's local X, Y, or Z axes, use Vector3.RIGHT for the X-axis, Vector3.UP for the Y-axis, and Vector3.FORWARD for the Z-axis.

Doing successive operations on transforms will result in a loss of precision due to floating-point error. This means the scale of each axis may no longer be exactly 1.0, and they may not be exactly 90 degrees from each other.

If a transform is rotated every frame, it will eventually start deforming over time. This is unavoidable.

There are two different ways to handle this. The first is to orthonormalize the transform after some time (maybe once per frame if you modify it every frame):

This will make all axes have 1.0 length again and be 90 degrees from each other. However, any scale applied to the transform will be lost.

It is recommended you not scale nodes that are going to be manipulated; scale their children nodes instead (such as MeshInstance3D). If you absolutely must scale the node, then re-apply it at the end:

You might be thinking at this point: "Ok, but how do I get angles from a transform?". The answer again is: you don't. You must do your best to stop thinking in angles.

Imagine you need to shoot a bullet in the direction your player is facing. Just use the forward axis (commonly Z or -Z).

Is the enemy looking at the player? Use the dot product for this (see the Vector math tutorial for an explanation of the dot product):

All common behaviors and logic can be done with just vectors.

There are, of course, cases where you want to set information to a transform. Imagine a first person controller or orbiting camera. Those are definitely done using angles, because you do want the transforms to happen in a specific order.

For such cases, keep the angles and rotations outside the transform and set them every frame. Don't try to retrieve and reuse them because the transform is not meant to be used this way.

Example of looking around, FPS style:

As you can see, in such cases it's even simpler to keep the rotation outside, then use the transform as the final orientation.

Interpolating between two transforms can efficiently be done with quaternions. More information about how quaternions work can be found in other places around the Internet. For practical use, it's enough to understand that pretty much their main use is doing a closest path interpolation. As in, if you have two rotations, a quaternion will smoothly allow interpolation between them using the closest axis.

Converting a rotation to quaternion is straightforward.

The Quaternion type reference has more information on the datatype (it can also do transform accumulation, transform points, etc., though this is used less often). If you interpolate or apply operations to quaternions many times, keep in mind they need to be eventually normalized. Otherwise, they will also suffer from numerical precision errors.

Quaternions are useful when doing camera/path/etc. interpolations, as the result will always be correct and smooth.

For most beginners, getting used to working with transforms can take some time. However, once you get used to them, you will appreciate their simplicity and power.

Don't hesitate to ask for help on this topic in any of Godot's online communities and, once you become confident enough, please help others!

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
var basis = Basis()
# Contains the following default values:
basis.x = Vector3(1, 0, 0) # Vector pointing along the X axis
basis.y = Vector3(0, 1, 0) # Vector pointing along the Y axis
basis.z = Vector3(0, 0, 1) # Vector pointing along the Z axis
```

Example 2 (unknown):
```unknown
// Due to technical limitations on structs in C# the default
// constructor will contain zero values for all fields.
var defaultBasis = new Basis();
GD.Print(defaultBasis); // prints: ((0, 0, 0), (0, 0, 0), (0, 0, 0))

// Instead we can use the Identity property.
var identityBasis = Basis.Identity;
GD.Print(identityBasis.X); // prints: (1, 0, 0)
GD.Print(identityBasis.Y); // prints: (0, 1, 0)
GD.Print(identityBasis.Z); // prints: (0, 0, 1)

// The Identity basis is equivalent to:
var basis = new Basis(Vector3.Right, Vector3.Up, Vector3.Back);
GD.Print(basis); // prints: ((1, 0, 0), (0, 1, 0), (0, 0, 1))
```

Example 3 (unknown):
```unknown
var axis = Vector3(1, 0, 0) # Or Vector3.RIGHT
var rotation_amount = 0.1
# Rotate the transform around the X axis by 0.1 radians.
transform.basis = Basis(axis, rotation_amount) * transform.basis
# shortened
transform.basis = transform.basis.rotated(axis, rotation_amount)
```

Example 4 (unknown):
```unknown
Transform3D transform = Transform;
Vector3 axis = new Vector3(1, 0, 0); // Or Vector3.Right
float rotationAmount = 0.1f;

// Rotate the transform around the X axis by 0.1 radians.
transform.Basis = new Basis(axis, rotationAmount) * transform.Basis;
// shortened
transform.Basis = transform.Basis.Rotated(axis, rotationAmount);

Transform = transform;
```

---

## Using decals — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/using_decals.html

**Contents:**
- Using decals
- Use cases
  - Static decoration
  - Dynamic gameplay elements
  - Blob shadows
- Quick start guide
  - Creating decals in the editor
- Decal node properties
  - Textures
  - Parameters

Decals are only supported in the Forward+ and Mobile renderers, not the Compatibility renderer.

If using the Compatibility renderer, consider using Sprite3D as an alternative for projecting decals onto (mostly) flat surfaces.

Decals are projected textures that apply on opaque or transparent surfaces in 3D. This projection happens in real-time and doesn't rely on mesh generation. This allows you to move decals every frame with only a small performance impact, even when applied on complex meshes.

While decals cannot add actual geometry detail onto the projected surface, decals can still make use of physically-based rendering to provide similar properties to full-blown PBR materials.

On this page, you'll learn:

How to set up decals in the 3D editor.

How to create decals during gameplay in a 3D scene (such as bullet impacts).

How to balance decal configuration between performance and quality.

The Godot demo projects repository contains a 3D decals demo.

If you're looking to write arbitrary 3D text on top of a surface, use 3D text placed close to a surface instead of a Decal node.

Sometimes, the fastest way to add texture detail to a scene is to use decals. This is especially the case for organic detail, such as patches of dirt or sand scattered on a large surface. Decals can help break up texture repetition in scenes and make patterns look more natural. On a smaller scale, decals can also be used to create detail variations for objects. For example, decals can be used to add nuts and bolts on top of hard-surface geometry.

Since decals can inject their own PBR properties on top of the projected surfaces, they can also be used to create footprints or wet puddles.

Dirt added on top of level geometry using decals

Decals can represent temporary or persistent gameplay effects such as bullet impacts and explosion scorches.

Using an AnimationPlayer node or a script, decals can be made to fade over time (and then be removed using queue_free()) to improve performance.

Blob shadows are frequently used in mobile projects (or to follow a retro art style), as real-time lighting tends to be too expensive on low-end mobile devices. However, when relying on baked lightmaps with fully baked lights, dynamic objects will not cast any shadow from those lights. This makes dynamic objects in lightmapped scenes look flat in comparison to real-time lighting, with dynamic objects almost looking like they're floating.

Thanks to blob shadows, dynamic objects can still cast an approximative shadow. Not only this helps with depth perception in the scene, but this can also be a gameplay element, especially in 3D platformers. The blob shadow's length can be extended to let the player know where they will land if they fall straight down.

Even with real-time lighting, blob shadows can still be useful as a form of ambient occlusion for situations where SSAO is too expensive or too unstable due to its screen-space nature. For example, vehicles' underside shadows are well-represented using blob shadows.

Blob shadow under object comparison

Create a Decal node in the 3D editor.

In the inspector, expand the Textures section and load a texture in Textures > Albedo.

Move the Decal node towards an object, then rotate it so the decal is visible (and in the right orientation). If the decal appears mirrored, try to rotate it by 180 degrees. You can double-check whether it's in the right orientation by increasing Parameters > Normal Fade to 0.5. This will prevent the Decal from being projected on surfaces that are not facing the decal.

If your decal is meant to affect static objects only, configure it to prevent affecting dynamic objects (or vice versa). To do so, change the decal's Cull Mask property to exclude certain layers. After doing this, modify your dynamic objects' MeshInstance3D nodes to change their visibility layers. For instance, you can move them from layer 1 to layer 2, then disable layer 2 in the decal's Cull Mask property.

Extents: The size of the decal. The Y axis determines the length of the decal's projection. Keep the projection length as short as possible to improve culling opportunities, therefore improving performance.

Albedo: The albedo (diffuse/color) map to use for the decal. In most situations, this is the texture you want to set first. If using a normal or ORM map, an albedo map must be set to provide an alpha channel. This alpha channel will be used as a mask to determine how much the normal/ORM maps will affect the underlying surface.

Normal: The normal map to use for the decal. This can be used to increase perceived detail on the decal by modifying how light reacts to it. The impact of this texture is multiplied by the albedo texture's alpha channel (but not Albedo Mix).

ORM: The Occlusion/Roughness/Metallic map to use for the decal. This is an optimized format for storing PBR material maps. Ambient Occlusion map is stored in the red channel, roughness map in the green channel, metallic map in the blue channel. The impact of this texture is multiplied by the albedo texture's alpha channel (but not Albedo Mix).

Emission: The emission texture to use for the decal. Unlike Albedo, this texture will appear to glow in the dark.

Emission Energy: The brightness of the emission texture.

Modulate: Multiplies the color of the albedo and emission textures. Use this to tint decals (e.g. for paint decals, or to increase variation by randomizing each decal's modulation).

Albedo Mix: The opacity of the albedo texture. Unlike using an albedo texture with a more transparent alpha channel, decreasing this value below 1.0 does not reduce the impact of the normal/ORM texture on the underlying surface. Set this to 0.0 when creating normal/ORM-only decals such as footsteps or wet puddles.

Normal Fade: Fades the Decal if the angle between the Decal's AABB and the target surface becomes too large. A value of 0.0 projects the decal regardless of angle, while a value of 0.999 limits the decal to surfaces that are nearly perpendicular. Setting Normal Fade to a value greater than 0.0 has a small performance cost due to the added normal angle computations.

Upper Fade: The curve over which the decal will fade as the surface gets further from the center of the AABB (towards the decal's projection angle). Only positive values are valid.

Lower Fade: The curve over which the decal will fade as the surface gets further from the center of the AABB (away from the decal's projection angle). Only positive values are valid.

Enabled: Controls whether distance fade (a form of LOD) is enabled. The decal will fade out over Begin + Length, after which it will be culled and not sent to the shader at all. Use this to reduce the number of active decals in a scene and thus improve performance.

Begin: The distance from the camera at which the decal begins to fade away (in 3D units).

Length: The distance over which the decal fades (in 3D units). The decal becomes slowly more transparent over this distance and is completely invisible at the end. Higher values result in a smoother fade-out transition, which is more suited when the camera moves fast.

Cull Mask: Specifies which VisualInstance3D layers this decal will project on. By default, decals affect all layers. This is used so you can specify which types of objects receive the decal and which do not. This is especially useful so you can ensure that dynamic objects don't accidentally receive a Decal intended for the terrain under them.

By default, decals are ordered based on the size of their AABB and the distance to the camera. AABBs that are closer to the camera are rendered first, which means that decal rendering order can sometimes appear to change depending on camera position if some decals are positioned at the same location.

To resolve this, you can adjust the Sorting Offset property in the VisualInstance3D section of the Decal node inspector. This offset is not a strict priority order, but a guideline that the renderer will use as the AABB size still affects how decal sorting works. Therefore, higher values will always result in the decal being drawn above other decals with a lower sorting offset.

If you want to ensure a decal is always rendered on top of other decals, you need to set its Sorting Offset property to a positive value greater than the AABB length of the largest decal that may overlap it. To make this decal drawn behind other decals instead, set the Sorting Offset to the same negative value.

VisualInstance3D Sorting Offset comparison on Decals

Decal rendering performance is mostly determined by their screen coverage, but also their number. In general, a few large decals that cover up most of the screen will be more expensive to render than many small decals that are scattered around.

To improve rendering performance, you can enable the Distance Fade property as described above. This will make distant decals fade out when they are far away from the camera (and may have little to no impact on the final scene rendering). Using node groups, you can also prevent non-essential decorative decals from spawning based on user configuration.

The way decals are rendered also has an impact on performance. The Rendering > Textures > Decals > Filter advanced project setting lets you control how decal textures should be filtered. Nearest/Linear does not use mipmaps. However, decals will look grainy at a distance. Nearest/Linear Mipmaps will look smoother at a distance, but decals will look blurry when viewed from oblique angles. This can be resolved by using Nearest/Linear Mipmaps Anisotropic, which provides the highest quality, but is also slower to render.

If your project has a pixel art style, consider setting the filter to one of the Nearest values so that decals use nearest-neighbor filtering. Otherwise, stick to Linear.

Decals cannot affect material properties other than the ones listed above, such as height (for parallax mapping).

For performance reasons, decals use purely fixed rendering logic. This means decals cannot use custom shaders. However, custom shaders on the projected surfaces are able to read the information that is overridden by decals on top of them, such as roughness and metallic.

When using the Forward+ renderer, Godot uses a clustering approach for decal rendering. As many decals as desired can be added (as long as performance allows). However, there's still a default limit of 512 clustered elements that can be present in the current camera view. A clustered element is an omni light, a spot light, a decal or a reflection probe. This limit can be increased by adjusting Max Clustered Elements in Project Settings > Rendering > Limits > Cluster Builder.

When using the Mobile renderer, only 8 decals can be applied on each individual Mesh resource. If there are more decals affecting a single mesh, not all of them will be rendered on the mesh.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Using GridMaps — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/using_gridmaps.html

**Contents:**
- Using GridMaps
- Introduction
- Example project
- Creating a MeshLibrary
- Collisions
- Materials
- NavigationMeshes
- Lightmaps
- MeshLibrary format
- Exporting the MeshLibrary

Gridmaps are a tool for creating 3D game levels, similar to the way TileMap works in 2D. You start with a predefined collection of 3D meshes (a MeshLibrary) that can be placed on a grid, as if you were building a level with an unlimited amount of Lego blocks.

Collisions and navigation can also be added to the meshes, just like you would do with the tiles of a tilemap.

To learn how GridMaps work, start by downloading the sample project: gridmap_starter.zip.

Unzip this project and add it to the Project Manager using the "Import" button. You may get a popup saying that it needs to be converted to a newer Godot version, click Convert project.godot.

To begin, you need a MeshLibrary, which is a collection of individual meshes that can be used in the gridmap. Open the "mesh_library_source.tscn" scene to see an example of how to set up the mesh library.

As you can see, this scene has a Node3D node as its root, and a number of MeshInstance3D node children.

If you don't need any physics in your scene, then you're done. However, in most cases you'll want to assign collision bodies to the meshes.

You can manually assign a StaticBody3D and CollisionShape3D to each mesh. Alternatively, you can use the "Mesh" menu to automatically create the collision body based on the mesh data.

Note that a "Convex" collision body will work better for simple meshes. For more complex shapes, select "Create Trimesh Static Body". Once each mesh has a physics body and collision shape assigned, your mesh library is ready to be used.

Only the materials from within the meshes are used when generating the mesh library. Materials set on the node will be ignored.

Like all mesh instances, MeshLibrary items can be assigned a NavigationMesh resource, which can be created manually, or baked as described below.

To create the NavigationMesh from a MeshLibrary scene export, place a NavigationRegion3D child node below the main MeshInstance3D for the GridMap item. Add a valid NavigationMesh resource to the NavigationRegion3D and some source geometry nodes below and bake the NavigationMesh.

With small grid cells it is often necessary to reduce the NavigationMesh properties for agent radius and region minimum size.

Nodes below the NavigationRegion3D are ignored for the MeshLibrary scene export, so additional nodes can be added as source geometry just for baking the navmesh.

The baked cell size of the NavigationMesh must match the NavigationServer map cell size to properly merge the navigation meshes of different grid cells.

It is possible to bake lightmaps onto a GridMap. Lightmap UV2 data will be reused from meshes if already present. If UV2 data is not present, then it will be automatically generated on bake with a lightmap texel size of 0.1 units. To generate UV2 data with a different lightmap texel size, you can set the global illumination mode in the Import dock to Static Lightmaps and specify the texel size there. This option must be changed before the scene is converted to a MeshLibrary, as changing it later on will not affect the existing MeshLibrary data.

Aside from this peculiarity, the lightmap baking process is the same as for any other 3D scene. See Using Lightmap global illumination for more information about lightmap baking.

To summarize the specific constraints of the MeshLibrary format, a MeshLibrary scene has a Node3D as the root node, and several child nodes which will become MeshLibrary items. Each child of the root node should:

Be a MeshInstance3D, which will become the MeshLibrary item. Only this visual mesh will be exported.

Have a material, in the mesh's material slot, not the MeshInstance3D's material slots.

Have up to one StaticBody3D child, for collision. The StaticBody3D should have one or more CollisionShape3D children.

Have up to one NavigationRegion3D child, for navigation. The NavigationRegion3D can have one or more additional MeshInstance3D children, which can be baked for navigation, but won't be exported as a visual mesh.

Only this specific format is recognized. Other node types placed as children will not be recognized and exported. GridMap is not a general-purpose system for placing nodes on a grid, but rather a specific, optimized system, designed to place meshes with collisions and navigation.

To export the library, click on Scene > Export As... > MeshLibrary..., and save it as a resource.

You can find an already exported MeshLibrary in the project named MeshLibrary.tres.

Create a new scene and add a GridMap node. Add the mesh library by dragging the resource file from the FileSystem dock and dropping it in the Mesh Library property in the Inspector.

The Physics Material setting allows you to override the physics material for every mesh in the NavigationMesh.

Under Cells, the Size property should be set to the size of your meshes. You can leave it at the default value for the demo. Uncheck the Center Y property.

The Collision options allow you to set the collision layer, collision mask, and priority for the entire grid. For more information on how those work see the Physics section.

Under Navigation is the "Bake Navigation" option. If enabled it creates a navigation region for each cell that uses a mesh library item with a navigation mesh.

If you click on the MeshLibrary itself in the inspector you can adjust settings for individual meshes, such as their navigation mesh, navigation layers, or if the mesh casts shadows.

At the bottom of the editor is the GridMap panel, which should have opened automatically when you added the GridMap node.

From left to right in the toolbar:

Transform: Adds a gizmo to the scene that allows you to change the relative position and rotation of the gridmap in the scene.

Selection: While active you can select an area in the viewport, click and drag to select more than one space on the grid.

Erase: While active, click in the viewport and delete meshes.

Paint: While active, click in the viewport and add whatever mesh is currently selected in the GridMap panel to the scene.

Pick: While active, clicking on a gridmap mesh in the viewport will cause it to be selected in the GridMap panel.

Fill: Fill the area that has been selected in the viewport with whatever mesh is selected in the GridMap bottom panel.

Move: Move whatever mesh or meshes are currently selected in the viewport.

Duplicate: Create a copy of whatever the selected mesh or meshes in the GridMap are.

Delete: Similar to erase, but for the entire selected area.

Cursor Rotate X: While the paint tool is selected, this will rotate the mesh that will be painted on the X-axis. This will also rotate selected areas if they are being moved.

Cursor Rotate Y: While the paint tool is selected, this will rotate the mesh that will be painted on the Y-axis. This will also rotate selected areas if they are being moved.

Cursor Rotate Z: While the paint tool is selected, this will rotate the mesh that will be painted on the Z-axis. This will also rotate selected areas if they are being moved.

Change Grid Floor: Adjusts what floor is currently being worked on, can be changed with the arrows or typing

Filter Meshes: Used to search for a specific mesh in the bottom panel.

Zoom: Controls the zoom level on meshes in the bottom panel.

Layout toggles: These two buttons toggle between different layouts for meshes in the bottom panel.

Tools dropdown: This button opens a dropdown menu with a few more options.

Clicking on Settings in that dropdown brings up a window that allows you to change the Pick Distance, which is the maximum distance at which tiles can be placed on a GridMap, relative to the camera position (in meters).

See GridMap for details on the node's methods and member variables.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Using ImmediateMesh — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/procedural_geometry/immediatemesh.html

**Contents:**
- Using ImmediateMesh
- User-contributed notes

The ImmediateMesh is a convenient tool to create dynamic geometry using an OpenGL 1.x-style API. Which makes it both approachable to use and efficient for meshes which need to be updated every frame.

Generating complex geometry (several thousand vertices) with this tool is inefficient, even if it's done only once. Instead, it is designed to generate simple geometry that changes every frame.

First, you need to create a MeshInstance3D and add an ImmediateMesh to it in the Inspector.

Next, add a script to the MeshInstance3D. The code for the ImmediateMesh should go in the _process() function if you want it to update each frame, or in the _ready() function if you want to create the mesh once and not update it. If you only generate a surface once, the ImmediateMesh is just as efficient as any other kind of mesh as the generated mesh is cached and reused.

To begin generating geometry you must call surface_begin(). surface_begin() takes a PrimitiveType as an argument. PrimitiveType instructs the GPU how to arrange the primitive based on the vertices given whether it is triangles, lines, points, etc. A complete list can be found under the Mesh class reference page.

Once you have called surface_begin() you are ready to start adding vertices. You add vertices one at a time. First you add vertex specific attributes such as normals or UVs using surface_set_****() (e.g. surface_set_normal()). Then you call surface_add_vertex() to add a vertex with those attributes. For example:

Only attributes added before the call to surface_add_vertex() will be included in that vertex. If you add an attribute twice before calling surface_add_vertex(), only the second call will be used.

Finally, once you have added all your vertices call surface_end() to signal that you have finished generating the surface. You can call surface_begin() and surface_end() multiple times to generate multiple surfaces for the mesh.

The example code below draws a single triangle in the _ready() function.

The ImmediateMesh can also be used across frames. Each time you call surface_begin() and surface_end(), you are adding a new surface to the ImmediateMesh. If you want to recreate the mesh from scratch each frame, call clear_surfaces() before calling surface_begin().

The above code will dynamically create and draw a single surface each frame.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
# Add a vertex with normal and uv.
surface_set_normal(Vector3(0, 1, 0))
surface_set_uv(Vector2(1, 1))
surface_add_vertex(Vector3(0, 0, 1))
```

Example 2 (unknown):
```unknown
extends MeshInstance3D

func _ready():
    # Begin draw.
    mesh.surface_begin(Mesh.PRIMITIVE_TRIANGLES)

    # Prepare attributes for add_vertex.
    mesh.surface_set_normal(Vector3(0, 0, 1))
    mesh.surface_set_uv(Vector2(0, 0))
    # Call last for each vertex, adds the above attributes.
    mesh.surface_add_vertex(Vector3(-1, -1, 0))

    mesh.surface_set_normal(Vector3(0, 0, 1))
    mesh.surface_set_uv(Vector2(0, 1))
    mesh.surface_add_vertex(Vector3(-1, 1, 0))

    mesh.surface_set_normal(Vector3(0, 0, 1))
    mesh.surface_set_uv(Vector2(1, 1))
    mesh.surface_add_vertex(Vector3(1, 1, 0))

    # End drawing.
    mesh.surface_end()
```

Example 3 (unknown):
```unknown
extends MeshInstance3D

func _process(delta):

    # Clean up before drawing.
    mesh.clear_surfaces()

    # Begin draw.
    mesh.surface_begin(Mesh.PRIMITIVE_TRIANGLES)

    # Draw mesh.

    # End drawing.
    mesh.surface_end()
```

---

## Using Lightmap global illumination — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/global_illumination/using_lightmap_gi.html

**Contents:**
- Using Lightmap global illumination
- Visual comparison
- Setting up
  - Unwrap on scene import (recommended)
  - Unwrap from within Godot
  - Unwrap from your 3D modeling software
  - Generating UV2 for primitive meshes
  - Generating UV2 for CSG nodes
  - Checking UV2
- Setting up the scene

Baked lightmaps are a workflow for adding indirect (or fully baked) lighting to a scene. Unlike the VoxelGI and SDFGI approaches, baked lightmaps work fine on low-end PCs and mobile devices, as they consume almost no resources at runtime. Also unlike VoxelGI and SDFGI, baked lightmaps can optionally be used to store direct lighting, which provides even further performance gains.

Unlike VoxelGI and SDFGI, baked lightmaps are completely static. Once baked, they can't be modified at all. They also don't provide the scene with reflections, so using Reflection probes together with it on interiors (or using a Sky on exteriors) is a requirement to get good quality.

As they are baked, they have fewer problems than VoxelGI and SDFGI regarding light bleeding, and indirect light will often look better. The downside is that baking lightmaps takes longer compared to baking VoxelGI. While baking VoxelGI can be done in a matter of seconds, baking lightmaps can take several minutes if not more. This can slow down iteration speed significantly, so it is recommended to bake lightmaps only when you actually need to see changes in lighting. Since Godot 4.0, lightmaps are baked on the GPU, making light baking faster if you have a mid-range or high-end dedicated GPU.

Baking lightmaps will also reserve baked materials' UV2 slot, which means you can no longer use it for other purposes in materials (either in the built-in Standard Material 3D and ORM Material 3D or in custom shaders).

Despite their lack of flexibility, baked lightmaps typically offer both the best quality and performance at the same time in (mostly) static scenes. This makes lightmaps still popular in game development, despite lightmaps being the oldest technique for global illumination in video games.

Not sure if LightmapGI is suited to your needs? See Which global illumination technique should I use? for a comparison of GI techniques available in Godot 4.

LightmapGI disabled.

LightmapGI enabled (with indirect light baked only). Direct light is still real-time, allowing for subtle changes during gameplay.

LightmapGI enabled (with direct and indirect light baked). Best performance, but lower quality visuals. Notice the blurrier sun shadow in the top-right corner.

Here are some comparisons of how LightmapGI vs. VoxelGI look. Notice that lightmaps are more accurate, but also suffer from the fact that lighting is on an unwrapped texture, so transitions and resolution may not be that good. VoxelGI looks less accurate (as it's an approximation), but smoother overall.

SDFGI is also less accurate compared to LightmapGI. However, SDFGI can support large open worlds without any need for baking.

Baking lightmaps in the web editors is not supported due to graphics API limitations. On the web platform, only rendering lightmaps that were baked on a different platform is supported.

The LightmapGI node only bakes nodes that are on the same level as the LightmapGI node (siblings), or nodes that are children of the LightmapGI node. This allows you to use several LightmapGI nodes to bake different parts of the scene, independently from each other.

First of all, before the lightmapper can do anything, the objects to be baked need a UV2 layer and a texture size. A UV2 layer is a set of secondary texture coordinates that ensures any face in the object has its own place in the UV map. Faces must not share pixels in the texture.

There are a few ways to ensure your object has a unique UV2 layer and texture size:

In most scenarios, this is the best approach to use. The only downside is that, on large models, unwrapping can take a while on import. Nonetheless, Godot will cache the UV2 across reimports, so it will only be regenerated when needed.

Select the imported scene in the filesystem dock, then go to the Import dock. There, the following option can be modified:

The Meshes > Light Baking option must be set to Static Lightmaps (VoxelGI/SDFGI/LightmapGI):

When unwrapping on import, you can adjust the texture size using the Meshes > Lightmap Texel Size option. Lower values will result in more detailed lightmaps, possibly resulting in higher visual quality at the cost of longer bake times and larger lightmap file sizes. The default value of 0.2 is suited for small/medium-sized scenes, but you may want to increase it to 0.5 or even more for larger scenes. This is especially the case if you're baking indirect lighting only, as indirect light is low-frequency data (which means it doesn't need high-resolution textures to be accurately represented).

The effect of setting this option is that all meshes within the scene will have their UV2 maps properly generated.

When reusing a mesh within a scene, keep in mind that UVs will be generated for the first instance found. If the mesh is re-used with different scales (and the scales are wildly different, more than half or twice), this will result in inefficient lightmaps. To avoid this, adjust the Lightmap Scale property in the GeometryInstance3D section of a MeshInstance3D node. This lets you increase the level of lightmap detail for specific MeshInstance3D nodes (but not decrease it).

Also, the *.unwrap_cache files should not be ignored in version control as these files guarantee that UV2 reimports are consistent across platforms and engine versions.

If this Mesh menu operation is used on an imported 3D scene, the generated UV2 will be lost when the scene is reloaded.

Godot has an option to unwrap meshes and visualize the UV channels. After selecting a MeshInstance3D node, it can be found in the Mesh menu at the top of the 3D editor viewport:

This will generate a second set of UV2 coordinates which can be used for baking. It will also set the texture size automatically.

The last option is to do it from your favorite 3D app. This approach is generally not recommended, but it's explained so that you know it exists. The main advantage is that, on complex objects that you may want to re-import a lot, the texture generation process can be quite costly within Godot, so having it unwrapped before import can be faster.

Simply do an unwrap on the second UV2 layer.

Then import the 3D scene normally. Remember you will need to set the texture size on the mesh after import.

If you use external meshes on import, the size will be kept. Be wary that most unwrappers in 3D modeling software are not quality-oriented, as they are meant to work quickly. You will mostly need to use seams or other techniques to create better unwrapping.

This option is only available for primitive meshes such as BoxMesh, CylinderMesh, PlaneMesh, etc.

Enabling UV2 on primitive meshes allows you to make them receive and contribute to baked lighting. This can be used in certain lighting setups. For instance, you could hide a torus that has an emissive material after baking lightmaps to create an area light that follows the shape of a torus.

By default, primitive meshes do not have UV2 generated to save resources (as these meshes may be created during gameplay). You can edit a primitive mesh in the inspector and enable Add UV2 to make the engine procedurally generate UV2 for a primitive mesh. The default UV2 Padding value is tuned to avoid most lightmap bleeding, without wasting too much space on the edges. If you notice lightmap bleeding on a specific primitive mesh only, you may have to increase UV2 Padding.

Lightmap Size Hint represents the size taken by a single mesh on the lightmap texture, which varies depending on the mesh's size properties and the UV2 Padding value. Lightmap Size Hint should not be manually changed, as any modifications will be lost when the scene is reloaded.

Since Godot 4.4, you can convert a CSG node and its children to a MeshInstance3D. This can be used to bake lightmaps on a CSG node by following these steps:

Select the root CSG node and choose CSG > Bake Mesh Instance at the top of the 3D editor viewport.

Hide the root CSG node that was just baked (it is not hidden automatically).

Select the newly created MeshInstance3D node and choose Mesh > Unwrap UV2 for Lightmap/AO.

Remember to keep the original CSG node in the scene tree, so that you can perform changes to the geometry later if needed. To make changes to the geometry, remove the MeshInstance3D node and make the root CSG node visible again.

In the Mesh menu mentioned before, the UV2 texture coordinates can be visualized. If something is failing, double-check that the meshes have these UV2 coordinates:

Before anything is done, a LightmapGI node needs to be added to a scene. This will enable light baking on all nodes (and sub-nodes) in that scene, even on instanced scenes.

A sub-scene can be instanced several times, as this is supported by the baker. Each instance will be assigned a lightmap of its own. To avoid issues with inconsistent lightmap texel scaling, make sure to respect the rule about mesh scaling mentioned before.

For a MeshInstance3D node to take part in the baking process, it needs to have its bake mode set to Static. Meshes that have their bake mode set to Disabled or Dynamic will be ignored by the lightmapper.

When auto-generating lightmaps on scene import, this is enabled automatically.

Lights are baked with indirect light only by default. This means that shadowmapping and lighting are still dynamic and affect moving objects, but light bounces from that light will be baked.

Lights can be disabled (no bake) or be fully baked (direct and indirect). This can be controlled from the Bake Mode menu in lights:

The light is ignored when baking lightmaps. This is the mode to use for dynamic lighting effects such as explosions and weapon effects.

Hiding a light has no effect on the resulting lightmap bake. This means you must use the Disabled bake mode instead of hiding the Light node by disabling its Visible property.

This is the default mode, and is a compromise between performance and real-time friendliness. Only indirect lighting will be baked. Direct light and shadows are still real-time, as they would be without LightmapGI.

This mode allows performing subtle changes to a light's color, energy and position while still looking fairly correct. For example, you can use this to create flickering static torches that have their indirect light baked.

Depending on the value of Shadowmask Mode, it is possible to still get distant baked shadows for DirectionalLight3D. This allows shadows up close to be real-time and show dynamic objects, while allowing static objects in the distance to still cast shadows.

Both indirect and direct lighting will be baked. Since static surfaces can skip lighting and shadow computations entirely, this mode provides the best performance along with smooth shadows that never fade based on distance. The real-time light will not affect baked surfaces anymore, but it will still affect dynamic objects. When using the All bake mode on a light, dynamic objects will not cast real-time shadows onto baked surfaces, so you need to use a different approach such as blob shadows instead. Blob shadows can be implemented with a Decal node.

The light will not be adjustable at all during gameplay. Moving the light or changing its color (or energy) will not have any effect on static surfaces.

Since bake modes can be adjusted on a per-light basis, it is possible to create hybrid baked light setups. One popular option is to use a real-time DirectionalLight with its bake mode set to Dynamic, and use the Static bake mode for OmniLights and SpotLights. This provides good performance while still allowing dynamic objects to cast real-time shadows in outdoor areas.

Fully baked lights can also make use of light nodes' Size (omni/spot) or Angular Distance (directional) properties. This allows for shadows with realistic penumbra that increases in size as the distance between the caster and the shadow increases. This also has a lower performance cost compared to real-time PCSS shadows, as only dynamic objects have real-time shadows rendered on them.

To begin the bake process, click the Bake Lightmaps button at the top of the 3D editor viewport when selecting the LightmapGI node:

This can take from seconds to minutes (or hours) depending on scene size, bake method and quality selected.

Baking lightmaps is a process that can require a lot of video memory, especially if the resulting texture is large. Due to internal limitations, the engine may also crash if the generated texture size is too large (even on systems with a lot of video memory).

To avoid crashes, make sure the lightmap texel size in the Import dock is set to a high enough value.

Quality: Four bake quality modes are provided: Low, Medium, High, and Ultra. Higher quality takes more time, but result in a better-looking lightmap with less noise. The difference is especially noticeable with emissive materials or areas that get little to no direct lighting. Each bake quality mode can be further adjusted in the Project Settings.

Supersampling: This creates the lightmap at a higher resolution and then downsamples it. This reduces noise and light leaking, and produces better shadows with small scale details. However, using it will increase bake times and memory usage during lightmap baking. The Supersampling Factor changes the size the lightmap is rendered at before downsampling.

Bounces: The number of bounces to use for indirect lighting. The default value (3) is a good compromise between bake times and quality. Higher values will make light bounce around more times before it stops, which makes indirect lighting look smoother (but also possibly brighter depending on materials and geometry).

Bounce Indirect Energy: The global multiplier to use when baking lights' indirect energy. This multiplies each light's own Indirect Energy value. Values different from 1.0 are not physically accurate, but can be used for artistic effect.

Directional: If enabled, stores directional information for lightmaps. This improves normal mapped materials' appearance for baked surfaces, especially with fully baked lights (since they also have direct light baked). The downside is that directional lightmaps are slightly more expensive to render. They also require more time to bake and result in larger file sizes.

Shadowmask Mode: If set to a mode other than None, the first DirectionalLight3D in the scene with the Dynamic global illumination mode will have its static shadows baked to a separate texture called a shadowmask. This can be used to allow distant static objects to cast shadows onto other static objects regardless of the distance from the camera. See the section on shadowmasking for further details.

Interior: If enabled, environment lighting will not be sourced. Use this for purely indoor scenes to avoid light leaks.

Use Texture for Bounces: If enabled, a texture with the lighting information will be generated to speed up the generation of indirect lighting at the cost of some accuracy. The geometry might exhibit extra light leak artifacts when using low resolution lightmaps or UVs that stretch the lightmap significantly across surfaces. Leave this enabled if unsure.

Use Denoiser: If enabled, uses a denoising algorithm to make the lightmap significantly less noisy. This increases bake times and can occasionally introduce artifacts, but the result is often worth it. See Denoising for more information.

Denoiser Strength: The strength of denoising step applied to the generated lightmaps. Higher values are more effective at removing noise, but can reduce shadow detail for static shadows. Only effective if denoising is enabled and the denoising method is JNLM (OIDN does not have a denoiser strength setting).

Bias: The offset value to use for shadows in 3D units. You generally don't need to change this value, except if you run into issues with light bleeding or dark spots in your lightmap after baking. This setting does not affect real-time shadows casted on baked surfaces (for lights with Dynamic bake mode).

Max Texture Size: The maximum texture size for the generated texture atlas. Higher values will result in fewer slices being generated, but may not work on all hardware as a result of hardware limitations on texture sizes. Leave this at its default value of 16384 if unsure.

Environment > Mode: Controls how environment lighting is sourced when baking lightmaps. The default value of Scene is suited for levels with visible exterior parts. For purely indoor scenes, set this to Disabled to avoid light leaks and speed up baking. This can also be set to Custom Sky or Custom Color to use environment lighting that differs from the actual scene's environment sky.

Gen Probes > Subdiv: See Dynamic objects.

Data > Light Data: See Lightmap data.

When using a DirectionalLight3D, the maximum distance at which it can draw real-time shadows is limited by its Shadow Max Distance property. This can be an issue in large scenes, as distant objects won't appear to have any shadows from the DirectionalLight3D. While this can be resolved by using the Static global illumination mode on the DirectionalLight3D, this has several downsides:

Since both direct and indirect light are baked, there is no way for dynamic objects to cast shadows onto static surfaces in a realistic manner. Godot skips shadow sampling entirely in this case to avoid "double lighting" artifacts.

Static shadows up close lack in detail, as they only rely on the lightmap texture and not on real-time shadow cascades.

We can avoid these downsides while still benefiting from distant shadows by using shadowmasking. While dynamic objects won't receive shadows from the shadowmask, it still greatly improves visuals since most scenes are primarily comprised of static objects.

Since the lightmap texture alone doesn't contain shadow information, we can bake this shadow information to a separate texture called a shadowmask.

Shadowmasking only affects the first DirectionalLight3D in the scene (determined by tree order) that has the Dynamic global illumination mode. It is not possible to use shadowmasking with the Static global illumination mode, as this mode skips shadow sampling on static objects entirely. This is because the Static global illumination mode bakes both direct and indirect light.

Three shadowmasking modes are available:

None (default): Don't bake a shadowmask texture. Directional shadows will not be visible outside the range specified by the DirectionalLight3D's Shadow Max Distance property.

Replace: Bakes a shadowmask texture, and uses it to draw directional shadows when outside the range specified by the DirectionalLight3D's Shadow Max Distance property. Shadows within this range remain fully real-time. This option generally makes the most sense for most scenes, as it can deal well with static objects that exhibit subtle motion (e.g. foliage shadows).

Overlay: Bakes a shadowmask texture, and uses it to draw directional shadows regardless of the distance from the camera. Shadows within the range of the DirectionalLight3D's Shadow Max Distance property will be overlaid with real-time shadows. This can make the transition between real-time and baked shadows less jarring, at the cost of a "smearing" effect present on static object shadows depending on lightmap texel density. Also, this mode can't deal as well with static objects that exhibit subtle motion (such as foliage), as the baked shadows can't be animated over time. Still, for scenes where the camera moves quickly, this may be a better choice than Replace.

Here's a visual comparison of the shadowmask modes with a scene where the Shadow Max Distance was set very low for comparison purposes. The blue boxes are dynamic objects, while the rest of the scene is a static object. There is only a single DirectionalLight3D in the scene with the Dynamic global illumination mode:

Comparison between shadowmask modes

It is possible to switch between the Replace and Overlay shadowmask modes without having to bake lightmaps again.

Since high-quality bakes can take very long (up to dozens of minutes for large complex scenes), it is recommended to use lower quality settings at first. Then, once you are confident with your scene's lighting setup, raise the quality settings and perform a "final" bake before exporting your project.

Reducing the lightmap resolution by increasing Lightmap Texel Size on the imported 3D scenes will also speed up baking significantly. However, this will require you to reimport all lightmapped 3D scenes before you can bake lightmaps again.

Since baking lightmaps relies on raytracing, there will always be visible noise in the "raw" baked lightmap. Noise is especially visible in areas that are difficult to reach by bounced light, such as indoor areas with small openings where the sunlight can enter. Noise can be reduced by increasing bake quality, but doing so will increase bake times significantly.

Comparison between denoising disabled and enabled (with the default JNLM denoiser).

To combat noise without increasing bake times too much, a denoiser can be used. A denoiser is an algorithm that runs on the final baked lightmap, detects patterns of noise and softens them while attempting to best preserve detail. Godot offers two denoising algorithms:

JNLM is the default denoising method and is included in Godot. It uses a simple but efficient denoising algorithm known as non-local means. JNLM runs on the GPU using a compute shader, and is compatible with any GPU that can run Godot 4's Vulkan-based rendering methods. No additional setup is required.

JNLM's denoising can be adjusted using the Denoiser Strength property that is visible when Use Denoiser enabled. Higher values can be more effective at removing noise, at the cost of suppressing shadow detail for static shadows.

Comparison between JNLM denoiser strength values. Higher values can reduce detail.

Unlike JNLM, OIDN uses a machine learning approach to denoising lightmaps. It features a model specifically trained to remove noise from lightmaps while preserving more shadow detail in most scenes compared to JNLM.

OIDN can run on the GPU if hardware acceleration is configured. With a modern high-end GPU, this can provide a speedup of over 50× over CPU-based denoising:

On AMD GPUs, HIP must be installed and configured.

On NVIDIA GPUs, CUDA must be installed and configured. This may automatically be done by the NVIDIA installer, but on Linux, CUDA libraries may not be installed by default. Double-check that the CUDA packages from your Linux distribution are installed.

On Intel GPUs, SYCL must be installed and configured.

If hardware acceleration is not available, OIDN will fall back to multithreaded CPU-based denoising. To confirm whether GPU-based denoising is working, use a GPU utilization monitor while baking lightmaps and look at the GPU utilization percentage and VRAM utilization while the denoising step is shown in the Godot editor. The nvidia-smi command line tool can be useful for this.

OIDN is not included with Godot due to its relatively large download size. You can download precompiled OIDN binary packages from its website. Extract the package to a location on your PC, then specify the path to the oidnDenoise executable in the Editor Settings (FileSystem > Tools > OIDN > OIDN Denoise Path). This executable is located within the bin folder of the binary package you extracted.

After specifying the path to the OIDN denoising executable, change the denoising method in the project settings by setting Rendering > Lightmapping > Denoiser to OIDN. This will affect all lightmap bakes on this project after the setting is changed.

The denoising method is configured in the project settings instead of the editor settings. This is done so that different team members working on the same project are assured to be using the same denoising method for consistent results.

Comparison between JNLM and OIDN denoisers. Notice how OIDN better preserves detail and reduces seams across different objects.

Unlike VoxelGI and SDFGI, dynamic objects receive indirect lighting differently compared to static objects. This is because lightmapping is only performed on static objects.

To display indirect lighting on dynamic objects, a 3D probe system is used, with light probes being spread throughout the scene. When baking lightmaps, the lightmapper will calculate the amount of indirect light received by the probe. Direct light is not stored within light probes, even for lights that have their bake mode set to Static (as dynamic objects continue to be lit in real-time).

There are 2 ways to add light probes to a scene:

Automatic: Set Gen Probes > Subdiv to a value other than Disabled, then bake lightmaps. The default is 8, but you can choose a greater value to improve precision at the cost of longer bake times and larger output file size.

Manual: In addition or as an alternative to generating probes automatically, you can add light probes manually by adding LightmapProbe nodes to the scene. This can be used to improve lighting detail in areas frequently travelled by dynamic objects. After placing LightmapProbe nodes in the scene, you must bake lightmaps again for them to be effective.

After baking lightmaps, you will notice white spheres in the 3D scene that represent how baked lighting will affect dynamic objects. These spheres do not appear in the running project.

If you want to hide these spheres in the editor, toggle View > Gizmos > LightmapGI at the top of the 3D editor (a "closed eye" icon indicates the gizmo is hidden).

The Data > Light Data property in the LightmapGI node contains the lightmap data after baking. Textures are saved to disk, but this also contains the capture data for dynamic objects, which can be heavy. If you are using a scene in .tscn format, you should save this resource to an external binary .lmbake file to avoid bloating the .tscn scene with binary data encoded in Base64.

The generated EXR file can be viewed and even edited using an image editor to perform post-processing if needed. However, keep in mind that changes to the EXR file will be lost when baking lightmaps again.

If you notice LightmapGI nodes popping in and out of existence as the camera moves, this is most likely because the engine is rendering too many LightmapGI instances at once. Godot is limited to rendering 8 LightmapGI nodes at once, which means up to 8 instances can be in the camera view before some of them will start flickering.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Using MultiMeshInstance3D — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/using_multi_mesh_instance.html

**Contents:**
- Using MultiMeshInstance3D
- Introduction
- Setting up the nodes
- MultiMesh settings
  - Target Surface
  - Source Mesh
  - Mesh Up Axis
  - Random Rotation
  - Random Tilt
  - Random Scale

The content of this page was not yet updated for Godot 4.5 and may be outdated. If you know how to improve this page or you can confirm that it's up to date, feel free to open a pull request.

In a normal scenario, you would use a MeshInstance3D node to display a 3D mesh like a human model for the main character, but in some cases, you would like to create multiple instances of the same mesh in a scene. You could duplicate the same node multiple times and adjust the transforms manually. This may be a tedious process and the result may look mechanical. Also, this method is not conducive to rapid iterations. MultiMeshInstance3D is one of the possible solutions to this problem.

MultiMeshInstance3D, as the name suggests, creates multiple copies of a MeshInstance over a surface of a specific mesh. An example would be having a tree mesh populate a landscape mesh with trees of random scales and orientations.

The basic setup requires three nodes: the MultiMeshInstance3D node and two MeshInstance3D nodes.

One node is used as the target, the surface mesh that you want to place multiple meshes on. In the tree example, this would be the landscape.

The other node is used as the source, the mesh that you want to have duplicated. In the tree case, this would be the tree itself.

In our example, we would use a Node3D node as the root node of the scene. Your scene tree would look like this:

For simplicity's sake, this tutorial uses built-in primitives.

Now you have everything ready. Select the MultiMeshInstance3D node and look at the toolbar, you should see an extra button called MultiMesh next to View. Click it and select Populate surface in the dropdown menu. A new window titled Populate MultiMesh will pop up.

Below are descriptions of the options.

The mesh used as the target surface on which to place copies of your source mesh.

The mesh you want duplicated on the target surface.

The axis used as the up axis of the source mesh.

Randomizing the rotation around the up axis of the source mesh.

Randomizing the overall rotation of the source mesh.

Randomizing the scale of the source mesh.

The scale of the source mesh that will be placed over the target surface.

The amount of mesh instances placed over the target surface.

Select the target surface. In the tree case, this should be the landscape node. The source mesh should be the tree node. Adjust the other parameters according to your preference. Press Populate and multiple copies of the source mesh will be placed over the target mesh. If you are satisfied with the result, you can delete the mesh instance used as the source mesh.

The end result should look like this:

To change the result, repeat the previous steps with different parameters.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Using navigation meshes — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/navigation/navigation_using_navigationmeshes.html

**Contents:**
- Using navigation meshes
- Baking a navigation mesh with a NavigationRegion
- Baking a navigation mesh with the NavigationServer
- Baking navigation mesh chunks for large worlds
- Navigation mesh baking common problems
- Navigation mesh script templates
- User-contributed notes

2D and 3D versions of the navigation mesh are available as NavigationPolygon and NavigationMesh respectively.

A navigation mesh only describes a traversable area for an agent's center position. Any radius values an agent may have are ignored. If you want pathfinding to account for an agent's (collision) size you need to shrink the navigation mesh accordingly.

Navigation works independently from other engine parts like rendering or physics. Navigation meshes are the only things considered when doing pathfinding, e.g. visuals and collision shapes for example are completely ignored by the navigation system. If you need to take other data (like visuals for example) into account when doing pathfinding, you need to adapt your navigation meshes accordingly. The process of factoring in navigation restrictions in navigation meshes is commonly referred to as navigation mesh baking.

A navigation mesh describes a surface that an agent can stand on safely with its center compared to physics shapes that describe outer collision bounds.

If you experience clipping or collision problems while following navigation paths, always remember that you need to tell the navigation system what your intentions are through an appropriate navigation mesh. By itself the navigation system will never know "this is a tree / rock / wall collision shape or visual mesh" because it only knows that "here I was told I can path safely because it is on a navigation mesh".

Navigation mesh baking can be done either by using a NavigationRegion2D or NavigationRegion3D, or by using the NavigationServer2D and NavigationServer3D API directly.

Baking a navigation mesh with agent radius offset from geometry.

The navigation mesh baking is made more accessible with the NavigationRegion node. When baking with a NavigationRegion node, the individual parsing, baking, and region update steps are all combined into one function.

The nodes are available in 2D and 3D as NavigationRegion2D and NavigationRegion3D respectively.

The navigation mesh source_geometry_mode can be switched to parse specific node group names so nodes that should be baked can be placed anywhere in the scene.

When a NavigationRegion2D node is selected in the Editor, bake options as well as polygon draw tools appear in the top bar of the Editor.

In order for the region to work a NavigationPolygon resource needs to be added.

The properties to parse and bake a navigation mesh are then part of the used resource and can be found in the resource Inspector.

The result of the source geometry parsing can be influenced with the following properties.

The parsed_geometry_type that filters if visual objects or physics objects or both should be parsed from the SceneTree. For more details on what objects are parsed and how, see the section about parsing source geometry below.

The collision_mask filters which physics collision objects are included when the parsed_geometry_type includes static colliders.

The source_geometry_mode that defines on which node(s) to start the parsing, and how to traverse the SceneTree.

The source_geometry_group_name is used when only a certain node group should be parsed. Depends on the selected source_geometry_mode.

With the source geometry added, the result of the baking can be controlled with the following properties.

The cell_size sets the rasterization grid size and should match the navigation map size.

The agent_radius shrinks the baked navigation mesh to have enough margin for the agent (collision) size.

The NavigationRegion2D baking can also be used at runtime with scripts.

To quickly test the 2D baking with default settings:

Add a NavigationRegion2D.

Add a NavigationPolygon resource to the NavigationRegion2D.

Add a Polygon2D below the NavigationRegion2D.

Draw 1 NavigationPolygon outline with the selected NavigationRegion2D draw tool.

Draw 1 Polygon2D outline inside the NavigationPolygon outline with the selected Polygon2D draw tool.

Hit the Editor bake button and a navigation mesh should appear.

When a NavigationRegion3D node is selected in the Editor, bake options appear in the top bar of the Editor.

In order for the region to work a NavigationMesh resource needs to be added.

The properties to parse and bake a navigation mesh are then part of the used resource and can be found in the resource Inspector.

The result of the source geometry parsing can be influenced with the following properties.

The parsed_geometry_type that filters if visual objects or physics objects or both should be parsed from the SceneTree. For more details on what objects are parsed and how, see the section about parsing source geometry below.

The collision_mask filters which physics collision objects are included when the parsed_geometry_type includes static colliders.

The source_geometry_mode that defines on which node(s) to start the parsing, and how to traverse the SceneTree.

The source_geometry_group_name is used when only a certain node group should be parsed. Depends on the selected source_geometry_mode.

With the source geometry added, the result of the baking can be controlled with the following properties.

The cell_size and cell_height sets the rasterization voxel grid size and should match the navigation map size.

The agent_radius shrinks the baked navigation mesh to have enough margin for the agent (collision) size.

The agent_height excludes areas from the navigation mesh where the agent is too tall to fit in.

The agent_max_climb and agent_max_slope removes areas where the height difference between neighboring voxels is too large, or where their surface is too steep.

A too small cell_size or cell_height can create so many voxels that it has the potential to freeze the game or even crash.

The NavigationRegion3D baking can also be used at runtime with scripts.

To quickly test the 3D baking with default settings:

Add a NavigationRegion3D.

Add a NavigationMesh resource to the NavigationRegion3D.

Add a MeshInstance3D below the NavigationRegion3D.

Add a PlaneMesh to the MeshInstance3D.

Hit the Editor bake button and a navigation mesh should appear.

The NavigationServer2D and NavigationServer3D have API functions to call each step of the navigation mesh baking process individually.

parse_source_geometry_data() can be used to parse source geometry to a reusable and serializable resource.

bake_from_source_geometry_data() can be used to bake a navigation mesh from already parsed data e.g. to avoid runtime performance issues with (redundant) parsing.

bake_from_source_geometry_data_async() is the same but bakes the navigation mesh deferred with threads, not blocking the main thread.

Compared to a NavigationRegion, the NavigationServer offers finer control over the navigation mesh baking process. In turn it is more complex to use but also provides more advanced options.

Some other advantages of the NavigationServer over a NavigationRegion are:

The server can parse source geometry without baking, e.g. to cache it for later use.

The server allows selecting the root node at which to start the source geometry parsing manually.

The server can accept and bake from procedurally generated source geometry data.

The server can bake multiple navigation meshes in sequence while (re)using the same source geometry data.

To bake navigation meshes with the NavigationServer, source geometry is required. Source geometry is geometry data that should be considered in a navigation mesh baking process. Both navigation meshes for 2D and 3D are created by baking them from source geometry.

2D and 3D versions of the source geometry resources are available as NavigationMeshSourceGeometryData2D and NavigationMeshSourceGeometryData3D respectively.

Source geometry can be geometry parsed from visual meshes, from physics collision, or procedural created arrays of data, like outlines (2D) and triangle faces (3D). For convenience, source geometry is commonly parsed directly from node setups in the SceneTree. For runtime navigation mesh (re)bakes, be aware that the geometry parsing always happens on the main thread.

The SceneTree is not thread-safe. Parsing source geometry from the SceneTree can only be done on the main thread.

The data from visual meshes and polygons needs to be received from the GPU, stalling the RenderingServer in the process. For runtime (re)baking prefer using physics shapes as parsed source geometry.

Source geometry is stored inside resources so the created geometry can be reused for multiple bakes. E.g. baking multiple navigation meshes for different agent sizes from the same source geometry. This also allows to save source geometry to disk so it can be loaded later, e.g. to avoid the overhead of parsing it again at runtime.

The geometry data should be in general kept very simple. As many edges as are required but as few as possible. Especially in 2D duplicated and nested geometry should be avoided as it forces polygon hole calculation that can result in flipped polygons. An example for nested geometry would be a smaller StaticBody2D shape placed completely inside the bounds of another StaticBody2D shape.

Building and updating individual navigation mesh chunks at runtime.

You can see the navigation mesh chunk baking in action in the Navigation Mesh Chunks 2D and Navigation Mesh Chunks 3D demo projects.

To avoid misaligned edges between different region chunks the navigation meshes have two important properties for the navigation mesh baking process. The baking bound and the border size. Together they can be used to ensure perfectly aligned edges between region chunks.

Navigation mesh chunk baked with bake bound or baked with additional border size.

The baking bound, which is an axis-aligned Rect2 for 2D and AABB for 3D, limits the used source geometry by discarding all the geometry that is outside of the bounds.

The NavigationPolygon properties baking_rect and baking_rect_offset can be used to create and place the 2D baking bound.

The NavigationMesh properties filter_baking_aabb and filter_baking_aabb_offset can be used to create and place the 3D baking bound.

With only the baking bound set another problem still exists. The resulting navigation mesh will inevitably be affected by necessary offsets like the agent_radius which makes the edges not align properly.

Navigation mesh chunks with noticeable gaps due to baked agent radius offset.

This is where the border_size property for navigation mesh comes in. The border size is an inward margin from the baking bound. The important characteristic of the border size is that it is unaffected by most offsets and postprocessing like the agent_radius.

Instead of discarding source geometry, the border size discards parts of the final surface of the baked navigation mesh. If the baking bound is large enough the border size can remove the problematic surface parts so that only the intended chunk size is left.

Navigation mesh chunks with aligned edges and without gaps.

The baking bounds need to be large enough to include a reasonable amount of source geometry from all the neighboring chunks.

In 3D the functionality of the border size is limited to the xz-axis.

There are some common user problems and important caveats to consider when creating or baking navigation meshes.

The navigation mesh baking is by default done on a background thread, so as long as the platform supports threads, the actual baking is rarely the source of any performance issues (assuming a reasonably sized and complex geometry for runtime rebakes).

The common source for performance issues at runtime is the parsing step for source geometry that involves nodes and the SceneTree. The SceneTree is not thread-safe so all the nodes need to be parsed on the main thread. Some nodes with a lot of data can be very heavy and slow to parse at runtime, e.g. a TileMap has one or more polygons for every single used cell and TileMapLayer to parse. Nodes that hold meshes need to request the data from the RenderingServer stalling the rendering in the process.

To improve performance, use more optimized shapes, e.g. collision shapes over detailed visual meshes, and merge and simplify as much geometry as possible upfront. If nothing helps, don't parse the SceneTree and add the source geometry procedural with scripts. If only pure data arrays are used as source geometry, the entire baking process can be done on a background thread.

The navigation mesh baking in 2D is done by doing polygon clipping operations based on outline paths. Polygons with "holes" are a necessary evil to create more complex 2D polygons but can become unpredictable for users with many complex shapes involved.

To avoid any unexpected problems with polygon hole calculations, avoid nesting any outlines inside other outlines of the same type (traversable / obstruction). This includes the parsed shapes from nodes. E.g. placing a smaller StaticBody2D shape inside a larger StaticBody2D shape can result in the resulting polygon being flipped.

The navigation mesh baking in 3D has no concept of "inside". The voxel cells used to rasterize the geometry are either occupied or not. Remove the geometry that is on the ground inside the other geometry. If that is not possible, add smaller "dummy" geometry inside with as few triangles as possible so the cells are occupied with something.

A NavigationObstacle3D shape set to bake with navigation mesh can be used to discard geometry as well.

A NavigationObstacle3D shape can be used to discard unwanted navigation mesh parts.

The following script uses the NavigationServer to parse source geometry from the scene tree, bakes a navigation mesh, and updates a navigation region with the updated navigation mesh.

The following script uses the NavigationServer to update a navigation region with procedurally generated navigation mesh data.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
var on_thread: bool = true
bake_navigation_polygon(on_thread)
```

Example 2 (unknown):
```unknown
bool onThread = true;
BakeNavigationPolygon(onThread);
```

Example 3 (unknown):
```unknown
var on_thread: bool = true
bake_navigation_mesh(on_thread)
```

Example 4 (unknown):
```unknown
bool onThread = true;
BakeNavigationMesh(onThread);
```

---

## Using the ArrayMesh — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/procedural_geometry/arraymesh.html

**Contents:**
- Using the ArrayMesh
- Setting up the ArrayMesh
- Generating geometry
- Saving
- User-contributed notes

This tutorial will present the basics of using an ArrayMesh.

To do so, we will use the function add_surface_from_arrays(), which takes up to five parameters. The first two are required, while the last three are optional.

The first parameter is the PrimitiveType, an OpenGL concept that instructs the GPU how to arrange the primitive based on the vertices given, i.e. whether they represent triangles, lines, points, etc. See Mesh.PrimitiveType for the options available.

The second parameter, arrays, is the actual Array that stores the mesh information. The array is a normal Godot array that is constructed with empty brackets []. It stores a Packed**Array (e.g. PackedVector3Array, PackedInt32Array, etc.) for each type of information that will be used to build the surface.

Common elements of arrays are listed below, together with the position they must have within arrays. See Mesh.ArrayType for a full list.

PackedVector3Array or PackedVector2Array

PackedFloat32Array or PackedFloat64Array of groups of 4 floats. The first 3 floats determine the tangent, and the last float the binormal direction as -1 or 1.

PackedVector2Array or PackedVector3Array

PackedVector2Array or PackedVector3Array

PackedFloat32Array of groups of 4 floats or PackedInt32Array of groups of 4 ints. Each group lists indexes of 4 bones that affects a given vertex.

PackedFloat32Array or PackedFloat64Array of groups of 4 floats. Each float lists the amount of weight the corresponding bone in ARRAY_BONES has on a given vertex.

In most cases when creating a mesh, we define it by its vertex positions. So usually, the array of vertices (at index 0) is required, while the index array (at index 12) is optional and will only be used if included. It is also possible to create a mesh with only the index array and no vertex array, but that's beyond the scope of this tutorial.

All the other arrays carry information about the vertices. They are optional and will only be used if included. Some of these arrays (e.g. ARRAY_COLOR) use one entry per vertex to provide extra information about vertices. They must have the same size as the vertex array. Other arrays (e.g. ARRAY_TANGENT) use four entries to describe a single vertex. These must be exactly four times larger than the vertex array.

For normal usage, the last three parameters in add_surface_from_arrays() are typically left empty.

In the editor, create a MeshInstance3D and add an ArrayMesh to it in the Inspector. Normally, adding an ArrayMesh in the editor is not useful, but in this case it allows us to access the ArrayMesh from code without creating one.

Next, add a script to the MeshInstance3D.

Under _ready(), create a new Array.

This will be the array that we keep our surface information in - it will hold all the arrays of data that the surface needs. Godot will expect it to be of size Mesh.ARRAY_MAX, so resize it accordingly.

Next create the arrays for each data type you will use.

Once you have filled your data arrays with your geometry you can create a mesh by adding each array to surface_array and then committing to the mesh.

In this example, we used Mesh.PRIMITIVE_TRIANGLES, but you can use any primitive type available from mesh.

Put together, the full code looks like:

The code that goes in the middle can be whatever you want. Below we will present some example code for generating a sphere.

Here is sample code for generating a sphere. Although the code is presented in GDScript, there is nothing Godot specific about the approach to generating it. This implementation has nothing in particular to do with ArrayMeshes and is just a generic approach to generating a sphere. If you are having trouble understanding it or want to learn more about procedural geometry in general, you can use any tutorial that you find online.

Finally, we can use the ResourceSaver class to save the ArrayMesh. This is useful when you want to generate a mesh and then use it later without having to re-generate it.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
var surface_array = []
```

Example 2 (unknown):
```unknown
Godot.Collections.Array surfaceArray = [];
```

Example 3 (unknown):
```unknown
var surface_array = []
surface_array.resize(Mesh.ARRAY_MAX)
```

Example 4 (unknown):
```unknown
Godot.Collections.Array surfaceArray = [];
surfaceArray.Resize((int)Mesh.ArrayType.Max);
```

---

## Using the MeshDataTool — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/procedural_geometry/meshdatatool.html

**Contents:**
- Using the MeshDataTool
- User-contributed notes

The MeshDataTool is not used to generate geometry. But it is helpful for dynamically altering geometry, for example if you want to write a script to tessellate, simplify, or deform meshes.

The MeshDataTool is not as fast as altering arrays directly using ArrayMesh. However, it provides more information and tools to work with meshes than the ArrayMesh does. When the MeshDataTool is used, it calculates mesh data that is not available in ArrayMeshes such as faces and edges, which are necessary for certain mesh algorithms. If you do not need this extra information then it may be better to use an ArrayMesh.

MeshDataTool can only be used on Meshes that use the PrimitiveType Mesh.PRIMITIVE_TRIANGLES.

We initialize the MeshDataTool from an ArrayMesh by calling create_from_surface(). If there is already data initialized in the MeshDataTool, calling create_from_surface() will clear it for you. Alternatively, you can call clear() yourself before re-using the MeshDataTool.

In the examples below, assume an ArrayMesh called mesh has already been created. See ArrayMesh tutorial for an example of mesh generation.

create_from_surface() uses the vertex arrays from the ArrayMesh to calculate two additional arrays, one for edges and one for faces, for a total of three arrays.

An edge is a connection between any two vertices. Each edge in the edge array contains a reference to the two vertices it is composed of, and up to two faces that it is contained within.

A face is a triangle made up of three vertices and three corresponding edges. Each face in the face array contains a reference to the three vertices and three edges it is composed of.

The vertex array contains edge, face, normal, color, tangent, uv, uv2, bone, and weight information connected with each vertex.

To access information from these arrays you use a function of the form get_****():

What you choose to do with these functions is up to you. A common use case is to iterate over all vertices and transform them in some way:

These modifications are not done in place on the ArrayMesh. If you are dynamically updating an existing ArrayMesh, first delete the existing surface before adding a new one using commit_to_surface():

Below is a complete example that turns a spherical mesh called mesh into a randomly deformed blob complete with updated normals and vertex colors. See ArrayMesh tutorial for how to generate the base mesh.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
var mdt = MeshDataTool.new()
mdt.create_from_surface(mesh, 0)
```

Example 2 (unknown):
```unknown
mdt.get_vertex_count() # Returns number of vertices in vertex array.
mdt.get_vertex_faces(0) # Returns array of faces that contain vertex[0].
mdt.get_face_normal(1) # Calculates and returns face normal of the second face.
mdt.get_edge_vertex(10, 1) # Returns the second vertex comprising the edge at index 10.
```

Example 3 (unknown):
```unknown
for i in range(get_vertex_count):
    var vert = mdt.get_vertex(i)
    vert *= 2.0 # Scales the vertex by doubling size.
    mdt.set_vertex(i, vert)
```

Example 4 (unknown):
```unknown
mesh.clear_surfaces() # Deletes all of the mesh's surfaces.
mdt.commit_to_surface(mesh)
```

---

## Using the SurfaceTool — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/procedural_geometry/surfacetool.html

**Contents:**
- Using the SurfaceTool
- User-contributed notes

The SurfaceTool provides a useful interface for constructing geometry. The interface is similar to the ImmediateMesh class. You set each per-vertex attribute (e.g. normal, uv, color) and then when you add a vertex it captures the attributes.

The SurfaceTool also provides some useful helper functions like index() and generate_normals().

Attributes are added before each vertex is added:

When finished generating your geometry with the SurfaceTool, call commit() to finish generating the mesh. If an ArrayMesh is passed to commit(), then it appends a new surface to the end of the ArrayMesh. While if nothing is passed in, commit() returns an ArrayMesh.

The code below creates a triangle without indices.

You can optionally add an index array, either by calling add_index() and adding vertices to the index array manually, or by calling index() once, which generates the index array automatically and shrinks the vertex array to remove duplicate vertices.

Similarly, if you have an index array, but you want each vertex to be unique (e.g. because you want to use unique normals or colors per face instead of per-vertex), you can call deindex().

If you don't add custom normals yourself, you can add them using generate_normals(), which should be called after generating geometry and before committing the mesh using commit() or commit_to_arrays(). Calling generate_normals(true) will flip the resulting normals. As a side note, generate_normals() only works if the primitive type is set to Mesh.PRIMITIVE_TRIANGLES.

You may notice that normal mapping or other material properties look broken on the generated mesh. This is because normal mapping requires the mesh to feature tangents, which are separate from normals. You can either add custom tangents manually, or generate them automatically with generate_tangents(). This method requires that each vertex have UVs and normals set already.

By default, when generating normals, they will be calculated on a per-vertex basis (i.e. they will be "smooth normals"). If you want flat vertex normals (i.e. a single normal vector per face), when adding vertices, call add_smooth_group(i) where i is a unique number per vertex. add_smooth_group() needs to be called while building the geometry, e.g. before the call to add_vertex().

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
var st = SurfaceTool.new()

st.begin(Mesh.PRIMITIVE_TRIANGLES)

st.set_normal() # Overwritten by normal below.
st.set_normal() # Added to next vertex.
st.set_color() # Added to next vertex.
st.add_vertex() # Captures normal and color above.
st.set_normal() # Normal never added to a vertex.
```

Example 2 (unknown):
```unknown
st.SetNormal(); // Overwritten by normal below.
st.SetNormal(); // Added to next vertex.
st.SetColor(); // Added to next vertex.
st.AddVertex(); // Captures normal and color above.
st.SetNormal(); // Normal never added to a vertex.
```

Example 3 (unknown):
```unknown
# Add surface to existing ArrayMesh.
st.commit(mesh)

# -- Or Alternatively --

# Create new ArrayMesh.
var mesh = st.commit()
```

Example 4 (unknown):
```unknown
st.Commit(mesh);
// Or:
var mesh = st.Commit();
```

---

## Using Voxel global illumination — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/global_illumination/using_voxel_gi.html

**Contents:**
- Using Voxel global illumination
- Visual comparison
- Setting up VoxelGI
- VoxelGI node properties
- VoxelGI interaction with lights and objects
- Adjusting VoxelGI performance and quality
- Reducing VoxelGI light leaks and artifacts
- User-contributed notes

VoxelGI is a form of fully real-time global illumination, intended to be used for small/medium-scale 3D scenes. VoxelGI is fairly demanding on the GPU, so it's best used when targeting dedicated graphics cards.

VoxelGI is only supported when using the Forward+ renderer, not the Mobile or Compatibility renderers.

Not sure if VoxelGI is suited to your needs? See Which global illumination technique should I use? for a comparison of GI techniques available in Godot 4.

Make sure your static level geometry is imported with the Light Baking option set to Static or Static Lightmaps in the Import dock. For manually added MeshInstance3D nodes, make sure the Global Illumination > Mode property is set to Static in the inspector.

Create a VoxelGI node in the Scene tree dock.

Move the VoxelGI node to the center of the area you want it to cover by dragging the manipulation gizmo in the 3D viewport. Then adjust the VoxelGI's extents by dragging the red points in the 3D viewport (or enter values in the inspector). Make sure the VoxelGI's extents aren't unnecessarily large, or quality will suffer.

Select the VoxelGI node and click Bake at the top of the 3D editor viewport. This will take at least a few seconds to complete (depending on the number of VoxelGI subdivisions and scene complexity).

If at least one mesh contained within the VoxelGI's extents has its global illumination mode set to Static, you should see indirect lighting appear within the scene.

To avoid bloating text-based scene files with large amounts of binary data, make sure the VoxelGIData resource is always saved to an external binary file. This file must be saved with a .res (binary resource) extension instead of .tres (text-based resource). Using an external binary resource for VoxelGIData will keep your text-based scene small while ensuring it loads and saves quickly.

The following properties can be adjusted in the VoxelGI node inspector before baking:

Subdiv: Higher values result in more precise indirect lighting, at the cost of lower performance, longer bake times and increased storage requirements.

Extents: Represents the size of the box in which indirect lighting should be baked. Extents are centered around the VoxelGI node's origin.

The following properties can be adjusted in the VoxelGIData resource that is contained within a VoxelGI node after it has been baked:

Dynamic Range: The maximum brightness that can be represented in indirect lighting. Higher values make it possible to represent brighter indirect light, at the cost of lower precision (which can result in visible banding). If in doubt, leave this unchanged.

Energy: The indirect lighting's overall energy. This also effects the energy of direct lighting emitted by meshes with emissive materials.

Bias: Optional bias added to lookups into the voxel buffer at runtime. This helps avoid self-occlusion artifacts.

Normal Bias: Similar to Bias, but offsets the lookup into the voxel buffer by the surface normal. This also helps avoid self-occlusion artifacts. Higher values reduce self-reflections visible in non-rough materials, at the cost of more visible light leaking and flatter-looking indirect lighting. To prioritize hiding self-reflections over lighting quality, set Bias to 0.0 and Normal Bias to a value between 1.0 and 2.0.

Propagation: The energy factor to use for bounced indirect lighting. Higher values will result in brighter, more diffuse lighting (which may end up looking too flat). When Use Two Bounces is enabled, you may want to decrease Propagation to compensate for the overall brighter indirect lighting.

Use Two Bounces: If enabled, lighting will bounce twice instead of just once. This results in more realistic-looking indirect lighting, and makes indirect lighting visible in reflections as well. Enabling this generally has no noticeable performance cost.

Interior: If enabled, environment sky lighting will not be taken into account by VoxelGI. This should be enabled in indoor scenes to avoid light leaking from the environment.

To ensure correct visuals when using VoxelGI, you must configure your meshes and lights' global illumination properties according to their purpose in the scene (static or dynamic).

There are 3 global illumination modes available for meshes:

Disabled: The mesh won't be taken into account for VoxelGI baking. The mesh will receive indirect lighting from the scene, but it will not contribute indirect lighting to the scene.

Static (default): The mesh will be taken into account for VoxelGI baking. The mesh will both receive and contribute indirect lighting to the scene. If the mesh is changed in any way after baking, the VoxelGI node must be baked again. Otherwise, indirect lighting will look incorrect.

Dynamic: The mesh won't be taken into account for VoxelGI baking, but it will still receive and contribute indirect lighting to the scene in real-time. This option is much slower compared to Static. Only use the Dynamic global illumination mode on large meshes that will change significantly during gameplay.

Additionally, there are 3 bake modes available for lights (DirectionalLight3D, OmniLight3D and SpotLight3D):

Disabled: The light won't be taken into account for VoxelGI baking. The light won't contribute indirect lighting to the scene.

Static: The light will be taken into account for VoxelGI baking. The light will contribute indirect lighting to the scene. If the light is changed in any way after baking, the VoxelGI node must be baked again or indirect lighting will look incorrect. If in doubt, use this mode for level lighting.

Dynamic (default): The light won't be taken into account for VoxelGI baking, but it will still contribute indirect lighting to the scene in real-time. This option is slower compared to Static. Only use the Dynamic global illumination mode on lights that will change significantly during gameplay.

The amount of indirect energy emitted by a light depends on its color, energy and indirect energy properties. To make a specific light emit more or less indirect energy without affecting the amount of direct light emitted by the light, adjust the Indirect Energy property in the Light3D inspector.

See Which global illumination mode should I use on meshes and lights? for general usage recommendations.

Since VoxelGI is relatively demanding, it will perform best on systems with recent dedicated GPUs. On older dedicated GPUs and integrated graphics, tweaking the settings is necessary to achieve reasonable performance.

In the Project Settings' Rendering > Global Illumination section, VoxelGI quality can also be adjusted in two ways:

Voxel Gi > Quality: If set to Low instead of High, voxel cone tracing will only use 4 taps instead of 6. This speeds up rendering at the cost of less pronounced ambient occlusion.

Gi > Use Half Resolution: If enabled, both VoxelGI and SDFGI will have their GI buffer rendering at halved resolution. For instance, when rendering in 3840×2160, the GI buffer will be computed at a 1920×1080 resolution. Enabling this option saves a lot of GPU time, but it can introduce visible aliasing around thin details.

Note that the Advanced toggle must be enabled in the project settings dialog for the above settings to be visible.

Additionally, VoxelGI can be disabled entirely by hiding the VoxelGI node. This can be used for comparison purposes or to improve performance on low-end systems.

After baking VoxelGI, you may notice indirect light is leaking at some spots in your level geometry. This can be remedied in several ways:

For both light leaking and artifacts, try moving or rotating the VoxelGI node then bake it again.

To combat light leaking in general, ensure your level geometry is fully sealed. This is best done in the 3D modeling software used to design the level, but primitive MeshInstance3D nodes with their global illumination mode set to Static can also be used.

To combat light leaking with thin geometry, it's recommended to make the geometry in question thicker. If this is not possible, then add a primitive MeshInstance3D node with its global illumination mode set to Static. Bake VoxelGI again, then hide the primitive MeshInstance3D node (it will still be taken into account by VoxelGI). For optimal results, the MeshInstance3D should have a material whose color matches the original thin geometry.

To combat artifacts that can appear on reflective surfaces, try increasing Bias and/or Normal Bias in the VoxelGIData resource as described above. Do not increase these values too high, or light leaking will become more pronounced.

If you notice VoxelGI nodes popping in and out of existence as the camera moves, this is most likely because the engine is rendering too many VoxelGI instances at once. Godot is limited to rendering 8 VoxelGI nodes at once, which means up to 8 instances can be in the camera view before some of them will start flickering.

Additionally, for performance reasons, Godot can only blend between 2 VoxelGI nodes at a given pixel on the screen. If you have more than 2 VoxelGI nodes overlapping, global illumination may appear to flicker as the camera moves or rotates.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Variable rate shading — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/variable_rate_shading.html

**Contents:**
- Variable rate shading
- What is variable rate shading?
- Hardware support
- Using variable rate shading in Godot
  - Creating a VRS density map
  - Performance comparison
- User-contributed notes

In modern 3D rendering engines, shaders are much more complex compared to before. The advent of physically-based rendering, real-time global illumination and screen-space effects has increased the number of per-pixel shading that must be performed to render each frame. Additionally, screen resolutions also have increased a lot, with 1440p and 4K now being common target resolutions. As a result, the total shading cost in scene rendering usually represents a significant amount of the time taken to render each frame.

Variable rate shading (VRS) is a method of decreasing this shading cost by reducing the resolution of per-pixel shading (also called fragment shading), while keeping the original resolution for rendering geometry. This means geometry edges remain as sharp as they would without VRS. VRS can be combined with any 3D antialiasing technique (MSAA, FXAA, TAA, SSAA).

VRS allows specifying the shading quality in a local manner, which makes it possible to have certain parts of the viewport receive more detailed shading than others. This is particularly useful in virtual reality (VR) to achieve foveated rendering, where the center of the viewport is more detailed than the edges.

Here's a scene rendered with rate shading disabled then enabled, using the density map linked at the bottom of this page:

Variable rate shading disabled in textured scene

Variable rate shading enabled in textured scene (lower quality, but higher performance)

When used in scenes with low-frequency detail (such as scenes with a stylized/low-poly aesthetic), it's possible to achieve similar performance gains, but with less reduction in visual quality:

Variable rate shading disabled in untextured scene

Variable rate shading enabled in untextured scene (lower quality, but higher performance)

Variable rate shading is only supported on specific GPUs:

NVIDIA Turing and newer (including GTX 1600 series)

AMD RDNA2 and newer (both integrated and dedicated GPUs – including Steam Deck)

Intel Arc Alchemist and newer (dedicated GPUs only)

Intel integrated graphics do not support variable rate shading.

Snapdragon 888 and newer

MediaTek Dimensity 9000 and newer

ARM Mali-G615 and newer

As of January 2023, Apple and Raspberry Pi GPUs do not support variable rate shading.

Both Forward+ and Mobile renderers support variable rate shading. VRS can be used in both pancake (non-XR) and XR display modes.

The Compatibility renderer does not support variable rate shading. For XR, you can use foveation level as an alternative.

In the advanced Project Settings, the Rendering > VRS section offers settings to control variable rate shading on the root viewport:

Mode: Controls the variable rate shading mode. Disabled disables variable rate shading. Texture uses a manually authored texture to set shading density (see the property below). XR automatically generates a texture suited for foveated rendering in virtual/augmented reality.

Texture: The texture to use to control shading density on the root viewport. Only used if Mode is Texture.

For custom viewports, the VRS mode and texture must be set manually to the Viewport node.

On unsupported hardware, there is no visual difference when variable rate shading is enabled. You can check whether hardware supports variable rate shading by running the editor or project with the --verbose command line argument.

If using the Texture VRS mode, you must set a texture to be used as a density map. Otherwise, no effect will be visible.

You can create your own VRS density map manually using an image editor, or generate it using another method (e.g. on the CPU using the Image class, or on the GPU using a shader). However, beware of performance implications when generating a VRS image dynamically. If opting for dynamic generation, make sure the VRS image generation process is fast enough to avoid outweighing the performance gains from VRS.

The texture must follow these rules:

The texture must use a lossless compression format so that colors can be matched precisely.

The following VRS densities are mapped to various colors, with brighter colors representing a lower level of shading precision:

rgb(0, 0, 0) - #000000

rgb(0, 85, 0) - #005500

rgb(85, 0, 0) - #550000

rgb(85, 85, 0) - #555500

rgb(85, 170, 0) - #55aa00

rgb(170, 85, 0) - #aa5500

rgb(170, 170, 0) - #aaaa00

rgb(170, 255, 0) - #aaff00

Not supported on most hardware.

rgb(255, 170, 0) - #ffaa00

Not supported on most hardware.

rgb(255, 255, 0) - #ffff00

Not supported on most hardware.

For example, this VRS density texture provides the highest shading density in the center of the viewport, and the lowest shading density in the corners:

Example VRS density map texture, simulating foveated rendering

There are no size or aspect ratio requirements for the VRS density texture. However, there is no benefit to using a VRS density map that is larger than the viewport resolution divided by the GPU's tile size. The tile size is what determines the smallest area of pixels where the shading density can be changed separately from other tiles. On most GPUs, this tile size is 8×8 pixels. You can view the tile size by running Godot with the --verbose command line argument, as it's printed in the VRS debugging information.

Therefore, sticking to a relatively low resolution such as 256×256 (square) or 480×270 (16:9) is recommended. Depending on your use cases, a square texture may be more suited compared to a texture that matches the most common viewport aspect ratio in your project (such as 16:9).

When using variable rate shading, you can use a negative texture mipmap LOD bias to reduce blurriness in areas with reduced shading rate.

Note that the texture LOD bias is set globally, so this will also affect areas of the viewport with full shading rate. Don't use values that are too low, or textures will appear grainy.

To give an idea of how much VRS can improve performance in theory, here's a performance comparison with the textured example scene shown at the top of this page. The VRS density map example present on this page is used.

Results were captured on a GeForce RTX 4090 with the NVIDIA 525.60.11 driver.

Performance improvement

In terms of performance improvements, variable rate shading is more beneficial at higher target resolutions. The reduction in visual quality is also less noticeable at high resolutions.

For non-VR games, you will probably have to use a less aggressive VRS texture than what was used in this example. As a result, the effective performance gains will be lower.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---

## Visibility ranges (HLOD) — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/visibility_ranges.html

**Contents:**
- Visibility ranges (HLOD)
- How it works
- Setting up visibility range
- Visibility range properties
  - Fade mode
  - Visibility parent
- Configuration tips
  - Use simpler materials at a distance to improve performance
  - Use dithering for LOD transitions
- User-contributed notes

Along with Mesh level of detail (LOD) and Occlusion culling, visibility ranges are another tool to improve performance in large, complex 3D scenes.

On this page, you'll learn:

What visibility ranges can do and which scenarios they are useful in.

How to set up visibility ranges (manual LOD) in Godot.

How to tune visibility ranges for best performance and quality.

If you only need meshes to become less detailed over distance, but don't have manually authored LOD meshes, consider relying on automatic Mesh level of detail (LOD) instead.

Note that automatic mesh LOD and visibility ranges can be used at the same time, even on the same mesh.

Visibility ranges can be used with any node that inherits from GeometryInstance3D. This means they can be used not only with MeshInstance3D and MultiMeshInstance3D for artist-controlled HLOD, but also GPUParticles3D, CPUParticles3D, Label3D, Sprite3D, AnimatedSprite3D and CSGShape3D.

Since visibility ranges are configured on a per-node basis, this makes it possible to use different node types as part of a LOD system. For example, you could display a MeshInstance3D representing a tree when up close, and replace it with a Sprite3D impostor in the distance to improve performance.

The benefit of HLOD over a traditional LOD system is its hierarchical nature. A single larger mesh can replace several smaller meshes, so that the number of draw calls can be reduced at a distance, but culling opportunities can be preserved when up close. For example, you can have a group of houses that uses individual MeshInstance3D nodes (one for each house) when up close, but turns into a single MeshInstance3D that represents a less detailed group of houses (or use a MultiMeshInstance3D).

Lastly, visibility ranges can also be used to fade certain objects entirely when the camera gets too close or too far. This can be used for gameplay purposes, but also to reduce visual clutter. For example, Label3D nodes can be faded using visibility ranges when they're too far away to be readable or relevant to the player.

This is a quick-start guide for configuring a basic LOD system. After following this guide, this LOD system will display a SphereMesh when up close and a BoxMesh when the camera is far away enough. A small hysteresis margin is also configured via the Begin Margin and End Margin properties. This prevents LODs from popping back and forth too quickly when the camera is moving at the "edge" of the LOD transition.

The visibility range properties can be found in the Visibility Range section of the GeometryInstance3D inspector after selecting the MeshInstance3D Node.

Add a Node3D node that will be used to group the two MeshInstance3D nodes together.

Add a first MeshInstance3D node as a child of the Node3D. Assign a new SphereMesh to its Mesh property.

Set the first MeshInstance3D's visibility range End to 10.0 and End Margin to 1.0.

Add a second MeshInstance3D node as a child of the Node3D. Assign a new BoxMesh to its Mesh property.

Set the second MeshInstance3D's visibility range Begin to 10.0 and Begin Margin to 1.0.

Move the camera away and back towards the object. Notice how the object will transition from a sphere to a box as the camera moves away.

In the inspector of any node that inherits from GeometryInstance3D, you can adjust the following properties in the GeometryInstance3D's Visibility Range section:

Begin: The instance will be hidden when the camera is closer to the instance's origin than this value (in 3D units).

Begin Margin: The hysteresis or alpha fade transition distance to use for the close-up transition (in 3D units). The behavior of this property depends on Fade Mode.

End: The instance will be hidden when the camera is further away from the instance's origin than this value (in 3D units).

End Margin: The hysteresis or alpha fade transition distance to use for the far-away transition (in 3D units). The behavior of this property depends on Fade Mode.

Fade Mode: Controls how the transition between LOD levels should be performed. See below for details.

The fade mode chosen only has a visible impact if either Visibility Range > Begin Margin or Visibility Range > End Margin is greater than 0.0.

In the inspector's Visibility Range section, there are 3 fade modes to choose from:

Disabled: Uses hysteresis to switch between LOD levels instantly. This prevents situations where LOD levels are switched back and forth quickly when the player moves forward and then backward at the LOD transition point. The hysteresis distance is determined by Visibility Range > Begin Margin and Visibility Range > End Margin. This mode provides the best performance as it doesn't force rendering to become transparent during the fade transition.

Self: Uses alpha blending to smoothly fade between LOD levels. The node will fade-out itself when reaching the limits of its own visibility range. The fade transition distance is determined by Visibility Range > Begin Margin and Visibility Range > End Margin. This mode forces transparent rendering on the object during its fade transition, so it has a performance impact.

Dependencies: Uses alpha blending to smoothly fade between LOD levels. The node will fade-in its dependencies when reaching the limits of its own visibility range. The fade transition distance is determined by Visibility Range > Begin Margin and Visibility Range > End Margin. This mode forces transparent rendering on the object during its fade transition, so it has a performance impact. This mode is intended for hierarchical LOD systems using Visibility parent. It acts the same as Self if visibility ranges are used to perform non-hierarchical LOD.

The Visibility Parent property makes it easier to set up HLOD. It allows automatically hiding child nodes if its parent is visible given its current visibility range properties.

The target of Visibility Parent must inherit from GeometryInstance3D.

Despite its name, the Visibility Parent property can point to a node that is not a parent of the node in the scene tree. However, it is impossible to point Visibility Parent towards a child node, as this creates a dependency cycle which is not supported. You will get an error message in the Output panel if a dependency cycle occurs.

Given the following scene tree (where all nodes inherit from GeometryInstance3D):

In this example, BatchOfHouses is a large mesh designed to represent all child nodes when viewed at a distance. House1 to House4 are smaller MeshInstance3Ds representing individual houses. To configure HLOD in this example, we only need to configure two things:

Set Visibility Range Begin to a number greater than 0.0 so that BatchOfHouses only appears when far away enough from the camera. Below this distance, we want House1 to House4 to be displayed instead.

On House1 to House4, assign the Visibility Parent property to BatchOfHouses.

This makes it easier to perform further adjustments, as you don't need to adjust the Visibility Range Begin of BatchOfHouses and Visibility Range End of House1 to House4.

Fade mode is automatically handled by the Visibility Parent property, so that the child nodes only become hidden once the parent node is fully faded out. This is done to minimize visible pop-in. Depending on your HLOD setup, you may want to try both the Self and Dependencies fade modes.

Nodes hidden via the Visible property are essentially removed from the visibility dependency tree, so dependent instances will not take the hidden node or its ancestors into account.

In practice, this means that if the target of the Visibility Parent node is hidden by setting its Visible property to false, the node will not be hidden according to the Visibility Range Begin value specified in the visibility parent.

One way to further improve performance is to use simpler materials for distant LOD meshes. While using LOD meshes will reduce the number of vertices that need to be rendered, the per-pixel shading load for materials remains identical. However, per-pixel shading load is regularly a bottleneck on the GPU in complex 3D scenes. One way to reduce this shading load on the GPU is to use simpler materials when they don't make much of a visual difference.

Performance gains when doing so should be carefully measured, as increasing the number of unique materials in a scene has a performance cost on its own. Still, using simpler materials for distant LOD meshes can still result in a net performance gain as a result of the fewer per-pixel calculations required.

For example, on the materials used by distant LOD meshes, you can disable expensive material features such as:

Normal Map (especially on mobile platforms)

Subsurface Scattering

Godot currently only supports alpha-based fading for visibility ranges. You can however use dithering instead by using several different materials for different LOD levels.

There are two advantages to using dithering over alpha blending for LOD transitions:

Higher performance, as dithering transparency is faster to render compared to alpha blending.

No visual glitches due to transparency sorting issues during LOD transitions.

The downside of dithering is that a "noisy" pattern is visible during LOD fade transitions. This may not be as noticeable at higher viewport resolutions or when temporal antialiasing is enabled.

Also, as distance fade in BaseMaterial3D only supports fading up close or fading when far away, this setup is best used with only two LODs as part of the setup.

Ensure Begin Margin and End Margin is set to 0.0 on both MeshInstance3D nodes, as hysteresis or alpha fade are not desired here.

On both MeshInstance3D nodes, decrease Begin by the desired fade transition distance and increase End by the same distance. This is required for the dithering transition to actually be visible.

On the MeshInstance3D that is displayed up close, edit its material in the inspector. Set its Distance Fade mode to Object Dither. Set Min Distance to the same value as the visibility range End. Set Max Distance to the same value minus the fade transition distance.

On the MeshInstance3D that is displayed far away, edit its material in the inspector. Set its Distance Fade mode to Object Dither. Set Min Distance to the same value as the visibility range Begin. Set Max Distance to the same value plus the fade transition distance.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

**Examples:**

Example 1 (unknown):
```unknown
┖╴BatchOfHouses
    ┠╴House1
    ┠╴House2
    ┠╴House3
    ┖╴House4
```

---

## Volumetric fog and fog volumes — Godot Engine (stable) documentation in English

**URL:** https://docs.godotengine.org/en/stable/tutorials/3d/volumetric_fog.html

**Contents:**
- Volumetric fog and fog volumes
- Volumetric fog properties
- Light interaction with volumetric fog
- Using volumetric fog as a volumetric lighting solution
- Balancing performance and quality
- Using fog volumes for local volumetric fog
- FogVolume properties
  - Using 3D noise density textures
- Custom FogVolume shaders
- Faking volumetric fog using quads

Volumetric fog is only supported in the Forward+ renderer, not the Mobile or Compatibility renderers.

As described in Environment and post-processing, Godot supports various visual effects including two types of fog: traditional (non-volumetric) fog and volumetric fog. Traditional fog affects the entire scene at once and cannot be customized with Fog shaders.

Volumetric fog can be used at the same time as non-volumetric fog if desired.

On this page, you'll learn:

How to set up volumetric fog in Godot.

What fog volumes are and how they differ from "global" volumetric fog.

You can see how volumetric fog works in action using the Volumetric Fog demo project.

Here is a comparison between traditional fog (which does not interact with lighting) and volumetric fog, which is able to interact with lighting:

After enabling volumetric fog in the WorldEnvironment node's Environment resource, you can edit the following properties:

Density: The base exponential density of the volumetric fog. Set this to the lowest density you want to have globally. FogVolumes can be used to add to or subtract from this density in specific areas. A value of 0.0 disables global volumetric fog while allowing FogVolumes to display volumetric fog in specific areas. Fog rendering is exponential as in real life.

Albedo: The Color of the volumetric fog when interacting with lights. Mist and fog have an albedo close to white (Color(1, 1, 1, 1)) while smoke has a darker albedo.

Emission: The emitted light from the volumetric fog. Even with emission, volumetric fog will not cast light onto other surfaces. Emission is useful to establish an ambient color. As the volumetric fog effect uses single-scattering only, fog tends to need a little bit of emission to soften the harsh shadows.

Emission Energy: The brightness of the emitted light from the volumetric fog.

GI Inject: Scales the strength of Global Illumination used in the volumetric fog's albedo color. A value of 0.0 means that Global Illumination will not impact the volumetric fog. This has a small performance cost when set above 0.0.

Anisotropy: The direction of scattered light as it goes through the volumetric fog. A value close to 1.0 means almost all light is scattered forward. A value close to 0.0 means light is scattered equally in all directions. A value close to -1.0 means light is scattered mostly backward. Fog and mist scatter light slightly forward, while smoke scatters light equally in all directions.

Length: The distance over which the volumetric fog is computed. Increase to compute fog over a greater range, decrease to add more detail when a long range is not needed. For best quality fog, keep this as low as possible.

Detail Spread: The distribution of size down the length of the froxel buffer. A higher value compresses the froxels closer to the camera and places more detail closer to the camera.

Ambient Inject: Scales the strength of ambient light used in the volumetric fog. A value of 0.0 means that ambient light will not impact the volumetric fog. This has a small performance cost when set above 0.0.

Sky Affect: Controls how much volumetric fog should be drawn onto the background sky. If set to 0.0, volumetric fog won't affect sky rendering at all (including FogVolumes).

Two additional properties are offered in the Temporal Reprojection section:

Temporal Reprojection > Enabled: Enables temporal reprojection in the volumetric fog. Temporal reprojection blends the current frame's volumetric fog with the last frame's volumetric fog to smooth out jagged edges. The performance cost is minimal, however it does lead to moving FogVolumes and Light3Ds "ghosting" and leaving a trail behind them. When temporal reprojection is enabled, try to avoid moving FogVolumes or Light3Ds too fast. Short-lived dynamic lighting effects should have Volumetric Fog Energy set to 0.0 to avoid ghosting.

Temporal Reprojection > Amount: The amount by which to blend the last frame with the current frame. A higher number results in smoother volumetric fog, but makes "ghosting" much worse. A lower value reduces ghosting but can result in the per-frame temporal jitter becoming visible.

Unlike non-volumetric fog, volumetric fog has a finite range. This means volumetric fog cannot entirely cover a large world, as it will eventually stop being rendered in the distance.

If you wish to hide distant areas from the player, it's recommended to enable both non-volumetric fog and volumetric fog at the same time, and adjust their density accordingly.

To simulate fog light scattering behavior in real life, all light types will interact with volumetric fog. How much each light will affect volumetric fog can be adjusted using the Volumetric Fog Energy property on each light. Enabling shadows on a light will also make those shadows visible on volumetric fog.

If fog light interaction is not desired for artistic reasons, this can be globally disabled by setting Volumetric Fog > Albedo to a pure black color in the Environment resource. Fog light interaction can also be disabled for specific lights by setting its Volumetric Fog Energy to 0. Doing so will also improve performance slightly by excluding the light from volumetric fog computations.

While not physically accurate, it is possible to tune volumetric fog's settings to work as volumetric lighting solution. This means that unlit parts of the environment will not be darkened anymore by fog, but light will still be able to make fog brighter in specific areas.

This can be done by setting volumetric fog density to the lowest permitted value greater than zero (0.0001), then increasing the Volumetric Fog Energy property on lights to much higher values than the default to compensate. Values between 10000 and 100000 usually work well for this.

There are a few project settings available to adjust volumetric fog performance and quality:

Rendering > Environment > Volumetric Fog > Volume Size: Base size used to determine size of froxel buffer in the camera X-axis and Y-axis. The final size is scaled by the aspect ratio of the screen, so actual values may differ from what is set. Set a larger size for more detailed fog, set a smaller size for better performance.

Rendering > Environment > Volumetric Fog > Volume Depth: Number of slices to use along the depth of the froxel buffer for volumetric fog. A lower number will be more efficient, but may result in artifacts appearing during camera movement.

Rendering > Environment > Volumetric Fog > Use Filter: Enables filtering of the volumetric fog effect prior to integration. This substantially blurs the fog which reduces fine details, but also smooths out harsh edges and aliasing artifacts. Disable when more detail is required.

Volumetric fog can cause banding to appear on the viewport, especially at higher density levels. See Color banding for guidance on reducing banding.

Sometimes, you want fog to be constrained to specific areas. Conversely, you may want to have global volumetric fog, but fog should be excluded from certain areas. Both approaches can be followed using FogVolume nodes.

Here's a quick start guide to using FogVolumes:

Make sure Volumetric Fog is enabled in the Environment properties. If global volumetric fog is undesired, set its Density to 0.0.

Create a FogVolume node.

Assign a new FogMaterial to the FogVolume node's Material property.

In the FogMaterial, set Density to a positive value to increase density within the FogVolume, or a negative value to subtract the density from global volumetric fog.

Configure the FogVolume's extents and shape as needed.

Thin fog volumes may appear to flicker when the camera moves or rotates. This can be alleviated by increasing the Rendering > Environment > Volumetric Fog > Volume Depth project setting (at a performance cost) or by decreasing Length in the Environment volumetric fog properties (at no performance cost, but at the cost of lower fog range). Alternatively, the FogVolume can be made thicker and use a lower density in the Material.

Extents: The size of the FogVolume when Shape is Ellipsoid, Cone, Cylinder or Box. If Shape is Cone or Cylinder, the cone/cylinder will be adjusted to fit within the extents. Non-uniform scaling of cone/cylinder shapes via the Extents property is not supported, but you can scale the FogVolume node instead.

Shape: The shape of the FogVolume. This can be set to Ellipsoid, Cone, Cylinder, Box or World (acts as global volumetric fog).

Material: The material used by the FogVolume. Can be either a built-in FogMaterial or a custom ShaderMaterial (Fog shaders).

After choosing New FogMaterial in the Material property, you can adjust the following properties in FogMaterial:

Density: The density of the FogVolume. Denser objects are more opaque, but may suffer from under-sampling artifacts that look like stripes. Negative values can be used to subtract fog from other FogVolumes or global volumetric fog.

Albedo: The single-scattering Color of the FogVolume. Internally, member albedo is converted into single-scattering, which is additively blended with other FogVolumes and global volumetric fog's Albedo.

Emission: The Color of the light emitted by the FogVolume. Emitted light will not cast light or shadows on other objects, but can be useful for modulating the Color of the FogVolume independently from light sources.

Height Falloff: The rate by which the height-based fog decreases in density as height increases in world space. A high falloff will result in a sharp transition, while a low falloff will result in a smoother transition. A value of 0.0 results in uniform-density fog. The height threshold is determined by the height of the associated FogVolume.

Edge Fade: The hardness of the edges of the FogVolume. A higher value will result in softer edges, while a lower value will result in harder edges.

Density Texture: The 3D texture that is used to scale the member density of the FogVolume. This can be used to vary fog density within the FogVolume with any kind of static pattern. For animated effects, consider using a custom fog shader. You can import any image as a 3D texture by changing its import type in the Import dock.

Since Godot 4.1, there is a NoiseTexture3D resource that can be used to procedurally generate 3D noise. This is well-suited to FogMaterial density textures, which can result in more detailed fog effects:

Screenshot taken with Volume Size project setting set to 192 to make high-frequency detail more visible in the fog.

To do so, select the Density Texture property and choose New NoiseTexture3D. Edit this NoiseTexture3D by clicking it, then click Noise at the bottom of the NoiseTexture3D properties and choose New FastNoiseLite. Adjust the noise texture's width, height and depth according to your fog volume's dimensions.

To improve performance, it's recommended to use low texture sizes (64×64×64 or lower), as high-frequency detail is difficult to notice in a FogVolume. If you wish to represent more detailed density variations, you will need to increase Rendering > Environment > Volumetric Fog > Volume Size in the project settings, which has a performance cost.

NoiseTexture3D's Color Ramp affects FogMaterial density textures, but since only the texture's red channel is sampled, only the color ramp's red channel will affect the resulting density.

However, using a color ramp will not tint the fog volume according to the texture. You would need to use a custom shader that reads a Texture3D to achieve this.

This page only covers the built-in settings offered by FogMaterial. If you need to customize fog behavior within a FogVolume node (such as creating animated fog), FogVolume nodes' appearance can be customized using Fog shaders.

In some cases, it may be better to use specially configured QuadMeshes as an alternative to volumetric fog:

Quads work with any rendering method, including Mobile and Compatibility.

Quads do not require temporal reprojection to look smooth, which makes them suited to fast-moving dynamic effects such as lasers. They can also represent small details which volumetric fog cannot do efficiently.

Quads generally have a lower performance cost than volumetric fog.

This approach has a few downsides though:

The fog effect has less realistic falloff, especially if the camera enters the fog.

Transparency sorting issues may occur when sprites overlap.

Performance will not necessarily be better than volumetric fog if there are lots of sprites close to the camera.

To create a QuadMesh-based fog sprite:

Create a MeshInstance3D node with a QuadMesh resource in the Mesh property. Set the size as desired.

Create a new StandardMaterial3D in the mesh's Material property.

In the StandardMaterial3D, set Shading > Shading Mode to Unshaded, Billboard > Mode to Enabled, enable Proximity Fade and set Distance Fade to Pixel Alpha.

Set the Albedo > Texture to the texture below (right-click and choose Save as…):

After setting the albedo texture, go to the Import dock, select the texture and change its compression mode to Lossless to improve quality.

The fog's color is set using the Albedo > Color property; its density is set using the color's alpha channel. For best results, you will have to adjust Proximity Fade > Distance and Distance Fade > Max Distance depending on the size of your QuadMesh.

Optionally, billboarding may be left disabled if you place the quad in a way where all of its corners are in solid geometry. This can be useful for fogging large planes that the camera cannot enter, such as bottomless pits.

Please read the User-contributed notes policy before submitting a comment.

© Copyright 2014-present Juan Linietsky, Ariel Manzur and the Godot community (CC BY 3.0).

---
